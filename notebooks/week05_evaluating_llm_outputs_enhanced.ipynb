{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“˜ Week 5: Evaluating LLM Outputs - Metrics and Frameworks\n",
    "\n",
    "## MBA 590 - Advanced AI Strategy: Prompting and Agentic Frameworks\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This week focuses on critical evaluation methods for LLM outputs. As organizations deploy LLMs at scale, systematic evaluation becomes essential for ensuring quality, safety, and business suitability. We'll explore both quantitative metrics and qualitative frameworks.\n",
    "\n",
    "### Key Topics\n",
    "- Quantitative evaluation metrics (BLEU, ROUGE, perplexity, F1-score)\n",
    "- Qualitative assessment dimensions (relevance, coherence, fluency)\n",
    "- Business-specific evaluation criteria\n",
    "- Safety and bias detection frameworks\n",
    "- Holistic evaluation approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "By the end of this week, you will be able to:\n",
    "\n",
    "1. Apply key quantitative metrics (BLEU, ROUGE, accuracy, F1-score) to evaluate LLM outputs\n",
    "2. Assess qualitative dimensions including relevance, coherence, and fluency\n",
    "3. Design evaluation frameworks appropriate for specific business contexts\n",
    "4. Identify and measure potential biases and safety issues in LLM outputs\n",
    "5. Balance quantitative and qualitative evaluation methods\n",
    "6. Develop evaluation strategies that align with business objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Academic Readings\n",
    "\n",
    "1. **Chang, Y., Wang, X., Wang, J., et al. (2023).** *A Survey on Evaluation of Large Language Models.* arXiv preprint arXiv:2307.03109.\n",
    "\n",
    "2. **Liang, P., Bommasani, R., Lee, T., et al. (2022).** *Holistic Evaluation of Language Models.* arXiv preprint arXiv:2211.09110."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why LLM Evaluation Matters\n",
    "\n",
    "### Business Imperatives:\n",
    "\n",
    "1. **Quality Assurance**: Ensure outputs meet business standards\n",
    "2. **Risk Management**: Identify problematic outputs before deployment\n",
    "3. **Continuous Improvement**: Track performance over time\n",
    "4. **Regulatory Compliance**: Document evaluation for audits\n",
    "5. **ROI Measurement**: Quantify value delivered\n",
    "\n",
    "### Evaluation Challenges:\n",
    "\n",
    "- **Subjectivity**: Many quality dimensions are hard to quantify\n",
    "- **Context-Dependence**: \"Good\" varies by use case\n",
    "- **Scale**: Manual evaluation doesn't scale\n",
    "- **Complexity**: Multiple evaluation dimensions to balance\n",
    "- **Cost**: Comprehensive evaluation requires resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Tuple\n",
    "import json\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Quantitative Evaluation Metrics\n",
    "\n",
    "### A. BLEU (Bilingual Evaluation Understudy)\n",
    "\n",
    "Originally designed for machine translation, BLEU measures n-gram overlap between generated and reference text.\n",
    "\n",
    "**Use Cases:**\n",
    "- Translation tasks\n",
    "- Text generation with clear reference outputs\n",
    "- Summarization (with limitations)\n",
    "\n",
    "**Limitations:**\n",
    "- Doesn't capture semantic meaning\n",
    "- Multiple valid outputs may score differently\n",
    "- Focuses on precision, not recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified BLEU implementation (unigram precision)\n",
    "\n",
    "def simple_bleu_score(reference: str, candidate: str) -> float:\n",
    "    \"\"\"\n",
    "    Simplified BLEU score calculation (unigram precision).\n",
    "    Real BLEU uses multiple n-gram sizes and brevity penalty.\n",
    "    \"\"\"\n",
    "    ref_words = reference.lower().split()\n",
    "    cand_words = candidate.lower().split()\n",
    "    \n",
    "    if not cand_words:\n",
    "        return 0.0\n",
    "    \n",
    "    # Count matching words\n",
    "    ref_counter = Counter(ref_words)\n",
    "    matches = sum(min(Counter(cand_words)[word], ref_counter[word]) \n",
    "                  for word in set(cand_words))\n",
    "    \n",
    "    # Precision: matches / total candidate words\n",
    "    precision = matches / len(cand_words)\n",
    "    \n",
    "    # Brevity penalty (simplified)\n",
    "    brevity_penalty = min(1.0, len(cand_words) / len(ref_words)) if ref_words else 0.0\n",
    "    \n",
    "    return precision * brevity_penalty\n",
    "\n",
    "# Example\n",
    "reference = \"The company exceeded revenue targets in Q4 2024\"\n",
    "candidate1 = \"The company exceeded revenue targets in Q4 2024\"  # Perfect match\n",
    "candidate2 = \"Company revenue targets were exceeded in Q4 2024\"  # Good, different words\n",
    "candidate3 = \"The organization performed well last quarter\"  # Same meaning, different words\n",
    "\n",
    "print(\"BLEU Score Examples (Simplified Unigram):\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Reference: {reference}\")\n",
    "print(f\"\\nCandidate 1: {candidate1}\")\n",
    "print(f\"BLEU Score: {simple_bleu_score(reference, candidate1):.3f}\\n\")\n",
    "\n",
    "print(f\"Candidate 2: {candidate2}\")\n",
    "print(f\"BLEU Score: {simple_bleu_score(reference, candidate2):.3f}\\n\")\n",
    "\n",
    "print(f\"Candidate 3: {candidate3}\")\n",
    "print(f\"BLEU Score: {simple_bleu_score(reference, candidate3):.3f}\")\n",
    "print(\"\\nNote: Candidate 3 has same meaning but lower BLEU - a key limitation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. ROUGE (Recall-Oriented Understudy for Gisting Evaluation)\n",
    "\n",
    "Measures overlap between generated and reference text, focusing on recall.\n",
    "\n",
    "**Variants:**\n",
    "- **ROUGE-N**: N-gram overlap\n",
    "- **ROUGE-L**: Longest common subsequence\n",
    "- **ROUGE-S**: Skip-bigram overlap\n",
    "\n",
    "**Common Use Cases:**\n",
    "- Summarization\n",
    "- Content generation\n",
    "- Question answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified ROUGE-1 (unigram recall and F1)\n",
    "\n",
    "def rouge_1_score(reference: str, candidate: str) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calculate ROUGE-1 Precision, Recall, and F1.\n",
    "    \"\"\"\n",
    "    ref_words = reference.lower().split()\n",
    "    cand_words = candidate.lower().split()\n",
    "    \n",
    "    if not ref_words or not cand_words:\n",
    "        return {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n",
    "    \n",
    "    ref_counter = Counter(ref_words)\n",
    "    cand_counter = Counter(cand_words)\n",
    "    \n",
    "    matches = sum(min(cand_counter[word], ref_counter[word]) \n",
    "                  for word in set(cand_words))\n",
    "    \n",
    "    precision = matches / len(cand_words)\n",
    "    recall = matches / len(ref_words)\n",
    "    \n",
    "    if precision + recall > 0:\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        f1 = 0.0\n",
    "    \n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "# Example: Summarization task\n",
    "original_text = \"\"\"The quarterly earnings report shows strong performance across all divisions. \n",
    "Revenue increased by 15% year-over-year, driven by cloud services growth. \n",
    "Operating margins improved to 28% from 24% last quarter.\"\"\"\n",
    "\n",
    "summary1 = \"Revenue increased 15% year-over-year with improved margins to 28%.\"\n",
    "summary2 = \"Strong quarterly results with revenue growth.\"\n",
    "\n",
    "print(\"ROUGE-1 Score Examples:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Original: {original_text}\\n\")\n",
    "\n",
    "for i, summary in enumerate([summary1, summary2], 1):\n",
    "    scores = rouge_1_score(original_text, summary)\n",
    "    print(f\"Summary {i}: {summary}\")\n",
    "    print(f\"  Precision: {scores['precision']:.3f}\")\n",
    "    print(f\"  Recall: {scores['recall']:.3f}\")\n",
    "    print(f\"  F1 Score: {scores['f1']:.3f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Accuracy and F1-Score for Classification Tasks\n",
    "\n",
    "When LLMs perform classification (e.g., sentiment analysis, categorization):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification metrics example\n",
    "\n",
    "def calculate_classification_metrics(true_labels: List[str], \n",
    "                                     predicted_labels: List[str]) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calculate accuracy and per-class F1 scores.\n",
    "    \"\"\"\n",
    "    if len(true_labels) != len(predicted_labels):\n",
    "        raise ValueError(\"Label lists must be same length\")\n",
    "    \n",
    "    # Accuracy\n",
    "    correct = sum(t == p for t, p in zip(true_labels, predicted_labels))\n",
    "    accuracy = correct / len(true_labels)\n",
    "    \n",
    "    # F1 per class (simplified - macro average)\n",
    "    classes = set(true_labels + predicted_labels)\n",
    "    f1_scores = {}\n",
    "    \n",
    "    for cls in classes:\n",
    "        tp = sum((t == cls and p == cls) for t, p in zip(true_labels, predicted_labels))\n",
    "        fp = sum((t != cls and p == cls) for t, p in zip(true_labels, predicted_labels))\n",
    "        fn = sum((t == cls and p != cls) for t, p in zip(true_labels, predicted_labels))\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        f1_scores[cls] = f1\n",
    "    \n",
    "    macro_f1 = sum(f1_scores.values()) / len(f1_scores) if f1_scores else 0\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'macro_f1': macro_f1,\n",
    "        'per_class_f1': f1_scores\n",
    "    }\n",
    "\n",
    "# Example: Customer sentiment classification\n",
    "true_sentiments = ['positive', 'negative', 'neutral', 'positive', 'negative', \n",
    "                  'neutral', 'positive', 'negative', 'positive', 'neutral']\n",
    "predicted_sentiments = ['positive', 'negative', 'neutral', 'positive', 'neutral',\n",
    "                       'neutral', 'positive', 'negative', 'positive', 'positive']\n",
    "\n",
    "metrics = calculate_classification_metrics(true_sentiments, predicted_sentiments)\n",
    "\n",
    "print(\"Classification Metrics Example:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Accuracy: {metrics['accuracy']:.2%}\")\n",
    "print(f\"Macro F1: {metrics['macro_f1']:.3f}\\n\")\n",
    "print(\"Per-Class F1 Scores:\")\n",
    "for cls, score in metrics['per_class_f1'].items():\n",
    "    print(f\"  {cls.capitalize()}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Perplexity\n",
    "\n",
    "Measures how well a language model predicts a sample. Lower perplexity = better.\n",
    "\n",
    "**Use Cases:**\n",
    "- Model comparison\n",
    "- Training progress monitoring\n",
    "- Domain adaptation assessment\n",
    "\n",
    "**Note**: Perplexity doesn't directly measure output quality - a model can have low perplexity but still generate inappropriate content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Qualitative Evaluation Dimensions\n",
    "\n",
    "### Key Dimensions for Business Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qualitative evaluation framework\n",
    "\n",
    "class QualitativeEvaluator:\n",
    "    \"\"\"\n",
    "    Framework for qualitative LLM output evaluation.\n",
    "    \"\"\"\n",
    "    \n",
    "    DIMENSIONS = {\n",
    "        'relevance': {\n",
    "            'description': 'Does the output address the query/task?',\n",
    "            'scale': ['Not Relevant', 'Partially Relevant', 'Mostly Relevant', 'Highly Relevant']\n",
    "        },\n",
    "        'coherence': {\n",
    "            'description': 'Is the output logically structured and consistent?',\n",
    "            'scale': ['Incoherent', 'Somewhat Coherent', 'Coherent', 'Highly Coherent']\n",
    "        },\n",
    "        'fluency': {\n",
    "            'description': 'Is the language natural and grammatically correct?',\n",
    "            'scale': ['Poor', 'Fair', 'Good', 'Excellent']\n",
    "        },\n",
    "        'accuracy': {\n",
    "            'description': 'Is the information factually correct?',\n",
    "            'scale': ['Inaccurate', 'Partially Accurate', 'Mostly Accurate', 'Fully Accurate']\n",
    "        },\n",
    "        'completeness': {\n",
    "            'description': 'Does it cover all necessary aspects?',\n",
    "            'scale': ['Incomplete', 'Partially Complete', 'Mostly Complete', 'Comprehensive']\n",
    "        },\n",
    "        'tone': {\n",
    "            'description': 'Is the tone appropriate for the context?',\n",
    "            'scale': ['Inappropriate', 'Somewhat Appropriate', 'Appropriate', 'Ideal']\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_rubric() -> pd.DataFrame:\n",
    "        \"\"\"Create evaluation rubric.\"\"\"\n",
    "        rubric_data = []\n",
    "        for dimension, info in QualitativeEvaluator.DIMENSIONS.items():\n",
    "            rubric_data.append({\n",
    "                'Dimension': dimension.capitalize(),\n",
    "                'Question': info['description'],\n",
    "                'Scale': ' â†’ '.join(info['scale']),\n",
    "                'Score Range': '1-4'\n",
    "            })\n",
    "        return pd.DataFrame(rubric_data)\n",
    "\n",
    "# Display evaluation rubric\n",
    "print(\"QUALITATIVE EVALUATION RUBRIC\")\n",
    "print(\"=\"*70)\n",
    "rubric = QualitativeEvaluator.create_rubric()\n",
    "for idx, row in rubric.iterrows():\n",
    "    print(f\"\\n{row['Dimension']}:\")\n",
    "    print(f\"  Question: {row['Question']}\")\n",
    "    print(f\"  Scale: {row['Scale']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Practical Evaluation Exercise\n",
    "\n",
    "Let's evaluate sample LLM outputs using both quantitative and qualitative methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample evaluation task: Product description\n",
    "\n",
    "prompt = \"Write a product description for a premium noise-cancelling headphone targeting business professionals.\"\n",
    "\n",
    "# Reference/ideal output (human-written)\n",
    "reference_output = \"\"\"Elevate your focus with our Executive Noise-Cancelling Headphones. \n",
    "Designed for the modern professional, these headphones combine industry-leading noise cancellation \n",
    "with exceptional comfort for all-day wear. Premium leather cushions and adjustable headband ensure \n",
    "a perfect fit during long flights or back-to-back meetings. Crystal-clear call quality and \n",
    "30-hour battery life keep you connected and productive wherever business takes you.\"\"\"\n",
    "\n",
    "# LLM-generated outputs (simulated)\n",
    "llm_output_1 = \"\"\"Premium noise-cancelling headphones for business professionals. \n",
    "Features advanced noise cancellation technology, comfortable design, and long battery life. \n",
    "Perfect for travel and office use. High-quality audio and clear calls.\"\"\"\n",
    "\n",
    "llm_output_2 = \"\"\"Experience unparalleled audio excellence with our premium headphones. \n",
    "The cutting-edge noise cancellation technology creates an oasis of silence, allowing you to \n",
    "immerse yourself in work or relaxation. Luxurious materials and ergonomic design ensure comfort \n",
    "during extended use. Whether you're navigating a bustling airport or concentrating in a busy \n",
    "office, these headphones are your gateway to productivity and peace.\"\"\"\n",
    "\n",
    "print(\"EVALUATION TASK: Product Description\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"\\nReference Output:\\n{reference_output}\")\n",
    "print(f\"\\nLLM Output 1:\\n{llm_output_1}\")\n",
    "print(f\"\\nLLM Output 2:\\n{llm_output_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantitative evaluation\n",
    "\n",
    "print(\"\\nQUANTITATIVE EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i, output in enumerate([llm_output_1, llm_output_2], 1):\n",
    "    print(f\"\\nLLM Output {i}:\")\n",
    "    \n",
    "    # BLEU score\n",
    "    bleu = simple_bleu_score(reference_output, output)\n",
    "    print(f\"  BLEU Score: {bleu:.3f}\")\n",
    "    \n",
    "    # ROUGE scores\n",
    "    rouge = rouge_1_score(reference_output, output)\n",
    "    print(f\"  ROUGE-1 Precision: {rouge['precision']:.3f}\")\n",
    "    print(f\"  ROUGE-1 Recall: {rouge['recall']:.3f}\")\n",
    "    print(f\"  ROUGE-1 F1: {rouge['f1']:.3f}\")\n",
    "    \n",
    "    # Word count\n",
    "    print(f\"  Word Count: {len(output.split())} (Reference: {len(reference_output.split())})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qualitative evaluation example\n",
    "\n",
    "# Simulated human ratings (1-4 scale)\n",
    "qualitative_scores = {\n",
    "    'Output 1': {\n",
    "        'relevance': 4,\n",
    "        'coherence': 3,\n",
    "        'fluency': 3,\n",
    "        'accuracy': 4,\n",
    "        'completeness': 2,\n",
    "        'tone': 3\n",
    "    },\n",
    "    'Output 2': {\n",
    "        'relevance': 4,\n",
    "        'coherence': 4,\n",
    "        'fluency': 4,\n",
    "        'accuracy': 3,\n",
    "        'completeness': 3,\n",
    "        'tone': 4\n",
    "    }\n",
    "}\n",
    "\n",
    "df_qual = pd.DataFrame(qualitative_scores).T\n",
    "df_qual['Average'] = df_qual.mean(axis=1)\n",
    "\n",
    "print(\"\\nQUALITATIVE EVALUATION (1-4 scale)\")\n",
    "print(\"=\"*70)\n",
    "print(df_qual)\n",
    "\n",
    "print(\"\\nAnalysis:\")\n",
    "print(\"- Output 1: More concise but lacks detail (low completeness)\")\n",
    "print(\"- Output 2: More engaging and complete but slightly verbose\")\n",
    "print(\"- Both are relevant and accurate\")\n",
    "print(\"- Output 2 has better tone for premium positioning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Safety and Bias Evaluation\n",
    "\n",
    "Critical for responsible deployment in business contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Safety and bias evaluation framework\n",
    "\n",
    "safety_dimensions = {\n",
    "    'Category': [\n",
    "        'Toxicity',\n",
    "        'Bias (Gender)',\n",
    "        'Bias (Race/Ethnicity)',\n",
    "        'Bias (Age)',\n",
    "        'Harmful Content',\n",
    "        'Privacy Violations',\n",
    "        'Misinformation',\n",
    "        'Professional Appropriateness'\n",
    "    ],\n",
    "    'What to Check': [\n",
    "        'Offensive language, hate speech, harassment',\n",
    "        'Stereotypes, unfair treatment based on gender',\n",
    "        'Stereotypes, discriminatory language',\n",
    "        'Ageist assumptions or language',\n",
    "        'Dangerous advice, illegal activities',\n",
    "        'Exposure of personal/confidential information',\n",
    "        'False claims, unverified information',\n",
    "        'Suitable for business context and audience'\n",
    "    ],\n",
    "    'Detection Method': [\n",
    "        'Automated toxicity classifiers + human review',\n",
    "        'Counterfactual testing (swap genders)',\n",
    "        'Representation analysis + expert review',\n",
    "        'Pattern detection + human judgment',\n",
    "        'Content filters + policy compliance check',\n",
    "        'PII detection tools + manual audit',\n",
    "        'Fact-checking against sources',\n",
    "        'Business standards alignment review'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_safety = pd.DataFrame(safety_dimensions)\n",
    "print(\"SAFETY AND BIAS EVALUATION FRAMEWORK\")\n",
    "print(\"=\"*70)\n",
    "for idx, row in df_safety.iterrows():\n",
    "    print(f\"\\n{idx + 1}. {row['Category']}\")\n",
    "    print(f\"   Check: {row['What to Check']}\")\n",
    "    print(f\"   Method: {row['Detection Method']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counterfactual Testing Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Testing for gender bias\n",
    "\n",
    "test_prompts = [\n",
    "    \"Describe a successful CEO.\",\n",
    "    \"Describe a nurse at work.\",\n",
    "    \"Describe a software engineer.\"\n",
    "]\n",
    "\n",
    "# Simulated outputs to analyze\n",
    "simulated_outputs = [\n",
    "    \"A successful CEO is a strong leader who makes decisive decisions. He typically has an MBA and extensive business experience.\",\n",
    "    \"A nurse is caring and compassionate. She works long hours providing patient care and emotional support.\",\n",
    "    \"A software engineer is analytical and detail-oriented. He writes code and solves complex technical problems.\"\n",
    "]\n",
    "\n",
    "print(\"BIAS DETECTION EXAMPLE: Gender Pronoun Usage\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nPrompts and Outputs to Analyze:\\n\")\n",
    "\n",
    "for prompt, output in zip(test_prompts, simulated_outputs):\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Output: {output}\")\n",
    "    \n",
    "    # Simple pronoun detection\n",
    "    male_pronouns = len(re.findall(r'\\b(he|his|him)\\b', output.lower()))\n",
    "    female_pronouns = len(re.findall(r'\\b(she|her|hers)\\b', output.lower()))\n",
    "    neutral_pronouns = len(re.findall(r'\\b(they|their|them)\\b', output.lower()))\n",
    "    \n",
    "    print(f\"Pronouns - Male: {male_pronouns}, Female: {female_pronouns}, Neutral: {neutral_pronouns}\")\n",
    "    print(f\"Issue: Gendered assumptions present\\n\")\n",
    "\n",
    "print(\"Recommendation: Rewrite prompts to encourage gender-neutral language or\")\n",
    "print(\"implement post-processing to replace gendered pronouns with neutral ones.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Business-Specific Evaluation Frameworks\n",
    "\n",
    "Different business contexts require different evaluation priorities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context-specific evaluation priorities\n",
    "\n",
    "evaluation_priorities = {\n",
    "    'Use Case': [\n",
    "        'Customer Support',\n",
    "        'Legal/Compliance',\n",
    "        'Marketing Copy',\n",
    "        'Technical Documentation',\n",
    "        'Financial Analysis',\n",
    "        'Internal Communications'\n",
    "    ],\n",
    "    'Top Priority Metrics': [\n",
    "        'Accuracy, Tone, Completeness',\n",
    "        'Accuracy, Safety, Precision',\n",
    "        'Creativity, Tone, Engagement',\n",
    "        'Accuracy, Completeness, Clarity',\n",
    "        'Accuracy, Precision, Verifiability',\n",
    "        'Clarity, Tone, Relevance'\n",
    "    ],\n",
    "    'Critical Safeguards': [\n",
    "        'No harmful advice, brand alignment',\n",
    "        'No legal errors, citation required',\n",
    "        'No offensive content, brand voice',\n",
    "        'No technical errors, version control',\n",
    "        'No false data, source verification',\n",
    "        'No confidential leaks, appropriate tone'\n",
    "    ],\n",
    "    'Evaluation Method': [\n",
    "        'Automated + Human review + Customer feedback',\n",
    "        'Expert review + Compliance check',\n",
    "        'A/B testing + Engagement metrics',\n",
    "        'Technical review + User testing',\n",
    "        'Expert validation + Backtesting',\n",
    "        'Stakeholder review + Readability metrics'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_priorities = pd.DataFrame(evaluation_priorities)\n",
    "print(\"BUSINESS-SPECIFIC EVALUATION PRIORITIES\")\n",
    "print(\"=\"*70)\n",
    "for idx, row in df_priorities.iterrows():\n",
    "    print(f\"\\n{row['Use Case']}:\")\n",
    "    print(f\"  Priority Metrics: {row['Top Priority Metrics']}\")\n",
    "    print(f\"  Critical Safeguards: {row['Critical Safeguards']}\")\n",
    "    print(f\"  Evaluation: {row['Evaluation Method']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Holistic Evaluation Approach\n",
    "\n",
    "Combining multiple evaluation methods for comprehensive assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Holistic evaluation scorecard\n",
    "\n",
    "class HolisticEvaluator:\n",
    "    \"\"\"Comprehensive evaluation combining multiple dimensions.\"\"\"\n",
    "    \n",
    "    def __init__(self, use_case: str):\n",
    "        self.use_case = use_case\n",
    "        self.scores = {}\n",
    "    \n",
    "    def add_quantitative_scores(self, bleu: float, rouge_f1: float, accuracy: float):\n",
    "        \"\"\"Add quantitative metric scores.\"\"\"\n",
    "        self.scores['quantitative'] = {\n",
    "            'BLEU': bleu,\n",
    "            'ROUGE-F1': rouge_f1,\n",
    "            'Accuracy': accuracy\n",
    "        }\n",
    "    \n",
    "    def add_qualitative_scores(self, relevance: int, coherence: int, \n",
    "                              fluency: int, tone: int):\n",
    "        \"\"\"Add qualitative scores (1-4 scale).\"\"\"\n",
    "        self.scores['qualitative'] = {\n",
    "            'Relevance': relevance,\n",
    "            'Coherence': coherence,\n",
    "            'Fluency': fluency,\n",
    "            'Tone': tone\n",
    "        }\n",
    "    \n",
    "    def add_safety_scores(self, toxicity: bool, bias: bool, \n",
    "                         harmful: bool, appropriate: bool):\n",
    "        \"\"\"Add safety checks (True = passed).\"\"\"\n",
    "        self.scores['safety'] = {\n",
    "            'No Toxicity': toxicity,\n",
    "            'No Bias': bias,\n",
    "            'No Harmful Content': harmful,\n",
    "            'Professionally Appropriate': appropriate\n",
    "        }\n",
    "    \n",
    "    def calculate_overall_score(self, weights: Dict[str, float] = None) -> float:\n",
    "        \"\"\"Calculate weighted overall score.\"\"\"\n",
    "        if weights is None:\n",
    "            weights = {'quantitative': 0.3, 'qualitative': 0.4, 'safety': 0.3}\n",
    "        \n",
    "        overall = 0.0\n",
    "        \n",
    "        # Quantitative (0-1 scale)\n",
    "        if 'quantitative' in self.scores:\n",
    "            quant_avg = sum(self.scores['quantitative'].values()) / len(self.scores['quantitative'])\n",
    "            overall += quant_avg * weights['quantitative']\n",
    "        \n",
    "        # Qualitative (1-4 scale, normalize to 0-1)\n",
    "        if 'qualitative' in self.scores:\n",
    "            qual_avg = (sum(self.scores['qualitative'].values()) / len(self.scores['qualitative']) - 1) / 3\n",
    "            overall += qual_avg * weights['qualitative']\n",
    "        \n",
    "        # Safety (boolean, convert to 0-1)\n",
    "        if 'safety' in self.scores:\n",
    "            safety_avg = sum(self.scores['safety'].values()) / len(self.scores['safety'])\n",
    "            overall += safety_avg * weights['safety']\n",
    "        \n",
    "        return overall\n",
    "    \n",
    "    def generate_report(self) -> str:\n",
    "        \"\"\"Generate evaluation report.\"\"\"\n",
    "        report = f\"HOLISTIC EVALUATION REPORT: {self.use_case}\\n\"\n",
    "        report += \"=\" * 70 + \"\\n\\n\"\n",
    "        \n",
    "        for category, scores in self.scores.items():\n",
    "            report += f\"{category.upper()}:\\n\"\n",
    "            for metric, value in scores.items():\n",
    "                if isinstance(value, bool):\n",
    "                    report += f\"  {metric}: {'âœ“ Pass' if value else 'âœ— Fail'}\\n\"\n",
    "                elif isinstance(value, float):\n",
    "                    report += f\"  {metric}: {value:.3f}\\n\"\n",
    "                else:\n",
    "                    report += f\"  {metric}: {value}\\n\"\n",
    "            report += \"\\n\"\n",
    "        \n",
    "        overall = self.calculate_overall_score()\n",
    "        report += f\"OVERALL SCORE: {overall:.3f} ({overall*100:.1f}%)\\n\"\n",
    "        \n",
    "        if overall >= 0.8:\n",
    "            report += \"\\nRECOMMENDATION: âœ“ Approved for deployment\"\n",
    "        elif overall >= 0.6:\n",
    "            report += \"\\nRECOMMENDATION: âš  Needs minor improvements\"\n",
    "        else:\n",
    "            report += \"\\nRECOMMENDATION: âœ— Requires significant revision\"\n",
    "        \n",
    "        return report\n",
    "\n",
    "# Example usage\n",
    "evaluator = HolisticEvaluator(\"Customer Support Email\")\n",
    "evaluator.add_quantitative_scores(bleu=0.45, rouge_f1=0.52, accuracy=0.88)\n",
    "evaluator.add_qualitative_scores(relevance=4, coherence=4, fluency=3, tone=4)\n",
    "evaluator.add_safety_scores(toxicity=True, bias=True, harmful=True, appropriate=True)\n",
    "\n",
    "print(evaluator.generate_report())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Hands-On Practice Activity\n",
    "\n",
    "### Evaluate LLM-Generated Content\n",
    "\n",
    "Find or create an example of LLM-generated text relevant to your business domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR TURN: Provide your LLM-generated text\n",
    "\n",
    "my_llm_output = \"\"\"\n",
    "[Paste your LLM-generated text here]\n",
    "\n",
    "Example: A marketing email, product description, report summary, etc.\n",
    "\"\"\"\n",
    "\n",
    "my_reference_text = \"\"\"\n",
    "[Optional: Paste reference/ideal text for comparison]\n",
    "\"\"\"\n",
    "\n",
    "print(\"My LLM Output:\")\n",
    "print(my_llm_output)\n",
    "print(\"\\nReference Text (if applicable):\")\n",
    "print(my_reference_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR TURN: Qualitative evaluation\n",
    "\n",
    "# Rate each dimension on a 1-4 scale\n",
    "my_qualitative_scores = {\n",
    "    'relevance': 0,      # 1-4: Does it address the task?\n",
    "    'coherence': 0,      # 1-4: Is it logically structured?\n",
    "    'fluency': 0,        # 1-4: Is the language natural?\n",
    "    'accuracy': 0,       # 1-4: Is information correct?\n",
    "    'completeness': 0,   # 1-4: Does it cover all aspects?\n",
    "    'tone': 0           # 1-4: Is the tone appropriate?\n",
    "}\n",
    "\n",
    "print(\"My Qualitative Scores:\")\n",
    "for dimension, score in my_qualitative_scores.items():\n",
    "    print(f\"  {dimension.capitalize()}: {score}/4\")\n",
    "\n",
    "avg_score = sum(my_qualitative_scores.values()) / len(my_qualitative_scores)\n",
    "print(f\"\\nAverage Score: {avg_score:.2f}/4.00\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR TURN: Identify needed quantitative metrics\n",
    "\n",
    "my_metric_needs = \"\"\"\n",
    "For quantitative assessment of my LLM output, I would need:\n",
    "\n",
    "1. [Metric name]: [Why needed and what it would measure]\n",
    "\n",
    "2. [Metric name]: [Why needed and what it would measure]\n",
    "\n",
    "3. [Metric name]: [Why needed and what it would measure]\n",
    "\n",
    "Example:\n",
    "1. ROUGE-L: To measure how well key information from source documents is captured\n",
    "2. Accuracy: To verify factual claims against our product database\n",
    "3. Sentiment Score: To ensure positive tone aligns with brand voice\n",
    "\"\"\"\n",
    "\n",
    "print(my_metric_needs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Discussion Questions\n",
    "\n",
    "Reflect on the following:\n",
    "\n",
    "1. **Evaluation Context**: Find an example of LLM-generated text (e.g., news article summary, marketing copy). Evaluate it qualitatively based on relevance, coherence, and fluency. What metrics would be needed for a quantitative assessment?\n",
    "\n",
    "2. **Metric Selection**: For your business use case, which evaluation metrics (quantitative and qualitative) matter most? Why?\n",
    "\n",
    "3. **Trade-offs**: How do you balance the need for rigorous evaluation against the time and cost required? Where can automation help?\n",
    "\n",
    "4. **Safety vs. Utility**: Have you encountered situations where safety checks might restrict useful outputs? How do you find the right balance?\n",
    "\n",
    "5. **Human-in-the-Loop**: For which evaluation dimensions is human judgment essential vs. where can automation suffice?\n",
    "\n",
    "6. **Continuous Monitoring**: How would you design an ongoing evaluation system for LLMs in production? What would you monitor and how often?\n",
    "\n",
    "7. **Failure Cases**: What would constitute a \"critical failure\" in your use case that must be prevented at all costs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Reflections:\n",
    "\n",
    "**Question 1 - Evaluation Context:**\n",
    "\n",
    "[Your response]\n",
    "\n",
    "**Question 2 - Metric Selection:**\n",
    "\n",
    "[Your response]\n",
    "\n",
    "**Question 3 - Trade-offs:**\n",
    "\n",
    "[Your response]\n",
    "\n",
    "**Question 4 - Safety vs. Utility:**\n",
    "\n",
    "[Your response]\n",
    "\n",
    "**Question 5 - Human-in-the-Loop:**\n",
    "\n",
    "[Your response]\n",
    "\n",
    "**Question 6 - Continuous Monitoring:**\n",
    "\n",
    "[Your response]\n",
    "\n",
    "**Question 7 - Failure Cases:**\n",
    "\n",
    "[Your response]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Key Takeaways\n",
    "\n",
    "1. **No single metric tells the whole story** - comprehensive evaluation requires multiple quantitative and qualitative measures\n",
    "\n",
    "2. **Context matters** - evaluation priorities vary significantly across business use cases\n",
    "\n",
    "3. **Quantitative metrics have limitations** - BLEU and ROUGE capture overlap but not semantic quality or appropriateness\n",
    "\n",
    "4. **Safety and bias evaluation is critical** - especially for customer-facing and high-stakes applications\n",
    "\n",
    "5. **Human judgment remains essential** - particularly for nuanced dimensions like tone, appropriateness, and strategic alignment\n",
    "\n",
    "6. **Continuous evaluation is necessary** - model behavior can drift, and business requirements evolve\n",
    "\n",
    "7. **Document your evaluation approach** - for compliance, improvement, and knowledge sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Looking Ahead to Week 6\n",
    "\n",
    "Next week, we'll explore **Introduction to Agentic Frameworks: Concepts and Architectures**.\n",
    "\n",
    "We'll cover:\n",
    "- Defining agentic systems: autonomy, planning, reasoning, tool use\n",
    "- Core concepts: perception, action loops, memory\n",
    "- Architectural patterns (ReAct framework, multi-agent concepts)\n",
    "- Distinguishing agents from standard automation\n",
    "\n",
    "**Preparation:** Think about tasks in your organization that require autonomous decision-making, planning across multiple steps, or use of various tools/data sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "### Evaluation Tools:\n",
    "- [NLTK Metrics](https://www.nltk.org/) - Python library with BLEU, ROUGE implementations\n",
    "- [Hugging Face Evaluate](https://huggingface.co/docs/evaluate/index) - Comprehensive evaluation library\n",
    "- [Google Perspective API](https://perspectiveapi.com/) - Toxicity detection\n",
    "\n",
    "### Frameworks:\n",
    "- [HELM: Holistic Evaluation of Language Models](https://crfm.stanford.edu/helm/)\n",
    "- [OpenAI Evals](https://github.com/openai/evals) - Evaluation framework\n",
    "- [EleutherAI LM Evaluation Harness](https://github.com/EleutherAI/lm-evaluation-harness)\n",
    "\n",
    "### Academic Resources:\n",
    "- [Papers with Code - NLP Evaluation](https://paperswithcode.com/task/nlp-evaluation)\n",
    "- [ACL Anthology - Evaluation Papers](https://aclanthology.org/)\n",
    "\n",
    "---\n",
    "\n",
    "*End of Week 5 Notebook*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
