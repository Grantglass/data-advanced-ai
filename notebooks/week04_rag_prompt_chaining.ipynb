{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4: Advanced Prompting III - RAG & Prompt Chaining\n",
    "\n",
    "## MBA 590 - Advanced AI Strategy: Prompting and Agentic Frameworks\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This week focuses on two powerful advanced techniques: Retrieval-Augmented Generation (RAG) and Prompt Chaining. RAG enables LLMs to access and ground their responses in specific knowledge bases, while prompt chaining allows us to orchestrate complex multi-step workflows.\n",
    "\n",
    "### Key Topics\n",
    "- Retrieval-Augmented Generation (RAG) concepts and architecture\n",
    "- Benefits of grounding LLMs in specific knowledge\n",
    "- Prompt chaining techniques for multi-step tasks\n",
    "- Combining RAG and prompt chaining for business workflows\n",
    "- Practical implementation patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this week, you will be able to:\n",
    "\n",
    "1. Understand the architecture and benefits of Retrieval-Augmented Generation (RAG)\n",
    "2. Identify business scenarios where RAG provides significant value\n",
    "3. Design and implement prompt chaining workflows for multi-step tasks\n",
    "4. Recognize the limitations and challenges of RAG systems\n",
    "5. Combine RAG and prompt chaining to automate complex business processes\n",
    "6. Evaluate trade-offs between different RAG implementation approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Academic Readings\n",
    "\n",
    "1. **Lewis, P., Perez, E., Piktus, A., et al. (2020).** *Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.* arXiv preprint arXiv:2005.11401.\n",
    "\n",
    "2. **Wu, C., Korbak, T., Pinto, L., et al. (2021).** *AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts.* arXiv preprint arXiv:2110.02491."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "### What is RAG?\n",
    "\n",
    "Retrieval-Augmented Generation combines two key components:\n",
    "1. **Retrieval System**: Searches and retrieves relevant information from a knowledge base\n",
    "2. **Generation System**: Uses the retrieved information to generate accurate, grounded responses\n",
    "\n",
    "### Why RAG Matters for Business\n",
    "\n",
    "**Key Benefits:**\n",
    "- **Factual Grounding**: Reduces hallucinations by providing source documents\n",
    "- **Up-to-date Information**: Access current data without retraining models\n",
    "- **Domain Specificity**: Leverage proprietary or specialized knowledge\n",
    "- **Transparency**: Responses can cite sources for verification\n",
    "- **Cost Efficiency**: Avoid expensive fine-tuning for knowledge updates\n",
    "\n",
    "### RAG Architecture Components\n",
    "\n",
    "1. **Document Store**: Repository of knowledge (databases, documents, wikis)\n",
    "2. **Embedding Model**: Converts text to vector representations\n",
    "3. **Vector Database**: Stores and indexes document embeddings\n",
    "4. **Retriever**: Finds relevant documents based on query similarity\n",
    "5. **LLM**: Generates responses using retrieved context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Tuple\n",
    "import json\n",
    "\n",
    "# Note: In production, you would use libraries like:\n",
    "# - chromadb, pinecone, or faiss for vector databases\n",
    "# - sentence-transformers for embeddings\n",
    "# - langchain or llamaindex for RAG orchestration\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Simulating a Simple RAG System\n",
    "\n",
    "Let's build a simplified RAG system to understand the core concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated company knowledge base\n",
    "knowledge_base = [\n",
    "    {\n",
    "        \"id\": \"doc1\",\n",
    "        \"title\": \"Return Policy\",\n",
    "        \"content\": \"Customers can return items within 30 days of purchase for a full refund. Items must be in original condition with tags attached. Return shipping is free for defective items.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc2\",\n",
    "        \"title\": \"Warranty Information\",\n",
    "        \"content\": \"All electronics come with a 1-year manufacturer warranty. Extended warranties up to 3 years are available for purchase. Warranty covers manufacturing defects but not accidental damage.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc3\",\n",
    "        \"title\": \"Shipping Policy\",\n",
    "        \"content\": \"Standard shipping takes 5-7 business days and costs $5.99. Express shipping takes 2-3 business days and costs $15.99. Free shipping on orders over $50.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc4\",\n",
    "        \"title\": \"Price Match Guarantee\",\n",
    "        \"content\": \"We will match any competitor's price within 14 days of purchase. Bring proof of the lower price and we'll refund the difference. Applies to identical products only.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Knowledge base loaded with {len(knowledge_base)} documents\")\n",
    "print(\"\\nSample document:\")\n",
    "print(json.dumps(knowledge_base[0], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified retrieval function (keyword-based)\n",
    "# In production, this would use semantic search with embeddings\n",
    "\n",
    "def simple_retrieve(query: str, knowledge_base: List[Dict], top_k: int = 2) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Simple keyword-based retrieval (simplified for demonstration).\n",
    "    In production, use semantic similarity with embeddings.\n",
    "    \"\"\"\n",
    "    query_words = set(query.lower().split())\n",
    "    \n",
    "    # Score documents based on keyword overlap\n",
    "    scored_docs = []\n",
    "    for doc in knowledge_base:\n",
    "        doc_words = set((doc['title'] + ' ' + doc['content']).lower().split())\n",
    "        overlap = len(query_words.intersection(doc_words))\n",
    "        scored_docs.append((overlap, doc))\n",
    "    \n",
    "    # Sort by score and return top-k\n",
    "    scored_docs.sort(reverse=True, key=lambda x: x[0])\n",
    "    return [doc for score, doc in scored_docs[:top_k]]\n",
    "\n",
    "# Test the retrieval function\n",
    "test_query = \"What is your return policy for defective items?\"\n",
    "retrieved_docs = simple_retrieve(test_query, knowledge_base, top_k=2)\n",
    "\n",
    "print(f\"Query: {test_query}\")\n",
    "print(f\"\\nRetrieved {len(retrieved_docs)} documents:\\n\")\n",
    "for i, doc in enumerate(retrieved_docs, 1):\n",
    "    print(f\"{i}. {doc['title']}\")\n",
    "    print(f\"   {doc['content'][:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG Prompt Construction\n",
    "\n",
    "def build_rag_prompt(query: str, retrieved_docs: List[Dict]) -> str:\n",
    "    \"\"\"\n",
    "    Constructs a RAG prompt by combining retrieved context with the query.\n",
    "    \"\"\"\n",
    "    context = \"\\n\\n\".join([\n",
    "        f\"Document {i+1}: {doc['title']}\\n{doc['content']}\"\n",
    "        for i, doc in enumerate(retrieved_docs)\n",
    "    ])\n",
    "    \n",
    "    prompt = f\"\"\"You are a helpful customer service assistant. Use the following information to answer the customer's question accurately.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "CUSTOMER QUESTION:\n",
    "{query}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Base your answer on the provided context\n",
    "- If the context doesn't contain the answer, say so clearly\n",
    "- Be specific and cite relevant policy details\n",
    "- Keep the response professional and helpful\n",
    "\n",
    "ANSWER:\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# Generate RAG prompt\n",
    "rag_prompt = build_rag_prompt(test_query, retrieved_docs)\n",
    "print(\"RAG PROMPT:\")\n",
    "print(\"=\"*70)\n",
    "print(rag_prompt)\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. RAG vs. Standard Prompting Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison: Standard vs. RAG approach\n",
    "\n",
    "comparison_data = {\n",
    "    'Aspect': [\n",
    "        'Knowledge Source',\n",
    "        'Accuracy',\n",
    "        'Hallucination Risk',\n",
    "        'Update Frequency',\n",
    "        'Domain Specificity',\n",
    "        'Response Verification',\n",
    "        'Implementation Complexity',\n",
    "        'Latency'\n",
    "    ],\n",
    "    'Standard Prompting': [\n",
    "        'Pre-trained knowledge only',\n",
    "        'Variable, depends on training data',\n",
    "        'Higher',\n",
    "        'Only when model is retrained',\n",
    "        'Limited to training data',\n",
    "        'Difficult to verify sources',\n",
    "        'Simple',\n",
    "        'Lower'\n",
    "    ],\n",
    "    'RAG Approach': [\n",
    "        'Pre-trained + retrieved documents',\n",
    "        'Higher with relevant docs',\n",
    "        'Lower (grounded in sources)',\n",
    "        'Real-time (update knowledge base)',\n",
    "        'High (custom knowledge base)',\n",
    "        'Can cite source documents',\n",
    "        'More complex',\n",
    "        'Higher (retrieval step)'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "print(\"Standard Prompting vs. RAG Comparison\")\n",
    "print(\"=\"*70)\n",
    "print(df_comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Introduction to Prompt Chaining\n",
    "\n",
    "### What is Prompt Chaining?\n",
    "\n",
    "Prompt chaining involves breaking down complex tasks into a sequence of simpler prompts, where the output of one prompt serves as input to the next.\n",
    "\n",
    "### Benefits of Prompt Chaining:\n",
    "\n",
    "1. **Modularity**: Each step focuses on a specific sub-task\n",
    "2. **Transparency**: Clear visibility into multi-step reasoning\n",
    "3. **Control**: Ability to intervene or modify between steps\n",
    "4. **Quality**: Better results through specialized prompts\n",
    "5. **Debugging**: Easier to identify and fix issues\n",
    "\n",
    "### Common Chaining Patterns:\n",
    "\n",
    "- **Sequential**: Linear flow (Step 1 → Step 2 → Step 3)\n",
    "- **Conditional**: Branch based on intermediate results\n",
    "- **Parallel**: Multiple chains running simultaneously\n",
    "- **Iterative**: Loops with refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Customer Feedback Analysis Chain\n",
    "\n",
    "# Sample customer feedback\n",
    "customer_feedback = \"\"\"\n",
    "I recently purchased your laptop model X500 and I'm extremely disappointed. \n",
    "The battery life is terrible - it only lasts 3 hours when you advertised 8 hours. \n",
    "The keyboard feels cheap and several keys are already sticking after just 2 weeks. \n",
    "On the positive side, the screen quality is excellent and the processing speed is good. \n",
    "Customer service was helpful when I called, but they couldn't resolve the battery issue.\n",
    "I'm considering returning it if these problems aren't fixed.\n",
    "\"\"\"\n",
    "\n",
    "print(\"CUSTOMER FEEDBACK:\")\n",
    "print(customer_feedback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Chain Design: Multi-Step Feedback Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Extract Key Issues\n",
    "\n",
    "prompt_step1 = f\"\"\"\n",
    "Analyze the following customer feedback and extract all specific issues mentioned.\n",
    "Format your response as a JSON list with the following structure:\n",
    "{{\n",
    "    \"issues\": [\n",
    "        {{\n",
    "            \"category\": \"category name\",\n",
    "            \"severity\": \"low/medium/high\",\n",
    "            \"description\": \"brief description\"\n",
    "        }}\n",
    "    ],\n",
    "    \"positive_aspects\": [\"aspect1\", \"aspect2\"]\n",
    "}}\n",
    "\n",
    "FEEDBACK:\n",
    "{customer_feedback}\n",
    "\n",
    "JSON OUTPUT:\n",
    "\"\"\"\n",
    "\n",
    "print(\"STEP 1 PROMPT: Extract Key Issues\")\n",
    "print(\"=\"*70)\n",
    "print(prompt_step1)\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated Step 1 Output\n",
    "step1_output = {\n",
    "    \"issues\": [\n",
    "        {\n",
    "            \"category\": \"Battery Performance\",\n",
    "            \"severity\": \"high\",\n",
    "            \"description\": \"Battery life only 3 hours vs advertised 8 hours\"\n",
    "        },\n",
    "        {\n",
    "            \"category\": \"Hardware Quality\",\n",
    "            \"severity\": \"medium\",\n",
    "            \"description\": \"Keyboard feels cheap and keys sticking after 2 weeks\"\n",
    "        },\n",
    "        {\n",
    "            \"category\": \"Customer Service\",\n",
    "            \"severity\": \"medium\",\n",
    "            \"description\": \"Unable to resolve battery issue\"\n",
    "        }\n",
    "    ],\n",
    "    \"positive_aspects\": [\n",
    "        \"Excellent screen quality\",\n",
    "        \"Good processing speed\",\n",
    "        \"Helpful customer service interaction\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"STEP 1 OUTPUT (Simulated LLM Response):\")\n",
    "print(json.dumps(step1_output, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Prioritize Issues\n",
    "\n",
    "prompt_step2 = f\"\"\"\n",
    "Based on the following extracted issues, prioritize them for action.\n",
    "Consider severity, customer impact, and likelihood of return/churn.\n",
    "\n",
    "EXTRACTED ISSUES:\n",
    "{json.dumps(step1_output['issues'], indent=2)}\n",
    "\n",
    "Provide:\n",
    "1. Priority ranking (1 = highest priority)\n",
    "2. Recommended action for each issue\n",
    "3. Urgency level (immediate/this week/this month)\n",
    "\n",
    "Format as JSON:\n",
    "{{\n",
    "    \"prioritized_actions\": [\n",
    "        {{\n",
    "            \"priority\": 1,\n",
    "            \"issue\": \"issue description\",\n",
    "            \"recommended_action\": \"specific action\",\n",
    "            \"urgency\": \"timeframe\",\n",
    "            \"rationale\": \"why this priority\"\n",
    "        }}\n",
    "    ]\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "print(\"STEP 2 PROMPT: Prioritize Issues\")\n",
    "print(\"=\"*70)\n",
    "print(prompt_step2)\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated Step 2 Output\n",
    "step2_output = {\n",
    "    \"prioritized_actions\": [\n",
    "        {\n",
    "            \"priority\": 1,\n",
    "            \"issue\": \"Battery Performance\",\n",
    "            \"recommended_action\": \"Immediate replacement or full refund; investigate if systemic issue with model X500\",\n",
    "            \"urgency\": \"immediate\",\n",
    "            \"rationale\": \"High severity, false advertising concern, customer considering return\"\n",
    "        },\n",
    "        {\n",
    "            \"priority\": 2,\n",
    "            \"issue\": \"Hardware Quality - Keyboard\",\n",
    "            \"recommended_action\": \"Offer keyboard replacement or device exchange; quality control review\",\n",
    "            \"urgency\": \"this week\",\n",
    "            \"rationale\": \"Indicates potential manufacturing defect, affects daily use\"\n",
    "        },\n",
    "        {\n",
    "            \"priority\": 3,\n",
    "            \"issue\": \"Customer Service Resolution\",\n",
    "            \"recommended_action\": \"Follow-up with customer service team training; empower with more resolution options\",\n",
    "            \"urgency\": \"this month\",\n",
    "            \"rationale\": \"Process improvement, prevents future escalations\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"STEP 2 OUTPUT (Simulated LLM Response):\")\n",
    "print(json.dumps(step2_output, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Draft Response Plan\n",
    "\n",
    "prompt_step3 = f\"\"\"\n",
    "Create a customer response email and internal action plan based on the following:\n",
    "\n",
    "ORIGINAL FEEDBACK:\n",
    "{customer_feedback}\n",
    "\n",
    "PRIORITIZED ACTIONS:\n",
    "{json.dumps(step2_output, indent=2)}\n",
    "\n",
    "POSITIVE ASPECTS TO ACKNOWLEDGE:\n",
    "{json.dumps(step1_output['positive_aspects'], indent=2)}\n",
    "\n",
    "Provide two outputs:\n",
    "\n",
    "1. CUSTOMER EMAIL (200-250 words):\n",
    "   - Acknowledge issues with empathy\n",
    "   - Reference positive feedback\n",
    "   - Propose concrete solutions\n",
    "   - Professional and apologetic tone\n",
    "\n",
    "2. INTERNAL ACTION PLAN:\n",
    "   - Immediate actions (next 24 hours)\n",
    "   - Short-term actions (this week)\n",
    "   - Follow-up tasks\n",
    "\"\"\"\n",
    "\n",
    "print(\"STEP 3 PROMPT: Draft Response Plan\")\n",
    "print(\"=\"*70)\n",
    "print(prompt_step3)\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain Visualization\n",
    "chain_steps = {\n",
    "    'Step': [1, 2, 3],\n",
    "    'Task': [\n",
    "        'Extract Issues & Positives',\n",
    "        'Prioritize & Recommend Actions',\n",
    "        'Draft Response & Action Plan'\n",
    "    ],\n",
    "    'Input': [\n",
    "        'Raw customer feedback',\n",
    "        'Extracted issues from Step 1',\n",
    "        'Priorities + Original feedback'\n",
    "    ],\n",
    "    'Output': [\n",
    "        'Structured JSON of issues',\n",
    "        'Prioritized action list',\n",
    "        'Email + Internal plan'\n",
    "    ],\n",
    "    'Focus': [\n",
    "        'Analysis',\n",
    "        'Decision-making',\n",
    "        'Communication'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_chain = pd.DataFrame(chain_steps)\n",
    "print(\"\\nPROMPT CHAIN WORKFLOW\")\n",
    "print(\"=\"*70)\n",
    "print(df_chain.to_string(index=False))\n",
    "print(\"\\nBenefits of this chain:\")\n",
    "print(\"- Each step has a clear, focused objective\")\n",
    "print(\"- Intermediate outputs can be validated\")\n",
    "print(\"- Easy to modify individual steps without rewriting entire prompt\")\n",
    "print(\"- Structured data flows naturally between steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Combining RAG and Prompt Chaining\n",
    "\n",
    "The real power comes from combining RAG with prompt chaining for complex business workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Policy-Compliant Customer Service Workflow\n",
    "\n",
    "def rag_chain_workflow(customer_query: str, knowledge_base: List[Dict]) -> Dict:\n",
    "    \"\"\"\n",
    "    Combines RAG and prompt chaining for customer service.\n",
    "    \n",
    "    Chain:\n",
    "    1. Retrieve relevant policies (RAG)\n",
    "    2. Analyze customer intent\n",
    "    3. Generate policy-compliant response\n",
    "    4. Add empathy and personalization\n",
    "    \"\"\"\n",
    "    \n",
    "    workflow = {\n",
    "        \"query\": customer_query,\n",
    "        \"steps\": []\n",
    "    }\n",
    "    \n",
    "    # Step 1: RAG - Retrieve relevant policies\n",
    "    retrieved_docs = simple_retrieve(customer_query, knowledge_base, top_k=2)\n",
    "    workflow[\"steps\"].append({\n",
    "        \"step\": 1,\n",
    "        \"name\": \"Retrieve Policies\",\n",
    "        \"docs_retrieved\": [doc['title'] for doc in retrieved_docs]\n",
    "    })\n",
    "    \n",
    "    # Step 2: Analyze intent (would be LLM call)\n",
    "    intent_prompt = f\"\"\"Analyze customer intent from: {customer_query}\n",
    "    Classify as: refund_request, warranty_question, shipping_inquiry, price_match, or other.\"\"\"\n",
    "    \n",
    "    workflow[\"steps\"].append({\n",
    "        \"step\": 2,\n",
    "        \"name\": \"Analyze Intent\",\n",
    "        \"prompt\": intent_prompt[:100] + \"...\"\n",
    "    })\n",
    "    \n",
    "    # Step 3: Generate policy-compliant response\n",
    "    response_prompt = build_rag_prompt(customer_query, retrieved_docs)\n",
    "    workflow[\"steps\"].append({\n",
    "        \"step\": 3,\n",
    "        \"name\": \"Generate Response\",\n",
    "        \"prompt_length\": len(response_prompt)\n",
    "    })\n",
    "    \n",
    "    # Step 4: Add empathy layer\n",
    "    empathy_prompt = \"\"\"Review the following customer service response and enhance it with:\n",
    "    - Empathetic language\n",
    "    - Personal touches\n",
    "    - Professional warmth\n",
    "    While maintaining all policy-accurate information.\"\"\"\n",
    "    \n",
    "    workflow[\"steps\"].append({\n",
    "        \"step\": 4,\n",
    "        \"name\": \"Add Empathy\",\n",
    "        \"prompt\": empathy_prompt[:100] + \"...\"\n",
    "    })\n",
    "    \n",
    "    return workflow\n",
    "\n",
    "# Test the workflow\n",
    "test_customer_query = \"I bought a laptop 3 weeks ago but it has a defect. Can I return it?\"\n",
    "workflow_result = rag_chain_workflow(test_customer_query, knowledge_base)\n",
    "\n",
    "print(\"RAG + CHAIN WORKFLOW EXAMPLE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Customer Query: {workflow_result['query']}\\n\")\n",
    "print(\"Workflow Steps:\")\n",
    "for step_info in workflow_result['steps']:\n",
    "    print(f\"\\nStep {step_info['step']}: {step_info['name']}\")\n",
    "    for key, value in step_info.items():\n",
    "        if key not in ['step', 'name']:\n",
    "            print(f\"  - {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Practical Business Applications\n",
    "\n",
    "### Use Cases for RAG + Prompt Chaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Business applications matrix\n",
    "use_cases = {\n",
    "    'Business Function': [\n",
    "        'Customer Support',\n",
    "        'Legal/Compliance',\n",
    "        'Sales Enablement',\n",
    "        'HR/Recruiting',\n",
    "        'Product Development',\n",
    "        'Financial Analysis'\n",
    "    ],\n",
    "    'RAG Component': [\n",
    "        'FAQs, policies, product docs',\n",
    "        'Regulations, case law, contracts',\n",
    "        'Product specs, case studies, pricing',\n",
    "        'Job descriptions, benefits, policies',\n",
    "        'Customer feedback, market research',\n",
    "        'Financial statements, market data'\n",
    "    ],\n",
    "    'Chain Workflow': [\n",
    "        'Analyze → Retrieve Policy → Draft Response → Quality Check',\n",
    "        'Extract Terms → Check Compliance → Risk Assessment → Report',\n",
    "        'Qualify Lead → Retrieve Materials → Customize Pitch → Follow-up',\n",
    "        'Screen Resume → Match Requirements → Schedule → Notify',\n",
    "        'Aggregate Feedback → Identify Themes → Prioritize → Roadmap',\n",
    "        'Retrieve Data → Calculate Metrics → Trend Analysis → Insights'\n",
    "    ],\n",
    "    'Business Value': [\n",
    "        'Faster, consistent responses',\n",
    "        'Reduced risk, audit trails',\n",
    "        'Higher conversion rates',\n",
    "        'Improved candidate experience',\n",
    "        'Data-driven decisions',\n",
    "        'Faster insights, accuracy'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_use_cases = pd.DataFrame(use_cases)\n",
    "print(\"\\nRAG + PROMPT CHAINING: BUSINESS APPLICATIONS\")\n",
    "print(\"=\"*70)\n",
    "for idx, row in df_use_cases.iterrows():\n",
    "    print(f\"\\n{row['Business Function']}:\")\n",
    "    print(f\"  Knowledge Base: {row['RAG Component']}\")\n",
    "    print(f\"  Workflow: {row['Chain Workflow']}\")\n",
    "    print(f\"  Value: {row['Business Value']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Implementation Challenges and Considerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenges and mitigation strategies\n",
    "challenges = {\n",
    "    'Challenge': [\n",
    "        'Retrieval Quality',\n",
    "        'Context Window Limits',\n",
    "        'Chain Complexity',\n",
    "        'Error Propagation',\n",
    "        'Latency/Performance',\n",
    "        'Cost Management',\n",
    "        'Knowledge Base Maintenance'\n",
    "    ],\n",
    "    'Impact': [\n",
    "        'Irrelevant or missing information',\n",
    "        'Cannot fit all retrieved docs',\n",
    "        'Difficult to debug and maintain',\n",
    "        'Early errors affect final output',\n",
    "        'Multiple LLM calls increase latency',\n",
    "        'Each chain step costs money',\n",
    "        'Outdated/incorrect information'\n",
    "    ],\n",
    "    'Mitigation Strategy': [\n",
    "        'Use semantic embeddings; tune retrieval parameters; hybrid search',\n",
    "        'Rank and select most relevant chunks; use summarization',\n",
    "        'Modular design; clear documentation; monitoring each step',\n",
    "        'Validation checkpoints; confidence scoring; human-in-loop',\n",
    "        'Parallel chains where possible; caching; optimize prompts',\n",
    "        'Monitor token usage; optimize chain length; batch processing',\n",
    "        'Automated updates; version control; regular audits'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_challenges = pd.DataFrame(challenges)\n",
    "print(\"\\nIMPLEMENTATION CHALLENGES & MITIGATION\")\n",
    "print(\"=\"*70)\n",
    "for idx, row in df_challenges.iterrows():\n",
    "    print(f\"\\n{idx + 1}. {row['Challenge']}\")\n",
    "    print(f\"   Impact: {row['Impact']}\")\n",
    "    print(f\"   Mitigation: {row['Mitigation Strategy']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Hands-On Practice Activity\n",
    "\n",
    "### Design Your Own RAG + Chain Workflow\n",
    "\n",
    "Choose a business process from your organization that requires:\n",
    "1. Access to specific knowledge/documentation\n",
    "2. Multiple processing steps\n",
    "3. A final output or action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR TURN: Define your business process\n",
    "\n",
    "my_business_process = \"\"\"\n",
    "Business Process Description:\n",
    "[Describe your chosen business process]\n",
    "\n",
    "Example: Analyzing contract proposals for compliance and risk\n",
    "\"\"\"\n",
    "\n",
    "my_knowledge_sources = \"\"\"\n",
    "Knowledge Sources (for RAG):\n",
    "[List the documents/data sources needed]\n",
    "\n",
    "Example: \n",
    "- Company contract templates\n",
    "- Legal compliance requirements\n",
    "- Industry standard terms\n",
    "- Historical contract issues\n",
    "\"\"\"\n",
    "\n",
    "my_chain_design = \"\"\"\n",
    "Chain Design (list steps):\n",
    "\n",
    "Step 1: [Task]\n",
    "  Input: \n",
    "  Output:\n",
    "\n",
    "Step 2: [Task]\n",
    "  Input:\n",
    "  Output:\n",
    "\n",
    "Step 3: [Task]\n",
    "  Input:\n",
    "  Output:\n",
    "\"\"\"\n",
    "\n",
    "print(my_business_process)\n",
    "print(my_knowledge_sources)\n",
    "print(my_chain_design)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR TURN: Write prompts for each chain step\n",
    "\n",
    "my_step1_prompt = \"\"\"\n",
    "[Write your Step 1 prompt here]\n",
    "\"\"\"\n",
    "\n",
    "my_step2_prompt = \"\"\"\n",
    "[Write your Step 2 prompt here]\n",
    "\"\"\"\n",
    "\n",
    "my_step3_prompt = \"\"\"\n",
    "[Write your Step 3 prompt here]\n",
    "\"\"\"\n",
    "\n",
    "print(\"Step 1 Prompt:\")\n",
    "print(my_step1_prompt)\n",
    "print(\"\\nStep 2 Prompt:\")\n",
    "print(my_step2_prompt)\n",
    "print(\"\\nStep 3 Prompt:\")\n",
    "print(my_step3_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Discussion Questions\n",
    "\n",
    "Consider and discuss the following:\n",
    "\n",
    "1. **RAG Application**: Consider a business process requiring information retrieval and subsequent action (e.g., summarizing recent customer feedback and drafting a response plan). How could a combination of RAG and prompt chaining automate or assist this?\n",
    "\n",
    "2. **Knowledge Base Design**: What are the key considerations when designing a knowledge base for RAG? How do you ensure information quality and relevance?\n",
    "\n",
    "3. **Chain Optimization**: When would you choose a longer, more detailed chain versus a shorter, simpler one? What are the trade-offs?\n",
    "\n",
    "4. **Error Handling**: In a multi-step chain, how would you handle situations where:\n",
    "   - Retrieval returns no relevant documents?\n",
    "   - An intermediate step produces poor output?\n",
    "   - The final output doesn't meet quality standards?\n",
    "\n",
    "5. **Human-in-the-Loop**: Which steps in your designed workflow would benefit most from human review or intervention? Why?\n",
    "\n",
    "6. **Measurement**: How would you measure the success of a RAG + chain implementation? What metrics matter most for your use case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Reflections:\n",
    "\n",
    "**Question 1 - RAG Application:**\n",
    "\n",
    "[Your response]\n",
    "\n",
    "**Question 2 - Knowledge Base Design:**\n",
    "\n",
    "[Your response]\n",
    "\n",
    "**Question 3 - Chain Optimization:**\n",
    "\n",
    "[Your response]\n",
    "\n",
    "**Question 4 - Error Handling:**\n",
    "\n",
    "[Your response]\n",
    "\n",
    "**Question 5 - Human-in-the-Loop:**\n",
    "\n",
    "[Your response]\n",
    "\n",
    "**Question 6 - Measurement:**\n",
    "\n",
    "[Your response]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Key Takeaways\n",
    "\n",
    "1. **RAG grounds LLMs in specific knowledge**, reducing hallucinations and enabling access to proprietary or current information\n",
    "\n",
    "2. **Prompt chaining breaks complex tasks into manageable steps**, improving quality, transparency, and control\n",
    "\n",
    "3. **Combining RAG and chaining creates powerful workflows** for complex business processes requiring both knowledge retrieval and multi-step reasoning\n",
    "\n",
    "4. **Implementation requires careful consideration** of retrieval quality, chain design, error handling, and performance\n",
    "\n",
    "5. **Different business functions benefit differently** - identify where document grounding and structured workflows add most value\n",
    "\n",
    "6. **Maintenance is ongoing** - knowledge bases need updates, chains need optimization, and workflows need monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Looking Ahead to Week 5\n",
    "\n",
    "Next week, we'll focus on **Evaluating LLM Outputs: Metrics and Frameworks**.\n",
    "\n",
    "We'll explore:\n",
    "- Key evaluation metrics (BLEU, ROUGE, perplexity, F1-score)\n",
    "- Frameworks for assessing relevance, coherence, fluency, safety, and bias\n",
    "- Business-specific evaluation criteria\n",
    "- Quantitative and qualitative assessment methods\n",
    "\n",
    "**Preparation:** Consider how you would evaluate the quality of outputs from the RAG and chain systems we discussed this week. What criteria matter most for your use cases?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "### RAG Resources:\n",
    "- [LangChain RAG Documentation](https://python.langchain.com/docs/use_cases/question_answering/)\n",
    "- [Building RAG Systems with LlamaIndex](https://docs.llamaindex.ai/en/stable/)\n",
    "- [Pinecone Vector Database Guide](https://www.pinecone.io/learn/vector-database/)\n",
    "\n",
    "### Prompt Chaining Resources:\n",
    "- [LangChain Sequential Chains](https://python.langchain.com/docs/modules/chains/)\n",
    "- [OpenAI Prompt Chaining Guide](https://platform.openai.com/docs/guides/prompt-engineering)\n",
    "- [Anthropic Claude Chaining Patterns](https://docs.anthropic.com/claude/docs/)\n",
    "\n",
    "### Tools and Libraries:\n",
    "- **Vector Databases**: Pinecone, Weaviate, ChromaDB, Faiss\n",
    "- **Embedding Models**: OpenAI Embeddings, Sentence-Transformers\n",
    "- **Orchestration**: LangChain, LlamaIndex, Haystack\n",
    "\n",
    "---\n",
    "\n",
    "*End of Week 4 Notebook*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
