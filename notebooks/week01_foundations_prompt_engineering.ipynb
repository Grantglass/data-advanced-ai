{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1: Foundations of Effective Prompt Engineering\n",
    "\n",
    "## MBA 590 - Advanced AI Strategy: Prompting and Agentic Frameworks\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This week focuses on reviewing core principles of prompt engineering, understanding LLM input/output processing, and mastering the fundamentals of context, specificity, and clarity in prompt design. We'll introduce basic prompt types including zero-shot and role-playing prompts.\n",
    "\n",
    "### Key Topics\n",
    "- Core principles of prompt engineering\n",
    "- LLM input/output processing\n",
    "- Context, specificity, and clarity in prompt design\n",
    "- Basic prompt types: zero-shot, role-playing, simple instruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this week, you will be able to:\n",
    "\n",
    "1. Understand the fundamental principles of effective prompt engineering\n",
    "2. Recognize how LLMs process inputs and generate outputs\n",
    "3. Apply context, specificity, and clarity to improve prompt quality\n",
    "4. Differentiate between zero-shot, role-playing, and simple instruction prompts\n",
    "5. Evaluate the strengths and weaknesses of different prompting approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Academic Readings\n",
    "\n",
    "1. **Fuentealba Cid, D., Flores-Fernández, C., & Aguilera Eguía, R. (2024).** *The art of prompts' formulation: limitations, potential, and practical examples in large language models.* Salud, Ciencia y Tecnología, 4, 969.\n",
    "\n",
    "2. **Johnson, S., & Hyland-Wood, D. (2024).** *A Primer on Large Language Models and their Limitations.* arXiv preprint arXiv:2412.04503."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Core Principles of Prompt Engineering\n",
    "\n",
    "### Key Principles:\n",
    "\n",
    "1. **Clarity**: Make your instructions explicit and unambiguous\n",
    "2. **Context**: Provide relevant background information\n",
    "3. **Specificity**: Be precise about what you want\n",
    "4. **Structure**: Organize prompts logically\n",
    "5. **Examples**: Use examples when appropriate (we'll explore this more in Week 2)\n",
    "\n",
    "### Understanding LLM Processing\n",
    "\n",
    "Large Language Models (LLMs) work by:\n",
    "- Tokenizing input text\n",
    "- Processing tokens through neural network layers\n",
    "- Predicting the most likely next tokens\n",
    "- Generating coherent responses based on training data patterns\n",
    "\n",
    "**Key Limitation**: LLMs don't \"understand\" in a human sense; they recognize patterns and generate statistically likely continuations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Types of Basic Prompts\n",
    "\n",
    "### A. Zero-Shot Prompts\n",
    "\n",
    "Zero-shot prompts provide a direct instruction without examples.\n",
    "\n",
    "**Characteristics:**\n",
    "- No examples provided\n",
    "- Relies on model's pre-existing knowledge\n",
    "- Simple and straightforward\n",
    "\n",
    "### B. Role-Playing Prompts\n",
    "\n",
    "Role-playing prompts assign a specific role or persona to the LLM.\n",
    "\n",
    "**Characteristics:**\n",
    "- Sets context through role assignment\n",
    "- Can improve response quality and consistency\n",
    "- Useful for specialized tasks\n",
    "\n",
    "### C. Simple Instruction Prompts\n",
    "\n",
    "Direct, task-oriented prompts without role-playing.\n",
    "\n",
    "**Characteristics:**\n",
    "- Clear task specification\n",
    "- Minimal context\n",
    "- Efficient for straightforward tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Practical Exercise: Prompt Comparison\n",
    "\n",
    "Let's demonstrate different prompting approaches for a common business task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Installing required libraries\n",
    "# Note: In practice, you would use an actual LLM API (OpenAI, Anthropic, etc.)\n",
    "# This example demonstrates the prompt structure\n",
    "\n",
    "import json\n",
    "from typing import Dict, List\n",
    "\n",
    "# Business Task: Drafting a customer service email response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Zero-Shot Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_prompt = \"\"\"\n",
    "Write an email response to a customer who received a damaged product.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Zero-Shot Prompt:\")\n",
    "print(zero_shot_prompt)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Strengths:\n",
    "strengths_zero_shot = [\n",
    "    \"Simple and quick to create\",\n",
    "    \"Works well for common tasks\",\n",
    "    \"No need for extensive context\"\n",
    "]\n",
    "\n",
    "# Weaknesses:\n",
    "weaknesses_zero_shot = [\n",
    "    \"May lack company-specific tone\",\n",
    "    \"Might miss important policy details\",\n",
    "    \"Generic response without specific guidance\"\n",
    "]\n",
    "\n",
    "print(\"Strengths:\", \"\\n- \" + \"\\n- \".join(strengths_zero_shot))\n",
    "print(\"\\nWeaknesses:\", \"\\n- \" + \"\\n- \".join(weaknesses_zero_shot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Role-Playing Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role_playing_prompt = \"\"\"\n",
    "You are a senior customer service representative at a premium e-commerce company \n",
    "known for exceptional customer care. You always maintain a warm, empathetic tone \n",
    "while being professional and solution-oriented.\n",
    "\n",
    "Write an email response to a customer who received a damaged product. The customer \n",
    "is understandably frustrated but has been polite in their complaint.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Role-Playing Prompt:\")\n",
    "print(role_playing_prompt)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "strengths_role = [\n",
    "    \"Establishes appropriate tone and style\",\n",
    "    \"Provides context about company values\",\n",
    "    \"More likely to generate brand-aligned responses\",\n",
    "    \"Adds empathy and professionalism\"\n",
    "]\n",
    "\n",
    "weaknesses_role = [\n",
    "    \"Longer prompt requires more tokens\",\n",
    "    \"May still lack specific policy guidance\",\n",
    "    \"Requires careful role definition\"\n",
    "]\n",
    "\n",
    "print(\"Strengths:\", \"\\n- \" + \"\\n- \".join(strengths_role))\n",
    "print(\"\\nWeaknesses:\", \"\\n- \" + \"\\n- \".join(weaknesses_role))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Detailed Instruction Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detailed_instruction_prompt = \"\"\"\n",
    "Write a customer service email response with the following requirements:\n",
    "\n",
    "Situation: Customer received a damaged product\n",
    "\n",
    "Instructions:\n",
    "1. Acknowledge the issue and apologize\n",
    "2. Express empathy for the inconvenience\n",
    "3. Offer two solutions: full refund or replacement shipment\n",
    "4. Mention that return shipping will be covered\n",
    "5. Provide a direct contact number for urgent assistance\n",
    "6. Thank them for their patience\n",
    "\n",
    "Tone: Professional, warm, and solution-focused\n",
    "Length: 150-200 words\n",
    "\"\"\"\n",
    "\n",
    "print(\"Detailed Instruction Prompt:\")\n",
    "print(detailed_instruction_prompt)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "strengths_detailed = [\n",
    "    \"Very specific guidance on content\",\n",
    "    \"Includes policy-specific information\",\n",
    "    \"Structured approach ensures completeness\",\n",
    "    \"Clear expectations for tone and length\"\n",
    "]\n",
    "\n",
    "weaknesses_detailed = [\n",
    "    \"Takes more time to create\",\n",
    "    \"Longest prompt (uses more tokens)\",\n",
    "    \"May feel formulaic if overused\",\n",
    "    \"Less flexible for variations\"\n",
    "]\n",
    "\n",
    "print(\"Strengths:\", \"\\n- \" + \"\\n- \".join(strengths_detailed))\n",
    "print(\"\\nWeaknesses:\", \"\\n- \" + \"\\n- \".join(weaknesses_detailed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prompt Evaluation Framework\n",
    "\n",
    "When evaluating prompts, consider these dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a comparison matrix\n",
    "evaluation_criteria = {\n",
    "    'Criteria': [\n",
    "        'Clarity',\n",
    "        'Specificity',\n",
    "        'Context Provision',\n",
    "        'Token Efficiency',\n",
    "        'Ease of Creation',\n",
    "        'Output Consistency',\n",
    "        'Business Alignment'\n",
    "    ],\n",
    "    'Zero-Shot': [3, 2, 1, 5, 5, 2, 2],\n",
    "    'Role-Playing': [4, 3, 4, 3, 3, 4, 4],\n",
    "    'Detailed Instruction': [5, 5, 5, 2, 2, 5, 5]\n",
    "}\n",
    "\n",
    "df_evaluation = pd.DataFrame(evaluation_criteria)\n",
    "print(\"Prompt Type Comparison (Scale: 1-5, where 5 is best)\")\n",
    "print(\"=\"*60)\n",
    "print(df_evaluation.to_string(index=False))\n",
    "print(\"\\nNote: The 'best' approach depends on your specific use case and constraints.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Best Practices for Effective Prompts\n",
    "\n",
    "### Do's:\n",
    "- ✓ Be specific about what you want\n",
    "- ✓ Provide relevant context\n",
    "- ✓ Use clear, unambiguous language\n",
    "- ✓ Specify output format when needed\n",
    "- ✓ Include constraints (length, tone, style)\n",
    "- ✓ Test and iterate on your prompts\n",
    "\n",
    "### Don'ts:\n",
    "- ✗ Use vague or ambiguous instructions\n",
    "- ✗ Assume the model has real-time information\n",
    "- ✗ Expect perfect outputs without iteration\n",
    "- ✗ Overload with unnecessary information\n",
    "- ✗ Use contradictory instructions\n",
    "- ✗ Ignore the importance of formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hands-On Practice Activity\n",
    "\n",
    "### Task: Create Three Different Prompts\n",
    "\n",
    "Select a common business task from your field (examples below) and create three different prompts:\n",
    "1. A zero-shot prompt\n",
    "2. A role-playing prompt\n",
    "3. A detailed instruction prompt\n",
    "\n",
    "**Business Task Examples:**\n",
    "- Summarizing a quarterly report\n",
    "- Drafting a meeting agenda\n",
    "- Creating a project status update\n",
    "- Writing a product description\n",
    "- Analyzing customer feedback\n",
    "\n",
    "Use the cells below to create your prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR TURN: Define your business task\n",
    "my_business_task = \"\"\"\n",
    "Description of your chosen business task:\n",
    "\n",
    "[Write your task description here]\n",
    "\"\"\"\n",
    "\n",
    "print(my_business_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR TURN: Create a zero-shot prompt\n",
    "my_zero_shot_prompt = \"\"\"\n",
    "[Write your zero-shot prompt here]\n",
    "\"\"\"\n",
    "\n",
    "print(\"My Zero-Shot Prompt:\")\n",
    "print(my_zero_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR TURN: Create a role-playing prompt\n",
    "my_role_playing_prompt = \"\"\"\n",
    "[Write your role-playing prompt here]\n",
    "\"\"\"\n",
    "\n",
    "print(\"My Role-Playing Prompt:\")\n",
    "print(my_role_playing_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR TURN: Create a detailed instruction prompt\n",
    "my_detailed_prompt = \"\"\"\n",
    "[Write your detailed instruction prompt here]\n",
    "\"\"\"\n",
    "\n",
    "print(\"My Detailed Instruction Prompt:\")\n",
    "print(my_detailed_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Self-Evaluation Exercise\n",
    "\n",
    "Evaluate your three prompts using the framework below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate your prompts (rate 1-5 for each criteria)\n",
    "my_evaluation = {\n",
    "    'Criteria': [\n",
    "        'Clarity',\n",
    "        'Specificity',\n",
    "        'Context Provision',\n",
    "        'Expected Output Quality',\n",
    "        'Ease of Use'\n",
    "    ],\n",
    "    'My Zero-Shot': [0, 0, 0, 0, 0],  # Replace 0s with your ratings\n",
    "    'My Role-Playing': [0, 0, 0, 0, 0],  # Replace 0s with your ratings\n",
    "    'My Detailed': [0, 0, 0, 0, 0]  # Replace 0s with your ratings\n",
    "}\n",
    "\n",
    "df_my_eval = pd.DataFrame(my_evaluation)\n",
    "print(\"My Prompt Evaluation:\")\n",
    "print(df_my_eval.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Discussion Questions\n",
    "\n",
    "Reflect on the following questions:\n",
    "\n",
    "1. **Which prompt style (zero-shot, role-playing, or detailed instruction) do you think would work best for your chosen business task? Why?**\n",
    "\n",
    "2. **What are the potential strengths of each prompting approach?**\n",
    "\n",
    "3. **What are the potential weaknesses or limitations of each approach?**\n",
    "\n",
    "4. **How might the choice of prompt style impact:**\n",
    "   - Output quality\n",
    "   - Consistency across multiple uses\n",
    "   - Time invested in prompt creation\n",
    "   - Cost (token usage)\n",
    "\n",
    "5. **In what business scenarios would you prioritize:**\n",
    "   - Speed and simplicity (zero-shot)?\n",
    "   - Tone and brand alignment (role-playing)?\n",
    "   - Precision and control (detailed instruction)?\n",
    "\n",
    "Use the cell below to write your reflections:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My Reflections:\n",
    "\n",
    "[Write your thoughts here]\n",
    "\n",
    "**Question 1:**\n",
    "\n",
    "**Question 2:**\n",
    "\n",
    "**Question 3:**\n",
    "\n",
    "**Question 4:**\n",
    "\n",
    "**Question 5:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Key Takeaways\n",
    "\n",
    "1. **Prompt engineering is foundational** to effectively using LLMs in business contexts\n",
    "\n",
    "2. **Different prompt types serve different purposes:**\n",
    "   - Zero-shot: Quick, simple tasks\n",
    "   - Role-playing: Tone and style consistency\n",
    "   - Detailed instruction: Precision and control\n",
    "\n",
    "3. **Effective prompts balance:**\n",
    "   - Clarity and specificity\n",
    "   - Context and brevity\n",
    "   - Flexibility and control\n",
    "\n",
    "4. **Iteration is essential** - prompts should be tested and refined\n",
    "\n",
    "5. **Context matters** - understanding how LLMs process information helps create better prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Looking Ahead to Week 2\n",
    "\n",
    "Next week, we'll explore advanced prompting techniques:\n",
    "- Few-shot learning (providing examples)\n",
    "- Chain-of-Thought (CoT) prompting for complex reasoning\n",
    "- Self-Ask techniques for problem decomposition\n",
    "\n",
    "**Preparation:** Think about a complex decision-making scenario in your field that requires multi-step reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "- [OpenAI Prompt Engineering Guide](https://platform.openai.com/docs/guides/prompt-engineering)\n",
    "- [Anthropic Prompt Engineering Documentation](https://docs.anthropic.com/claude/docs/prompt-engineering)\n",
    "- [Google's Introduction to Prompt Design](https://developers.google.com/machine-learning/resources/prompt-eng)\n",
    "\n",
    "---\n",
    "\n",
    "*End of Week 1 Notebook*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
