{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 12: Technology Governance & Ethics II - Regulation & Implementation\n",
    "\n",
    "## MBA 590 - Advanced AI Strategy: Prompting and Agentic Frameworks\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "Building on last week's foundation of ethical frameworks and principles, this week focuses on the practical aspects of implementing governance: understanding regulatory requirements, conducting bias audits, implementing privacy controls, and establishing human oversight mechanisms. We'll examine real-world regulatory landscapes and translate compliance requirements into operational practices.\n",
    "\n",
    "### Key Topics\n",
    "- Global AI regulation landscape (EU AI Act, GDPR, US approaches)\n",
    "- Implementing bias audits and fairness testing\n",
    "- Transparency reporting and documentation\n",
    "- Data privacy controls and security protocols\n",
    "- Human oversight and human-in-the-loop design\n",
    "- Compliance monitoring and auditing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this week, you will be able to:\n",
    "\n",
    "1. Navigate key regulatory requirements for AI systems globally\n",
    "2. Conduct bias audits and implement fairness testing\n",
    "3. Design transparency reporting mechanisms\n",
    "4. Implement data privacy and security controls\n",
    "5. Establish effective human oversight mechanisms\n",
    "6. Create compliance monitoring systems\n",
    "7. Develop incident response protocols for AI systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Academic Readings\n",
    "\n",
    "1. **European Parliament. (2024).** *EU AI Act: First Regulation on Artificial Intelligence.* (Focus on risk categories and business implications)\n",
    "\n",
    "2. **Rajpurkar, P., Chen, E., Banerjee, O., & Topol, E. J. (2022).** *AI in health and medicine.* Nature Medicine, 28(1), 31-38. (Discusses regulatory and ethical challenges in a specific high-stakes domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Tuple\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Set style for visualizations\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('Set2')\n",
    "\n",
    "print('Libraries imported successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Global AI Regulation Landscape\n",
    "\n",
    "### A. European Union: AI Act\n",
    "\n",
    "**World's first comprehensive AI regulation** (enacted 2024)\n",
    "\n",
    "**Risk-Based Approach**:\n",
    "\n",
    "**Unacceptable Risk** (Banned):\n",
    "- Social scoring by governments\n",
    "- Subliminal manipulation causing harm\n",
    "- Exploitation of vulnerable groups\n",
    "- Real-time biometric identification in public (limited exceptions)\n",
    "\n",
    "**High Risk** (Strict Requirements):\n",
    "- Biometric identification\n",
    "- Critical infrastructure\n",
    "- Education and employment\n",
    "- Essential services (credit scoring, insurance)\n",
    "- Law enforcement\n",
    "- Migration and border management\n",
    "- Justice administration\n",
    "\n",
    "**Requirements for High-Risk Systems**:\n",
    "- Risk assessment and mitigation\n",
    "- High-quality training data\n",
    "- Activity logging and traceability\n",
    "- Transparency and user information\n",
    "- Human oversight measures\n",
    "- Accuracy, robustness, cybersecurity\n",
    "\n",
    "**Penalties**: Up to €35M or 7% of global revenue\n",
    "\n",
    "### B. GDPR (Still Applies)\n",
    "\n",
    "**Key Provisions for AI**:\n",
    "- Right to explanation for automated decisions\n",
    "- Data minimization principles\n",
    "- Purpose limitation\n",
    "- Consent requirements\n",
    "- Right to be forgotten\n",
    "- Data protection impact assessments (DPIA)\n",
    "\n",
    "### C. United States Approach\n",
    "\n",
    "**Federal Level**:\n",
    "- No comprehensive federal AI law (yet)\n",
    "- Executive Order on Safe, Secure AI (Oct 2023)\n",
    "- NIST AI RMF (voluntary)\n",
    "- Sector-specific regulations (finance, healthcare, etc.)\n",
    "\n",
    "**State Level**:\n",
    "- California: CCPA/CPRA (privacy)\n",
    "- Colorado AI Act (enacted 2024)\n",
    "- New York City: Automated Employment Decision Tools law\n",
    "- Illinois: Biometric Information Privacy Act\n",
    "\n",
    "### D. Other Jurisdictions\n",
    "\n",
    "**China**: \n",
    "- Algorithm Recommendation Regulations\n",
    "- Deep Synthesis Regulations\n",
    "- Focus on content control and social stability\n",
    "\n",
    "**Canada**: \n",
    "- Artificial Intelligence and Data Act (AIDA) - proposed\n",
    "- Risk-based approach similar to EU\n",
    "\n",
    "**UK**: \n",
    "- Sector-specific approach\n",
    "- Focus on existing regulators adapting to AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regulatory landscape comparison\n",
    "\n",
    "regulations = {\n",
    "    'Jurisdiction': ['EU', 'EU', 'US Federal', 'US State', 'China', 'Canada', 'UK'],\n",
    "    'Regulation': [\n",
    "        'AI Act',\n",
    "        'GDPR',\n",
    "        'Executive Order',\n",
    "        'State Laws (Various)',\n",
    "        'Algorithm Regulations',\n",
    "        'AIDA (Proposed)',\n",
    "        'Sector Approach'\n",
    "    ],\n",
    "    'Status': ['Enacted', 'Enacted', 'Active', 'Varies', 'Enacted', 'Proposed', 'Developing'],\n",
    "    'Scope': ['Comprehensive', 'Data Privacy', 'High-Risk AI', 'Specific Issues', 'Content Control', 'Comprehensive', 'Sector-Specific'],\n",
    "    'Approach': ['Risk-Based', 'Rights-Based', 'Voluntary+Sector', 'Patchwork', 'Control-Based', 'Risk-Based', 'Adaptive'],\n",
    "    'Penalties': ['€35M or 7%', '€20M or 4%', 'Varies', 'Varies', 'Severe', 'TBD', 'Sector-dependent'],\n",
    "    'Business_Impact': ['Very High', 'Very High', 'Medium', 'Medium', 'High', 'High', 'Medium']\n",
    "}\n",
    "\n",
    "df_regulations = pd.DataFrame(regulations)\n",
    "\n",
    "print(\"GLOBAL AI REGULATION LANDSCAPE\")\n",
    "print(\"=\"*80)\n",
    "print(df_regulations.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY INSIGHTS:\")\n",
    "print(\"- EU has most comprehensive AI-specific regulation\")\n",
    "print(\"- US approach is fragmented across federal/state/sector\")\n",
    "print(\"- GDPR remains critical for data privacy in AI systems\")\n",
    "print(\"- Global operations require multi-jurisdiction compliance strategy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compliance requirement mapping tool\n",
    "\n",
    "def assess_compliance_requirements(use_case: str,\n",
    "                                  jurisdictions: List[str],\n",
    "                                  processes_personal_data: bool,\n",
    "                                  high_risk_use: bool,\n",
    "                                  automated_decisions: bool) -> Dict:\n",
    "    \"\"\"\n",
    "    Assess regulatory compliance requirements for an AI system.\n",
    "    \n",
    "    Parameters:\n",
    "    - use_case: Description of the AI system\n",
    "    - jurisdictions: List of jurisdictions (e.g., ['EU', 'US', 'China'])\n",
    "    - processes_personal_data: Whether system processes personal data\n",
    "    - high_risk_use: Whether use case falls into high-risk categories\n",
    "    - automated_decisions: Whether system makes automated decisions about individuals\n",
    "    \"\"\"\n",
    "    requirements = []\n",
    "    \n",
    "    # EU requirements\n",
    "    if 'EU' in jurisdictions:\n",
    "        if high_risk_use:\n",
    "            requirements.extend([\n",
    "                'EU AI Act: High-risk system requirements',\n",
    "                'EU AI Act: Conformity assessment',\n",
    "                'EU AI Act: CE marking',\n",
    "                'EU AI Act: Registration in EU database'\n",
    "            ])\n",
    "        if processes_personal_data:\n",
    "            requirements.extend([\n",
    "                'GDPR: Data Protection Impact Assessment',\n",
    "                'GDPR: Legal basis for processing',\n",
    "                'GDPR: Data subject rights (access, deletion, etc.)'\n",
    "            ])\n",
    "        if automated_decisions:\n",
    "            requirements.append('GDPR: Right to explanation for automated decisions')\n",
    "    \n",
    "    # US requirements\n",
    "    if 'US' in jurisdictions:\n",
    "        if processes_personal_data:\n",
    "            requirements.extend([\n",
    "                'US: Review state privacy laws (CCPA, CPRA, etc.)',\n",
    "                'US: Sector-specific regulations (HIPAA, FCRA, etc.)'\n",
    "            ])\n",
    "        if use_case.lower().__contains__('employ'):\n",
    "            requirements.append('US: NYC Automated Employment Decision Tools law (if applicable)')\n",
    "    \n",
    "    # China requirements\n",
    "    if 'China' in jurisdictions:\n",
    "        requirements.extend([\n",
    "            'China: Algorithm filing requirements',\n",
    "            'China: Content moderation obligations'\n",
    "        ])\n",
    "    \n",
    "    # Universal requirements\n",
    "    if high_risk_use or automated_decisions:\n",
    "        requirements.extend([\n",
    "            'Best Practice: Bias testing and fairness audits',\n",
    "            'Best Practice: Model documentation and transparency',\n",
    "            'Best Practice: Human oversight mechanisms'\n",
    "        ])\n",
    "    \n",
    "    return {\n",
    "        'use_case': use_case,\n",
    "        'jurisdictions': jurisdictions,\n",
    "        'total_requirements': len(requirements),\n",
    "        'requirements': requirements,\n",
    "        'compliance_complexity': 'High' if len(requirements) > 8 else 'Medium' if len(requirements) > 4 else 'Low'\n",
    "    }\n",
    "\n",
    "# Example assessment\n",
    "example = assess_compliance_requirements(\n",
    "    use_case=\"AI-powered resume screening for employment decisions\",\n",
    "    jurisdictions=['EU', 'US'],\n",
    "    processes_personal_data=True,\n",
    "    high_risk_use=True,\n",
    "    automated_decisions=True\n",
    ")\n",
    "\n",
    "print(\"\\nCOMPLIANCE ASSESSMENT EXAMPLE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Use Case: {example['use_case']}\")\n",
    "print(f\"Jurisdictions: {', '.join(example['jurisdictions'])}\")\n",
    "print(f\"Compliance Complexity: {example['compliance_complexity']}\")\n",
    "print(f\"\\nTotal Requirements: {example['total_requirements']}\")\n",
    "print(\"\\nDetailed Requirements:\")\n",
    "for i, req in enumerate(example['requirements'], 1):\n",
    "    print(f\"{i}. {req}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implementing Bias Audits\n",
    "\n",
    "### Bias Audit Process\n",
    "\n",
    "**Step 1: Identify Protected Attributes**\n",
    "- Race/ethnicity\n",
    "- Gender\n",
    "- Age\n",
    "- Disability status\n",
    "- Religion\n",
    "- Other legally protected categories\n",
    "\n",
    "**Step 2: Define Fairness Metrics**\n",
    "\n",
    "**Demographic Parity**: Equal positive outcome rates across groups\n",
    "- P(Ŷ=1|A=0) = P(Ŷ=1|A=1)\n",
    "\n",
    "**Equal Opportunity**: Equal true positive rates across groups\n",
    "- P(Ŷ=1|Y=1,A=0) = P(Ŷ=1|Y=1,A=1)\n",
    "\n",
    "**Equalized Odds**: Equal TPR and FPR across groups\n",
    "- Equal opportunity + equal false positive rates\n",
    "\n",
    "**Calibration**: Predictions equally accurate across groups\n",
    "- P(Y=1|Ŷ=p,A=0) = P(Y=1|Ŷ=p,A=1)\n",
    "\n",
    "**Step 3: Collect Disaggregated Data**\n",
    "\n",
    "**Step 4: Calculate Metrics by Group**\n",
    "\n",
    "**Step 5: Assess Disparities**\n",
    "- Compare metrics across groups\n",
    "- Apply thresholds (e.g., 80% rule, <10% difference)\n",
    "\n",
    "**Step 6: Investigate Root Causes**\n",
    "\n",
    "**Step 7: Implement Mitigations**\n",
    "- Pre-processing: Adjust training data\n",
    "- In-processing: Fairness constraints during training\n",
    "- Post-processing: Adjust predictions\n",
    "\n",
    "**Step 8: Document and Monitor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bias audit simulation\n",
    "\n",
    "# Generate synthetic hiring data for demonstration\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "# Simulate biased hiring outcomes\n",
    "data = {\n",
    "    'applicant_id': range(1, n_samples + 1),\n",
    "    'gender': np.random.choice(['Male', 'Female'], n_samples, p=[0.6, 0.4]),\n",
    "    'ethnicity': np.random.choice(['Group A', 'Group B', 'Group C'], n_samples, p=[0.5, 0.3, 0.2]),\n",
    "    'qualifications_score': np.random.normal(70, 15, n_samples),\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df['qualifications_score'] = df['qualifications_score'].clip(0, 100)\n",
    "\n",
    "# Simulate biased predictions (gender bias)\n",
    "# Males get 5-point boost, some ethnicities get different treatment\n",
    "df['adjusted_score'] = df['qualifications_score'].copy()\n",
    "df.loc[df['gender'] == 'Male', 'adjusted_score'] += 5\n",
    "df.loc[df['ethnicity'] == 'Group C', 'adjusted_score'] -= 3\n",
    "\n",
    "# Hiring decision based on adjusted score\n",
    "threshold = 75\n",
    "df['predicted_hire'] = (df['adjusted_score'] >= threshold).astype(int)\n",
    "df['actual_hire'] = (df['qualifications_score'] >= 73).astype(int)  # True criterion\n",
    "\n",
    "print(\"BIAS AUDIT: HIRING ALGORITHM\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total Applicants: {len(df)}\")\n",
    "print(f\"Hiring Threshold: {threshold}\")\n",
    "print(f\"\\nSample Data:\")\n",
    "print(df.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate fairness metrics by group\n",
    "\n",
    "def calculate_fairness_metrics(df: pd.DataFrame, protected_attr: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate key fairness metrics for each group in a protected attribute.\n",
    "    \"\"\"\n",
    "    metrics = []\n",
    "    \n",
    "    for group in df[protected_attr].unique():\n",
    "        group_data = df[df[protected_attr] == group]\n",
    "        \n",
    "        # Selection rate (demographic parity)\n",
    "        selection_rate = group_data['predicted_hire'].mean()\n",
    "        \n",
    "        # True positive rate (equal opportunity)\n",
    "        true_positives = group_data[group_data['actual_hire'] == 1]\n",
    "        tpr = true_positives['predicted_hire'].mean() if len(true_positives) > 0 else 0\n",
    "        \n",
    "        # False positive rate\n",
    "        true_negatives = group_data[group_data['actual_hire'] == 0]\n",
    "        fpr = true_negatives['predicted_hire'].mean() if len(true_negatives) > 0 else 0\n",
    "        \n",
    "        # Precision\n",
    "        predicted_positives = group_data[group_data['predicted_hire'] == 1]\n",
    "        precision = predicted_positives['actual_hire'].mean() if len(predicted_positives) > 0 else 0\n",
    "        \n",
    "        metrics.append({\n",
    "            'Group': group,\n",
    "            'Count': len(group_data),\n",
    "            'Selection_Rate': selection_rate,\n",
    "            'True_Positive_Rate': tpr,\n",
    "            'False_Positive_Rate': fpr,\n",
    "            'Precision': precision\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(metrics)\n",
    "\n",
    "# Analyze by gender\n",
    "gender_metrics = calculate_fairness_metrics(df, 'gender')\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FAIRNESS METRICS BY GENDER\")\n",
    "print(\"=\"*80)\n",
    "print(gender_metrics.to_string(index=False))\n",
    "\n",
    "# Calculate disparities\n",
    "male_selection = gender_metrics[gender_metrics['Group'] == 'Male']['Selection_Rate'].values[0]\n",
    "female_selection = gender_metrics[gender_metrics['Group'] == 'Female']['Selection_Rate'].values[0]\n",
    "disparity_ratio = female_selection / male_selection if male_selection > 0 else 0\n",
    "\n",
    "print(f\"\\nDemographic Parity Analysis:\")\n",
    "print(f\"  Male Selection Rate: {male_selection:.1%}\")\n",
    "print(f\"  Female Selection Rate: {female_selection:.1%}\")\n",
    "print(f\"  Disparity Ratio: {disparity_ratio:.2f}\")\n",
    "print(f\"  80% Rule: {'PASS' if disparity_ratio >= 0.8 else 'FAIL'}\")\n",
    "\n",
    "# Analyze by ethnicity\n",
    "ethnicity_metrics = calculate_fairness_metrics(df, 'ethnicity')\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FAIRNESS METRICS BY ETHNICITY\")\n",
    "print(\"=\"*80)\n",
    "print(ethnicity_metrics.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize bias audit results\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Gender comparison\n",
    "metrics_to_plot = ['Selection_Rate', 'True_Positive_Rate', 'Precision']\n",
    "x = np.arange(len(metrics_to_plot))\n",
    "width = 0.35\n",
    "\n",
    "male_values = gender_metrics[gender_metrics['Group'] == 'Male'][metrics_to_plot].values[0]\n",
    "female_values = gender_metrics[gender_metrics['Group'] == 'Female'][metrics_to_plot].values[0]\n",
    "\n",
    "axes[0].bar(x - width/2, male_values, width, label='Male', alpha=0.8)\n",
    "axes[0].bar(x + width/2, female_values, width, label='Female', alpha=0.8)\n",
    "axes[0].set_ylabel('Rate', fontweight='bold')\n",
    "axes[0].set_title('Fairness Metrics by Gender', fontweight='bold', fontsize=14)\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(['Selection\\nRate', 'True Positive\\nRate', 'Precision'])\n",
    "axes[0].legend()\n",
    "axes[0].axhline(y=0.8, color='red', linestyle='--', alpha=0.5, label='80% threshold')\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "axes[0].set_ylim(0, 1)\n",
    "\n",
    "# Ethnicity comparison\n",
    "ethnicity_metrics.plot(x='Group', y='Selection_Rate', kind='bar', ax=axes[1], legend=False, alpha=0.8)\n",
    "axes[1].set_ylabel('Selection Rate', fontweight='bold')\n",
    "axes[1].set_xlabel('Ethnicity Group', fontweight='bold')\n",
    "axes[1].set_title('Selection Rate by Ethnicity', fontweight='bold', fontsize=14)\n",
    "axes[1].axhline(y=df['predicted_hire'].mean(), color='red', linestyle='--', alpha=0.5, label='Overall Rate')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "axes[1].set_ylim(0, 1)\n",
    "axes[1].tick_params(axis='x', rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nBIAS AUDIT FINDINGS:\")\n",
    "print(\"- Gender bias detected: Males have higher selection rate\")\n",
    "print(\"- Ethnicity bias detected: Group C has lower selection rate\")\n",
    "print(\"- Recommendation: Implement debiasing techniques and re-audit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Transparency and Documentation\n",
    "\n",
    "### Model Cards\n",
    "\n",
    "Standardized documentation for ML models:\n",
    "\n",
    "**Required Sections**:\n",
    "1. **Model Details**: Version, type, architecture, developer\n",
    "2. **Intended Use**: Primary use cases, out-of-scope uses\n",
    "3. **Training Data**: Sources, size, preprocessing, limitations\n",
    "4. **Performance**: Metrics by subgroup, test conditions\n",
    "5. **Fairness Assessment**: Bias testing results\n",
    "6. **Limitations**: Known issues, failure modes\n",
    "7. **Ethical Considerations**: Risks, mitigation strategies\n",
    "8. **Recommendations**: Deployment guidance, monitoring needs\n",
    "\n",
    "### Transparency Reporting\n",
    "\n",
    "**Internal Reporting**:\n",
    "- Regular bias audits\n",
    "- Performance monitoring\n",
    "- Incident tracking\n",
    "- Compliance attestations\n",
    "\n",
    "**External Reporting** (as required):\n",
    "- Public model cards\n",
    "- Impact assessments\n",
    "- Algorithmic transparency reports\n",
    "- Regulatory filings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model card template\n",
    "\n",
    "model_card_template = {\n",
    "    \"model_details\": {\n",
    "        \"name\": \"Resume Screening Model v2.1\",\n",
    "        \"version\": \"2.1.0\",\n",
    "        \"date\": \"2025-11-17\",\n",
    "        \"model_type\": \"Binary Classification (LLM-based)\",\n",
    "        \"developer\": \"HR Technology Team\",\n",
    "        \"contact\": \"ai-ethics@company.com\"\n",
    "    },\n",
    "    \"intended_use\": {\n",
    "        \"primary_uses\": [\n",
    "            \"Initial screening of job applications\",\n",
    "            \"Ranking candidates for human review\"\n",
    "        ],\n",
    "        \"primary_users\": [\"HR recruiters\", \"Hiring managers\"],\n",
    "        \"out_of_scope\": [\n",
    "            \"Final hiring decisions (requires human review)\",\n",
    "            \"Performance evaluation of existing employees\",\n",
    "            \"Use outside of hiring context\"\n",
    "        ]\n",
    "    },\n",
    "    \"training_data\": {\n",
    "        \"sources\": \"Historical hiring data 2020-2024\",\n",
    "        \"size\": \"50,000 applications\",\n",
    "        \"preprocessing\": \"PII removal, standardization, deduplication\",\n",
    "        \"limitations\": \"Historical data may reflect past biases\"\n",
    "    },\n",
    "    \"performance\": {\n",
    "        \"overall_metrics\": {\n",
    "            \"accuracy\": 0.82,\n",
    "            \"precision\": 0.78,\n",
    "            \"recall\": 0.85,\n",
    "            \"f1_score\": 0.81\n",
    "        },\n",
    "        \"performance_by_group\": \"See fairness assessment section\"\n",
    "    },\n",
    "    \"fairness_assessment\": {\n",
    "        \"bias_testing_date\": \"2025-11-01\",\n",
    "        \"protected_attributes_tested\": [\"gender\", \"ethnicity\", \"age\"],\n",
    "        \"findings\": \"Initial bias detected and mitigated. Current system meets 80% rule.\",\n",
    "        \"ongoing_monitoring\": \"Monthly bias audits\"\n",
    "    },\n",
    "    \"limitations\": [\n",
    "        \"May not generalize to roles significantly different from training data\",\n",
    "        \"Requires periodic retraining to avoid drift\",\n",
    "        \"Cannot assess soft skills or cultural fit\",\n",
    "        \"English language only\"\n",
    "    ],\n",
    "    \"ethical_considerations\": {\n",
    "        \"risks\": [\n",
    "            \"Potential for bias amplification\",\n",
    "            \"Privacy concerns with applicant data\",\n",
    "            \"Over-reliance on automated screening\"\n",
    "        ],\n",
    "        \"mitigations\": [\n",
    "            \"Regular bias audits and retraining\",\n",
    "            \"Strict data governance and access controls\",\n",
    "            \"Mandatory human review for all final decisions\",\n",
    "            \"Candidate appeal process\"\n",
    "        ]\n",
    "    },\n",
    "    \"recommendations\": {\n",
    "        \"deployment\": \"Use as decision support only, not autonomous decision-making\",\n",
    "        \"monitoring\": \"Monthly performance and bias monitoring\",\n",
    "        \"human_oversight\": \"All recommendations reviewed by trained HR staff\",\n",
    "        \"retraining\": \"Quarterly model updates with bias testing\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"MODEL CARD EXAMPLE\")\n",
    "print(\"=\"*80)\n",
    "print(json.dumps(model_card_template, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Privacy and Security Controls\n",
    "\n",
    "### Privacy Controls\n",
    "\n",
    "**1. Data Minimization**\n",
    "- Collect only necessary data\n",
    "- Limit retention periods\n",
    "- Regular data purging\n",
    "\n",
    "**2. Purpose Limitation**\n",
    "- Use data only for stated purposes\n",
    "- Obtain consent for new uses\n",
    "- Document all use cases\n",
    "\n",
    "**3. Access Controls**\n",
    "- Role-based access (RBAC)\n",
    "- Principle of least privilege\n",
    "- Access logging and auditing\n",
    "\n",
    "**4. Anonymization/Pseudonymization**\n",
    "- Remove direct identifiers\n",
    "- Aggregate data where possible\n",
    "- Assess re-identification risks\n",
    "\n",
    "**5. Encryption**\n",
    "- Data at rest encryption\n",
    "- Data in transit encryption\n",
    "- Key management procedures\n",
    "\n",
    "**6. Data Subject Rights**\n",
    "- Access requests\n",
    "- Correction/deletion\n",
    "- Portability\n",
    "- Objection to processing\n",
    "\n",
    "### Security Controls\n",
    "\n",
    "**1. Adversarial Robustness**\n",
    "- Test against adversarial examples\n",
    "- Input validation and sanitization\n",
    "- Anomaly detection\n",
    "\n",
    "**2. Model Security**\n",
    "- Protect model weights/parameters\n",
    "- Prevent model extraction\n",
    "- Monitor for unusual queries\n",
    "\n",
    "**3. Infrastructure Security**\n",
    "- Secure deployment environments\n",
    "- Network segmentation\n",
    "- Regular security updates\n",
    "\n",
    "**4. Incident Response**\n",
    "- Detection and alerting\n",
    "- Response procedures\n",
    "- Communication plans\n",
    "- Post-incident review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Privacy and security controls checklist\n",
    "\n",
    "controls_checklist = {\n",
    "    'Control_Category': [\n",
    "        'Data Minimization',\n",
    "        'Purpose Limitation',\n",
    "        'Access Controls',\n",
    "        'Anonymization',\n",
    "        'Encryption',\n",
    "        'Data Subject Rights',\n",
    "        'Adversarial Robustness',\n",
    "        'Model Security',\n",
    "        'Infrastructure Security',\n",
    "        'Incident Response'\n",
    "    ],\n",
    "    'Priority': ['High', 'Critical', 'Critical', 'High', 'Critical', 'Critical', \n",
    "                 'Medium', 'High', 'Critical', 'High'],\n",
    "    'Implementation_Difficulty': ['Low', 'Medium', 'Medium', 'High', 'Low', 'High',\n",
    "                                   'High', 'Medium', 'Medium', 'Medium'],\n",
    "    'Regulatory_Requirement': ['GDPR', 'GDPR', 'GDPR+Security', 'GDPR', 'GDPR+Security', \n",
    "                               'GDPR', 'Best Practice', 'Best Practice', 'Security', 'Security'],\n",
    "    'Key_Tool_or_Practice': [\n",
    "        'Data inventory and classification',\n",
    "        'Consent management system',\n",
    "        'IAM system with RBAC',\n",
    "        'De-identification tools',\n",
    "        'AES-256 encryption',\n",
    "        'Data subject request portal',\n",
    "        'Adversarial testing tools',\n",
    "        'Model versioning and access logs',\n",
    "        'SIEM, firewalls, patching',\n",
    "        'Incident response playbook'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_controls = pd.DataFrame(controls_checklist)\n",
    "\n",
    "print(\"\\nPRIVACY & SECURITY CONTROLS CHECKLIST\")\n",
    "print(\"=\"*80)\n",
    "print(df_controls.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CRITICAL PRIORITY CONTROLS:\")\n",
    "critical = df_controls[df_controls['Priority'] == 'Critical']\n",
    "for idx, row in critical.iterrows():\n",
    "    print(f\"\\n{row['Control_Category']}:\")\n",
    "    print(f\"  Tool/Practice: {row['Key_Tool_or_Practice']}\")\n",
    "    print(f\"  Difficulty: {row['Implementation_Difficulty']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Human Oversight Mechanisms\n",
    "\n",
    "### Human-in-the-Loop (HITL) Design Patterns\n",
    "\n",
    "**1. Human-in-Command**\n",
    "- AI provides recommendations\n",
    "- Human makes final decision\n",
    "- **Use for**: High-stakes decisions, complex judgment\n",
    "\n",
    "**2. Human-on-the-Loop**\n",
    "- AI operates autonomously\n",
    "- Human monitors and can intervene\n",
    "- **Use for**: Real-time systems, operational contexts\n",
    "\n",
    "**3. Human-out-of-the-Loop**\n",
    "- AI operates fully autonomously\n",
    "- Human reviews outcomes periodically\n",
    "- **Use for**: Low-stakes, high-volume decisions\n",
    "\n",
    "### Designing Effective Oversight\n",
    "\n",
    "**Key Principles**:\n",
    "\n",
    "1. **Meaningful Control**: Humans must have genuine ability to influence outcomes\n",
    "2. **Appropriate Information**: Provide sufficient context without overwhelming\n",
    "3. **Time to Decide**: Adequate time for human judgment\n",
    "4. **Skill Match**: Human reviewers with appropriate expertise\n",
    "5. **Avoid Automation Bias**: Design to prevent rubber-stamping\n",
    "\n",
    "**Warning Signs of Ineffective Oversight**:\n",
    "- >95% of AI recommendations accepted without change\n",
    "- Review time <10 seconds for complex decisions\n",
    "- Humans can't articulate reasoning for decisions\n",
    "- No mechanism to provide feedback to improve AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Human oversight decision framework\n",
    "\n",
    "def determine_oversight_level(stakes: str, \n",
    "                             reversibility: str,\n",
    "                             volume: str,\n",
    "                             ai_confidence: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Determine appropriate level of human oversight.\n",
    "    \n",
    "    Parameters:\n",
    "    - stakes: 'Low', 'Medium', 'High', 'Critical'\n",
    "    - reversibility: 'Easy', 'Moderate', 'Difficult', 'Impossible'\n",
    "    - volume: 'Low' (<100/day), 'Medium' (100-1000/day), 'High' (>1000/day)\n",
    "    - ai_confidence: 'Low' (<70%), 'Medium' (70-90%), 'High' (>90%)\n",
    "    \"\"\"\n",
    "    \n",
    "    # High stakes or irreversible always requires human-in-command\n",
    "    if stakes in ['High', 'Critical'] or reversibility in ['Difficult', 'Impossible']:\n",
    "        return {\n",
    "            'oversight_level': 'Human-in-Command',\n",
    "            'description': 'AI provides recommendations; human makes all final decisions',\n",
    "            'review_rate': '100%',\n",
    "            'automation_allowed': False,\n",
    "            'rationale': 'High stakes or irreversible decisions require human judgment'\n",
    "        }\n",
    "    \n",
    "    # Low confidence requires more oversight\n",
    "    if ai_confidence == 'Low':\n",
    "        return {\n",
    "            'oversight_level': 'Human-in-Command',\n",
    "            'description': 'AI provides recommendations; human makes all final decisions',\n",
    "            'review_rate': '100%',\n",
    "            'automation_allowed': False,\n",
    "            'rationale': 'Low AI confidence requires human verification'\n",
    "        }\n",
    "    \n",
    "    # High volume + medium stakes + good AI = human-on-the-loop\n",
    "    if volume == 'High' and stakes == 'Medium' and ai_confidence == 'High':\n",
    "        return {\n",
    "            'oversight_level': 'Human-on-the-Loop',\n",
    "            'description': 'AI operates autonomously with human monitoring and intervention capability',\n",
    "            'review_rate': '10-20% sampling',\n",
    "            'automation_allowed': True,\n",
    "            'rationale': 'High volume and high confidence enable automation with monitoring'\n",
    "        }\n",
    "    \n",
    "    # Low stakes + easy reversibility = human-out-of-loop possible\n",
    "    if stakes == 'Low' and reversibility == 'Easy':\n",
    "        return {\n",
    "            'oversight_level': 'Human-out-of-the-Loop',\n",
    "            'description': 'AI operates fully autonomously; human reviews aggregated outcomes',\n",
    "            'review_rate': 'Weekly aggregate review',\n",
    "            'automation_allowed': True,\n",
    "            'rationale': 'Low stakes and easy reversibility allow full automation'\n",
    "        }\n",
    "    \n",
    "    # Default: human-in-command for safety\n",
    "    return {\n",
    "        'oversight_level': 'Human-in-Command (Default)',\n",
    "        'description': 'AI provides recommendations; human makes all final decisions',\n",
    "        'review_rate': '100%',\n",
    "        'automation_allowed': False,\n",
    "        'rationale': 'Conservative default for unclear scenarios'\n",
    "    }\n",
    "\n",
    "# Example scenarios\n",
    "scenarios = [\n",
    "    {'name': 'Hiring Decision', 'stakes': 'High', 'reversibility': 'Difficult', 'volume': 'Medium', 'ai_confidence': 'High'},\n",
    "    {'name': 'Spam Filtering', 'stakes': 'Low', 'reversibility': 'Easy', 'volume': 'High', 'ai_confidence': 'High'},\n",
    "    {'name': 'Loan Approval', 'stakes': 'High', 'reversibility': 'Moderate', 'volume': 'High', 'ai_confidence': 'Medium'},\n",
    "    {'name': 'Product Recommendation', 'stakes': 'Low', 'reversibility': 'Easy', 'volume': 'High', 'ai_confidence': 'Medium'},\n",
    "]\n",
    "\n",
    "print(\"\\nHUMAN OVERSIGHT RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for scenario in scenarios:\n",
    "    result = determine_oversight_level(\n",
    "        scenario['stakes'],\n",
    "        scenario['reversibility'],\n",
    "        scenario['volume'],\n",
    "        scenario['ai_confidence']\n",
    "    )\n",
    "    print(f\"\\n{scenario['name'].upper()}:\")\n",
    "    print(f\"  Recommended: {result['oversight_level']}\")\n",
    "    print(f\"  Description: {result['description']}\")\n",
    "    print(f\"  Review Rate: {result['review_rate']}\")\n",
    "    print(f\"  Rationale: {result['rationale']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Practical Exercise\n",
    "\n",
    "### Design a Governance Implementation Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR TURN: Create governance implementation plan for your AI system\n",
    "\n",
    "my_governance_plan = \"\"\"\n",
    "AI SYSTEM DESCRIPTION:\n",
    "Name: [Your AI system]\n",
    "Use Case: [What does it do?]\n",
    "Jurisdictions: [Where will it operate?]\n",
    "User Impact: [Who is affected and how?]\n",
    "\n",
    "REGULATORY COMPLIANCE:\n",
    "Applicable Regulations:\n",
    "1. [Regulation]: [Key requirements]\n",
    "2. [Regulation]: [Key requirements]\n",
    "3. [Regulation]: [Key requirements]\n",
    "\n",
    "Compliance Actions:\n",
    "- [Action 1]: [Timeline and owner]\n",
    "- [Action 2]: [Timeline and owner]\n",
    "\n",
    "BIAS AUDIT PLAN:\n",
    "Protected Attributes to Test: [List attributes]\n",
    "Fairness Metrics: [Which metrics?]\n",
    "Acceptable Thresholds: [Define thresholds]\n",
    "Audit Frequency: [How often?]\n",
    "Responsible Party: [Who conducts audits?]\n",
    "\n",
    "TRANSPARENCY & DOCUMENTATION:\n",
    "Model Card: [Will you create one? When?]\n",
    "Internal Reporting: [What and to whom?]\n",
    "External Reporting: [Any public disclosures?]\n",
    "Documentation Updates: [How often?]\n",
    "\n",
    "PRIVACY & SECURITY:\n",
    "Priority Controls:\n",
    "1. [Control]: [Implementation approach]\n",
    "2. [Control]: [Implementation approach]\n",
    "3. [Control]: [Implementation approach]\n",
    "\n",
    "Data Protection Measures:\n",
    "- Data minimization: [How?]\n",
    "- Encryption: [What type?]\n",
    "- Access controls: [Who has access?]\n",
    "\n",
    "HUMAN OVERSIGHT:\n",
    "Oversight Level: [Human-in-command / on-loop / out-of-loop]\n",
    "Justification: [Why this level?]\n",
    "Review Process: [How will humans review?]\n",
    "Review Rate: [What percentage or frequency?]\n",
    "Escalation: [When to escalate to humans?]\n",
    "\n",
    "INCIDENT RESPONSE:\n",
    "Potential Incidents:\n",
    "- [Incident type]: [Response procedure]\n",
    "- [Incident type]: [Response procedure]\n",
    "\n",
    "Response Team: [Who responds?]\n",
    "Communication Plan: [Internal and external communication]\n",
    "\n",
    "MONITORING & CONTINUOUS IMPROVEMENT:\n",
    "KPIs to Track:\n",
    "- [KPI 1]: [Target and frequency]\n",
    "- [KPI 2]: [Target and frequency]\n",
    "\n",
    "Review Cycles:\n",
    "- Monthly: [What is reviewed?]\n",
    "- Quarterly: [What is reviewed?]\n",
    "- Annually: [What is reviewed?]\n",
    "\n",
    "IMPLEMENTATION TIMELINE:\n",
    "Month 1-3: [What will be implemented?]\n",
    "Month 4-6: [What will be implemented?]\n",
    "Month 7-12: [What will be implemented?]\n",
    "\"\"\"\n",
    "\n",
    "print(my_governance_plan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Discussion Questions\n",
    "\n",
    "1. **Regulatory Strategy**: How should a global organization balance different regulatory requirements across jurisdictions (EU AI Act vs. US patchwork)?\n",
    "\n",
    "2. **Bias Metrics**: Different fairness metrics can conflict (demographic parity vs. equal opportunity). How do you choose which metric to prioritize?\n",
    "\n",
    "3. **Transparency vs. IP**: How do you balance transparency requirements with protecting proprietary technology?\n",
    "\n",
    "4. **Privacy vs. Performance**: Better AI often requires more data. How do you balance privacy protection with model performance?\n",
    "\n",
    "5. **Automation Bias**: How do you prevent human reviewers from simply rubber-stamping AI recommendations?\n",
    "\n",
    "6. **Cost of Compliance**: Comprehensive governance is expensive. How do you justify the investment to leadership?\n",
    "\n",
    "7. **Evolving Standards**: Regulations and best practices evolve rapidly. How do you build adaptable governance?\n",
    "\n",
    "8. **High-Stakes Domains**: In domains like healthcare or criminal justice, what additional safeguards beyond standard frameworks are necessary?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Reflections:\n",
    "\n",
    "[Write your responses here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Key Takeaways\n",
    "\n",
    "1. **Regulation is here** - AI-specific laws are being enacted globally, particularly in the EU\n",
    "\n",
    "2. **Risk-based approach** - Most frameworks categorize AI systems by risk level with proportionate requirements\n",
    "\n",
    "3. **Bias audits are essential** - Regular fairness testing is both ethically necessary and increasingly legally required\n",
    "\n",
    "4. **Documentation matters** - Model cards and transparency reports are becoming standard practice\n",
    "\n",
    "5. **Privacy is non-negotiable** - GDPR and similar laws apply strict requirements to AI systems processing personal data\n",
    "\n",
    "6. **Human oversight is critical** - High-stakes decisions require meaningful human control, not just automation\n",
    "\n",
    "7. **Compliance is ongoing** - Governance is not one-time but requires continuous monitoring and improvement\n",
    "\n",
    "8. **Proactive beats reactive** - Building governance early is easier and cheaper than retrofitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Looking Ahead to Week 13\n",
    "\n",
    "Next week, we'll shift to **Developing Technology Strategy & Portfolio Management**.\n",
    "\n",
    "We'll explore:\n",
    "- Frameworks for technology strategy development\n",
    "- Aligning initiatives with business goals\n",
    "- Project prioritization methodologies\n",
    "- Portfolio management for technology initiatives\n",
    "- Roadmap development\n",
    "\n",
    "**Assignment 3 Due This Week**: Tech-Ready Operating Model Design\n",
    "\n",
    "**Preparation:** Inventory your organization's current and planned AI/technology initiatives. How are they prioritized today?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "### Regulations:\n",
    "- EU AI Act full text: [https://artificialintelligenceact.eu/](https://artificialintelligenceact.eu/)\n",
    "- GDPR compliance resources: [https://gdpr.eu/](https://gdpr.eu/)\n",
    "- US AI Executive Order: [https://www.whitehouse.gov/ai/](https://www.whitehouse.gov/ai/)\n",
    "\n",
    "### Bias Auditing:\n",
    "- Fairlearn (Microsoft): Open-source fairness assessment toolkit\n",
    "- AI Fairness 360 (IBM): Bias detection and mitigation toolkit\n",
    "- Google's What-If Tool: Model understanding and fairness\n",
    "\n",
    "### Documentation:\n",
    "- Model Cards for Model Reporting (Google)\n",
    "- Datasheets for Datasets (Microsoft)\n",
    "- FactSheets (IBM)\n",
    "\n",
    "### Privacy:\n",
    "- NIST Privacy Framework\n",
    "- IAPP (International Association of Privacy Professionals) resources\n",
    "- Privacy-preserving ML techniques overview\n",
    "\n",
    "---\n",
    "\n",
    "*End of Week 12 Notebook*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
