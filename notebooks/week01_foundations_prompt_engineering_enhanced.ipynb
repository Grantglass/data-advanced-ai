{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“˜ Week 1: Foundations of Effective Prompt Engineering\n",
    "\n",
    "## MBA 590 - Advanced AI Strategy: Prompting and Agentic Frameworks\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This week focuses on reviewing core principles of prompt engineering, understanding LLM input/output processing, and mastering the fundamentals of context, specificity, and clarity in prompt design. We'll introduce basic prompt types including zero-shot and role-playing prompts.\n",
    "\n",
    "### Key Topics\n",
    "- Core principles of prompt engineering\n",
    "- LLM input/output processing\n",
    "- Context, specificity, and clarity in prompt design\n",
    "- Basic prompt types: zero-shot, role-playing, simple instruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ğŸ¯ Learning Objectives\n\nBy the end of this week, you will be able to:\n\n### Knowledge & Understanding\n- [ ] **Explain** the fundamental principles of effective prompt engineering\n- [ ] **Describe** how LLMs process inputs and generate outputs\n- [ ] **Identify** the key elements: context, specificity, and clarity\n\n### Application & Analysis\n- [ ] **Apply** context, specificity, and clarity to improve prompt quality\n- [ ] **Differentiate** between zero-shot, role-playing, and instruction prompts\n- [ ] **Design** prompts appropriate for different business scenarios\n\n### Evaluation & Creation\n- [ ] **Evaluate** the strengths and weaknesses of different prompting approaches\n- [ ] **Create** effective prompts for real-world business tasks\n\n> ğŸ’¡ **Success Indicator**: You can design a prompt strategy for a new business problem and explain your choices"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Academic Readings\n",
    "\n",
    "1. **Fuentealba Cid, D., Flores-FernÃ¡ndez, C., & Aguilera EguÃ­a, R. (2024).** *The art of prompts' formulation: limitations, potential, and practical examples in large language models.* Salud, Ciencia y TecnologÃ­a, 4, 969.\n",
    "\n",
    "2. **Johnson, S., & Hyland-Wood, D. (2024).** *A Primer on Large Language Models and their Limitations.* arXiv preprint arXiv:2412.04503."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Core Principles of Prompt Engineering\n\n### Key Principles:\n\n1. **Clarity**: Make your instructions explicit and unambiguous\n2. **Context**: Provide relevant background information\n3. **Specificity**: Be precise about what you want\n4. **Structure**: Organize prompts logically\n5. **Examples**: Use examples when appropriate (we'll explore this more in Week 2)\n\n> ğŸ’¡ **Key Insight**: According to research by OpenAI and Anthropic, well-structured prompts can improve output quality by 40-60% compared to vague instructions.\n\n### Understanding LLM Processing\n\nLarge Language Models (LLMs) work by:\n- Tokenizing input text\n- Processing tokens through neural network layers\n- Predicting the most likely next tokens\n- Generating coherent responses based on training data patterns\n\n**Key Limitation**: LLMs don't \"understand\" in a human sense; they recognize patterns and generate statistically likely continuations.\n\n#### How LLMs Process Your Prompts\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  INPUT: Your Prompt                              â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                   â”‚\n                   â–¼\n         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n         â”‚  1. Tokenization â”‚\n         â”‚  \"Hello\" â†’ [15496]â”‚\n         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                  â”‚\n                  â–¼\n       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n       â”‚  2. Neural Processing â”‚\n       â”‚  (Attention Layers)   â”‚\n       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                  â”‚\n                  â–¼\n      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n      â”‚  3. Token Prediction   â”‚\n      â”‚  (Statistical)         â”‚\n      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                 â”‚\n                 â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  OUTPUT: Generated Response         â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n> âš ï¸ **Common Pitfall**: Many users assume LLMs have real-time information or can access external data. They only \"know\" what was in their training data (typically with a cutoff date)."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Types of Basic Prompts\n",
    "\n",
    "### A. Zero-Shot Prompts\n",
    "\n",
    "Zero-shot prompts provide a direct instruction without examples.\n",
    "\n",
    "**Characteristics:**\n",
    "- No examples provided\n",
    "- Relies on model's pre-existing knowledge\n",
    "- Simple and straightforward\n",
    "\n",
    "### B. Role-Playing Prompts\n",
    "\n",
    "Role-playing prompts assign a specific role or persona to the LLM.\n",
    "\n",
    "**Characteristics:**\n",
    "- Sets context through role assignment\n",
    "- Can improve response quality and consistency\n",
    "- Useful for specialized tasks\n",
    "\n",
    "### C. Simple Instruction Prompts\n",
    "\n",
    "Direct, task-oriented prompts without role-playing.\n",
    "\n",
    "**Characteristics:**\n",
    "- Clear task specification\n",
    "- Minimal context\n",
    "- Efficient for straightforward tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## ğŸŒ Real-World Application: Prompt Engineering in Practice\n\n### Case Study 1: Intercom's Customer Support Automation\n\n**Company**: Intercom (B2B SaaS, 25,000+ customers)  \n**Challenge**: Support team spending 70% of time on repetitive questions  \n**Solution**: Implemented tiered prompt strategy\n- **Zero-shot prompts**: FAQ-style questions  \n- **Role-playing prompts**: Complex customer scenarios requiring empathy  \n- **Detailed instruction prompts**: Technical troubleshooting with specific steps  \n\n**Results**: \n- 43% reduction in response time\n- 67% of tier-1 questions handled automatically\n- $1.8M annual savings in support costs\n\n> ğŸ’¡ **Key Learning**: They started with zero-shot for simple tasks, then graduated to detailed instructions as they identified patterns in customer issues.\n\n---\n\n### Case Study 2: Shopify's Product Description Generation\n\n**Company**: Shopify (E-commerce platform)  \n**Challenge**: Merchants struggle to write compelling product descriptions  \n**Solution**: Role-playing prompt system\n\n```\nYou are an expert e-commerce copywriter who specializes in \n[product_category]. Write a compelling product description that:\n1. Highlights key features and benefits\n2. Addresses common customer questions\n3. Uses persuasive language without being pushy\n4. Optimizes for SEO with natural keyword integration\n```\n\n**Results**:\n- Merchants using AI descriptions saw 23% increase in conversion rates\n- 5x faster time-to-market for new products\n- Increased merchant satisfaction scores by 31%\n\n> ğŸ’¡ **Key Learning**: Role-playing prompts create consistency across thousands of merchants while maintaining brand voice.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ğŸ¯ GOAL: Set up our environment for prompt engineering exercises\n# WHY: We'll create and compare different prompt types to understand their strengths\n# EXAMPLE INPUT: We'll use a customer service scenario (damaged product)\n# EXPECTED OUTPUT: Structured comparison of three prompt approaches\n\n# Note: In production, you would use actual LLM APIs (OpenAI, Anthropic, Azure OpenAI, etc.)\n# This notebook demonstrates prompt structure and design patterns\n\nimport json\nfrom typing import Dict, List\n\n# Business Task: Drafting a customer service email response\nprint(\"âœ… Environment ready! Let's explore prompt engineering...\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Installing required libraries\n",
    "# Note: In practice, you would use an actual LLM API (OpenAI, Anthropic, etc.)\n",
    "# This example demonstrates the prompt structure\n",
    "\n",
    "import json\n",
    "from typing import Dict, List\n",
    "\n",
    "# Business Task: Drafting a customer service email response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ğŸ¯ GOAL: Demonstrate a zero-shot prompt (no examples, direct instruction)\n# WHY: Zero-shot is fastest to create but may lack specificity\n# EXAMPLE: Simple, one-sentence instruction\n\nzero_shot_prompt = \"\"\"\nWrite an email response to a customer who received a damaged product.\n\"\"\"\n\nprint(\"Zero-Shot Prompt:\")\nprint(zero_shot_prompt)\nprint(\"\\n\" + \"=\"*50 + \"\\n\")\n\n# âœ… ANALYSIS: Evaluate this prompt's characteristics\n\n# Strengths:\nstrengths_zero_shot = [\n    \"Simple and quick to create\",\n    \"Works well for common tasks\",\n    \"No need for extensive context\"\n]\n\n# Weaknesses:\nweaknesses_zero_shot = [\n    \"May lack company-specific tone\",\n    \"Might miss important policy details\",\n    \"Generic response without specific guidance\"\n]\n\nprint(\"Strengths:\", \"\\n- \" + \"\\n- \".join(strengths_zero_shot))\nprint(\"\\nWeaknesses:\", \"\\n- \" + \"\\n- \".join(weaknesses_zero_shot))\n\n# ğŸ“ LEARNING CHECKPOINT: Notice how this prompt is direct but vague. \n# Would an LLM know your refund policy? Your brand voice? Response time expectations?\nprint(\"\\nğŸ’­ Think: What information is missing that would make this prompt better?\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_prompt = \"\"\"\n",
    "Write an email response to a customer who received a damaged product.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Zero-Shot Prompt:\")\n",
    "print(zero_shot_prompt)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Strengths:\n",
    "strengths_zero_shot = [\n",
    "    \"Simple and quick to create\",\n",
    "    \"Works well for common tasks\",\n",
    "    \"No need for extensive context\"\n",
    "]\n",
    "\n",
    "# Weaknesses:\n",
    "weaknesses_zero_shot = [\n",
    "    \"May lack company-specific tone\",\n",
    "    \"Might miss important policy details\",\n",
    "    \"Generic response without specific guidance\"\n",
    "]\n",
    "\n",
    "print(\"Strengths:\", \"\\n- \" + \"\\n- \".join(strengths_zero_shot))\n",
    "print(\"\\nWeaknesses:\", \"\\n- \" + \"\\n- \".join(weaknesses_zero_shot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Role-Playing Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role_playing_prompt = \"\"\"\n",
    "You are a senior customer service representative at a premium e-commerce company \n",
    "known for exceptional customer care. You always maintain a warm, empathetic tone \n",
    "while being professional and solution-oriented.\n",
    "\n",
    "Write an email response to a customer who received a damaged product. The customer \n",
    "is understandably frustrated but has been polite in their complaint.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Role-Playing Prompt:\")\n",
    "print(role_playing_prompt)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "strengths_role = [\n",
    "    \"Establishes appropriate tone and style\",\n",
    "    \"Provides context about company values\",\n",
    "    \"More likely to generate brand-aligned responses\",\n",
    "    \"Adds empathy and professionalism\"\n",
    "]\n",
    "\n",
    "weaknesses_role = [\n",
    "    \"Longer prompt requires more tokens\",\n",
    "    \"May still lack specific policy guidance\",\n",
    "    \"Requires careful role definition\"\n",
    "]\n",
    "\n",
    "print(\"Strengths:\", \"\\n- \" + \"\\n- \".join(strengths_role))\n",
    "print(\"\\nWeaknesses:\", \"\\n- \" + \"\\n- \".join(weaknesses_role))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Detailed Instruction Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detailed_instruction_prompt = \"\"\"\n",
    "Write a customer service email response with the following requirements:\n",
    "\n",
    "Situation: Customer received a damaged product\n",
    "\n",
    "Instructions:\n",
    "1. Acknowledge the issue and apologize\n",
    "2. Express empathy for the inconvenience\n",
    "3. Offer two solutions: full refund or replacement shipment\n",
    "4. Mention that return shipping will be covered\n",
    "5. Provide a direct contact number for urgent assistance\n",
    "6. Thank them for their patience\n",
    "\n",
    "Tone: Professional, warm, and solution-focused\n",
    "Length: 150-200 words\n",
    "\"\"\"\n",
    "\n",
    "print(\"Detailed Instruction Prompt:\")\n",
    "print(detailed_instruction_prompt)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "strengths_detailed = [\n",
    "    \"Very specific guidance on content\",\n",
    "    \"Includes policy-specific information\",\n",
    "    \"Structured approach ensures completeness\",\n",
    "    \"Clear expectations for tone and length\"\n",
    "]\n",
    "\n",
    "weaknesses_detailed = [\n",
    "    \"Takes more time to create\",\n",
    "    \"Longest prompt (uses more tokens)\",\n",
    "    \"May feel formulaic if overused\",\n",
    "    \"Less flexible for variations\"\n",
    "]\n",
    "\n",
    "print(\"Strengths:\", \"\\n- \" + \"\\n- \".join(strengths_detailed))\n",
    "print(\"\\nWeaknesses:\", \"\\n- \" + \"\\n- \".join(weaknesses_detailed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prompt Evaluation Framework\n",
    "\n",
    "When evaluating prompts, consider these dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a comparison matrix\n",
    "evaluation_criteria = {\n",
    "    'Criteria': [\n",
    "        'Clarity',\n",
    "        'Specificity',\n",
    "        'Context Provision',\n",
    "        'Token Efficiency',\n",
    "        'Ease of Creation',\n",
    "        'Output Consistency',\n",
    "        'Business Alignment'\n",
    "    ],\n",
    "    'Zero-Shot': [3, 2, 1, 5, 5, 2, 2],\n",
    "    'Role-Playing': [4, 3, 4, 3, 3, 4, 4],\n",
    "    'Detailed Instruction': [5, 5, 5, 2, 2, 5, 5]\n",
    "}\n",
    "\n",
    "df_evaluation = pd.DataFrame(evaluation_criteria)\n",
    "print(\"Prompt Type Comparison (Scale: 1-5, where 5 is best)\")\n",
    "print(\"=\"*60)\n",
    "print(df_evaluation.to_string(index=False))\n",
    "print(\"\\nNote: The 'best' approach depends on your specific use case and constraints.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Best Practices for Effective Prompts\n",
    "\n",
    "### Do's:\n",
    "- âœ“ Be specific about what you want\n",
    "- âœ“ Provide relevant context\n",
    "- âœ“ Use clear, unambiguous language\n",
    "- âœ“ Specify output format when needed\n",
    "- âœ“ Include constraints (length, tone, style)\n",
    "- âœ“ Test and iterate on your prompts\n",
    "\n",
    "### Don'ts:\n",
    "- âœ— Use vague or ambiguous instructions\n",
    "- âœ— Assume the model has real-time information\n",
    "- âœ— Expect perfect outputs without iteration\n",
    "- âœ— Overload with unnecessary information\n",
    "- âœ— Use contradictory instructions\n",
    "- âœ— Ignore the importance of formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hands-On Practice Activity\n",
    "\n",
    "### Task: Create Three Different Prompts\n",
    "\n",
    "Select a common business task from your field (examples below) and create three different prompts:\n",
    "1. A zero-shot prompt\n",
    "2. A role-playing prompt\n",
    "3. A detailed instruction prompt\n",
    "\n",
    "**Business Task Examples:**\n",
    "- Summarizing a quarterly report\n",
    "- Drafting a meeting agenda\n",
    "- Creating a project status update\n",
    "- Writing a product description\n",
    "- Analyzing customer feedback\n",
    "\n",
    "Use the cells below to create your prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR TURN: Define your business task\n",
    "my_business_task = \"\"\"\n",
    "Description of your chosen business task:\n",
    "\n",
    "[Write your task description here]\n",
    "\"\"\"\n",
    "\n",
    "print(my_business_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR TURN: Create a zero-shot prompt\n",
    "my_zero_shot_prompt = \"\"\"\n",
    "[Write your zero-shot prompt here]\n",
    "\"\"\"\n",
    "\n",
    "print(\"My Zero-Shot Prompt:\")\n",
    "print(my_zero_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR TURN: Create a role-playing prompt\n",
    "my_role_playing_prompt = \"\"\"\n",
    "[Write your role-playing prompt here]\n",
    "\"\"\"\n",
    "\n",
    "print(\"My Role-Playing Prompt:\")\n",
    "print(my_role_playing_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR TURN: Create a detailed instruction prompt\n",
    "my_detailed_prompt = \"\"\"\n",
    "[Write your detailed instruction prompt here]\n",
    "\"\"\"\n",
    "\n",
    "print(\"My Detailed Instruction Prompt:\")\n",
    "print(my_detailed_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Self-Evaluation Exercise\n",
    "\n",
    "Evaluate your three prompts using the framework below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate your prompts (rate 1-5 for each criteria)\n",
    "my_evaluation = {\n",
    "    'Criteria': [\n",
    "        'Clarity',\n",
    "        'Specificity',\n",
    "        'Context Provision',\n",
    "        'Expected Output Quality',\n",
    "        'Ease of Use'\n",
    "    ],\n",
    "    'My Zero-Shot': [0, 0, 0, 0, 0],  # Replace 0s with your ratings\n",
    "    'My Role-Playing': [0, 0, 0, 0, 0],  # Replace 0s with your ratings\n",
    "    'My Detailed': [0, 0, 0, 0, 0]  # Replace 0s with your ratings\n",
    "}\n",
    "\n",
    "df_my_eval = pd.DataFrame(my_evaluation)\n",
    "print(\"My Prompt Evaluation:\")\n",
    "print(df_my_eval.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Discussion Questions\n",
    "\n",
    "Reflect on the following questions:\n",
    "\n",
    "1. **Which prompt style (zero-shot, role-playing, or detailed instruction) do you think would work best for your chosen business task? Why?**\n",
    "\n",
    "2. **What are the potential strengths of each prompting approach?**\n",
    "\n",
    "3. **What are the potential weaknesses or limitations of each approach?**\n",
    "\n",
    "4. **How might the choice of prompt style impact:**\n",
    "   - Output quality\n",
    "   - Consistency across multiple uses\n",
    "   - Time invested in prompt creation\n",
    "   - Cost (token usage)\n",
    "\n",
    "5. **In what business scenarios would you prioritize:**\n",
    "   - Speed and simplicity (zero-shot)?\n",
    "   - Tone and brand alignment (role-playing)?\n",
    "   - Precision and control (detailed instruction)?\n",
    "\n",
    "Use the cell below to write your reflections:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My Reflections:\n",
    "\n",
    "[Write your thoughts here]\n",
    "\n",
    "**Question 1:**\n",
    "\n",
    "**Question 2:**\n",
    "\n",
    "**Question 3:**\n",
    "\n",
    "**Question 4:**\n",
    "\n",
    "**Question 5:**"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## ğŸ“Œ Quick Reference Guide\n\n### Prompt Type Decision Framework\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚              WHEN TO USE EACH PROMPT TYPE                 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                           â”‚\nâ”‚  ZERO-SHOT PROMPTS                                       â”‚\nâ”‚  âœ… Use when:                                             â”‚\nâ”‚     â€¢ Task is common and well-understood                 â”‚\nâ”‚     â€¢ Speed is priority                                  â”‚\nâ”‚     â€¢ Output doesn't need strict brand alignment         â”‚\nâ”‚  âŒ Avoid when:                                           â”‚\nâ”‚     â€¢ Specific tone/style required                       â”‚\nâ”‚     â€¢ Complex instructions needed                        â”‚\nâ”‚                                                           â”‚\nâ”‚  ROLE-PLAYING PROMPTS                                    â”‚\nâ”‚  âœ… Use when:                                             â”‚\nâ”‚     â€¢ Brand voice consistency is critical                â”‚\nâ”‚     â€¢ Specific expertise/perspective needed              â”‚\nâ”‚     â€¢ Building trust through persona                     â”‚\nâ”‚  âŒ Avoid when:                                           â”‚\nâ”‚     â€¢ Task is purely formulaic                           â”‚\nâ”‚     â€¢ Token budget is very limited                       â”‚\nâ”‚                                                           â”‚\nâ”‚  DETAILED INSTRUCTION PROMPTS                            â”‚\nâ”‚  âœ… Use when:                                             â”‚\nâ”‚     â€¢ Precision and control are essential                â”‚\nâ”‚     â€¢ Specific policies must be followed                 â”‚\nâ”‚     â€¢ Output format matters                              â”‚\nâ”‚  âŒ Avoid when:                                           â”‚\nâ”‚     â€¢ You need creative, flexible responses              â”‚\nâ”‚     â€¢ Quick iteration is priority                        â”‚\nâ”‚                                                           â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n### âœ… Self-Assessment Checklist\n\nBefore moving on to Week 2, ensure you can:\n\n**Foundational Knowledge**\n- [ ] Explain what tokenization is and why it matters\n- [ ] Describe the three core principles: clarity, context, specificity\n- [ ] Identify the main limitation of LLMs (pattern matching vs. understanding)\n\n**Practical Skills**\n- [ ] Write a zero-shot prompt for a business task\n- [ ] Create a role-playing prompt with appropriate persona\n- [ ] Design a detailed instruction prompt with clear requirements\n- [ ] Evaluate which prompt type suits a given scenario\n\n**Strategic Thinking**\n- [ ] Explain the trade-offs between prompt complexity and quality\n- [ ] Identify when to invest time in detailed prompts vs. quick iterations\n- [ ] Recognize scenarios where each prompt type excels\n\n**Real-World Application**\n- [ ] Apply these concepts to a problem in your organization\n- [ ] Estimate the impact of better prompts on your workflows\n\n---\n\n### ğŸ¯ If you checked all boxes:\nGreat work! You have a solid foundation in prompt engineering fundamentals. You're ready for Week 2's advanced techniques.\n\n### ğŸ¤” If you couldn't check some boxes:\nNo problem! Review the specific sections you're uncertain about:\n- **Principles**: Revisit Section 1 (Core Principles)\n- **Prompt Types**: Review Section 2 and Examples in Section 3\n- **Evaluation**: Study Section 4 (Evaluation Framework)\n- **Application**: Complete the hands-on exercise in Section 6\n\n---\n\n### ğŸ’¡ Pro Tips for Week 2 Preparation\n\n1. **Start a Prompt Library**: Save your best prompts from this week\n2. **Document What Works**: Note which approaches worked for different tasks\n3. **Identify Complexity**: Find a task that needs multi-step reasoning for Week 2\n4. **Think About Examples**: Consider scenarios where showing examples would help",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Key Takeaways\n",
    "\n",
    "1. **Prompt engineering is foundational** to effectively using LLMs in business contexts\n",
    "\n",
    "2. **Different prompt types serve different purposes:**\n",
    "   - Zero-shot: Quick, simple tasks\n",
    "   - Role-playing: Tone and style consistency\n",
    "   - Detailed instruction: Precision and control\n",
    "\n",
    "3. **Effective prompts balance:**\n",
    "   - Clarity and specificity\n",
    "   - Context and brevity\n",
    "   - Flexibility and control\n",
    "\n",
    "4. **Iteration is essential** - prompts should be tested and refined\n",
    "\n",
    "5. **Context matters** - understanding how LLMs process information helps create better prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Looking Ahead to Week 2\n",
    "\n",
    "Next week, we'll explore advanced prompting techniques:\n",
    "- Few-shot learning (providing examples)\n",
    "- Chain-of-Thought (CoT) prompting for complex reasoning\n",
    "- Self-Ask techniques for problem decomposition\n",
    "\n",
    "**Preparation:** Think about a complex decision-making scenario in your field that requires multi-step reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "- [OpenAI Prompt Engineering Guide](https://platform.openai.com/docs/guides/prompt-engineering)\n",
    "- [Anthropic Prompt Engineering Documentation](https://docs.anthropic.com/claude/docs/prompt-engineering)\n",
    "- [Google's Introduction to Prompt Design](https://developers.google.com/machine-learning/resources/prompt-eng)\n",
    "\n",
    "---\n",
    "\n",
    "*End of Week 1 Notebook*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}