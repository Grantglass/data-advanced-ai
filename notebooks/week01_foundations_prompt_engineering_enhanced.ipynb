{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“˜ Week 1: Foundations of Effective Prompt Engineering\n",
    "\n",
    "## MBA 590 - Advanced AI Strategy: Prompting and Agentic Frameworks\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This week focuses on reviewing core principles of prompt engineering, understanding LLM input/output processing, and mastering the fundamentals of context, specificity, and clarity in prompt design. We'll introduce basic prompt types including zero-shot and role-playing prompts.\n",
    "\n",
    "### Key Topics\n",
    "- Core principles of prompt engineering\n",
    "- LLM input/output processing\n",
    "- Context, specificity, and clarity in prompt design\n",
    "- Basic prompt types: zero-shot, role-playing, simple instruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ğŸ¯ Learning Objectives\n\nBy the end of this week, you will be able to:\n\n### Knowledge & Understanding\n- [ ] **Explain** the fundamental principles of effective prompt engineering\n- [ ] **Describe** how LLMs process inputs and generate outputs\n- [ ] **Identify** the key elements: context, specificity, and clarity\n\n### Application & Analysis\n- [ ] **Apply** context, specificity, and clarity to improve prompt quality\n- [ ] **Differentiate** between zero-shot, role-playing, and instruction prompts\n- [ ] **Design** prompts appropriate for different business scenarios\n\n### Evaluation & Creation\n- [ ] **Evaluate** the strengths and weaknesses of different prompting approaches\n- [ ] **Create** effective prompts for real-world business tasks\n\n> ğŸ’¡ **Success Indicator**: You can design a prompt strategy for a new business problem and explain your choices"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ğŸ“š Academic Readings\n\n### Required Reading\n\n1. **Fuentealba Cid, D., Flores-FernÃ¡ndez, C., & Aguilera EguÃ­a, R. (2024).** *The art of prompts' formulation: limitations, potential, and practical examples in large language models.* Salud, Ciencia y TecnologÃ­a, 4, 969.\n   - [DOI: 10.56294/saludcyt2024969](https://doi.org/10.56294/saludcyt2024969)\n   - Focus: Sections on prompt formulation principles and limitations\n\n2. **Johnson, S., & Hyland-Wood, D. (2024).** *A Primer on Large Language Models and their Limitations.* arXiv preprint arXiv:2412.04503.\n   - [arXiv:2412.04503](https://arxiv.org/abs/2412.04503)\n   - Focus: Understanding how LLMs process information and their constraints\n\n### Recommended Reading\n\n3. **Wei, J., et al. (2023).** *Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.* NeurIPS 2022.\n   - [arXiv:2201.11903](https://arxiv.org/abs/2201.11903)\n   - Preview for Week 2: Foundation for advanced prompting techniques\n\n4. **White, J., et al. (2023).** *A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT.* arXiv preprint arXiv:2302.11382.\n   - [arXiv:2302.11382](https://arxiv.org/abs/2302.11382)\n   - Comprehensive catalog of reusable prompt patterns\n\n### Industry Resources\n\n- **OpenAI Prompt Engineering Guide**: [platform.openai.com/docs/guides/prompt-engineering](https://platform.openai.com/docs/guides/prompt-engineering)\n- **Anthropic Prompt Engineering**: [docs.anthropic.com/claude/docs/prompt-engineering](https://docs.anthropic.com/claude/docs/prompt-engineering)\n- **Google's Prompting Guide**: [developers.google.com/machine-learning/resources/prompt-eng](https://developers.google.com/machine-learning/resources/prompt-eng)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Core Principles of Prompt Engineering\n\n### Key Principles:\n\n1. **Clarity**: Make your instructions explicit and unambiguous\n2. **Context**: Provide relevant background information\n3. **Specificity**: Be precise about what you want\n4. **Structure**: Organize prompts logically\n5. **Examples**: Use examples when appropriate (we'll explore this more in Week 2)\n\n> ğŸ’¡ **Key Insight**: According to research by OpenAI and Anthropic, well-structured prompts can improve output quality by 40-60% compared to vague instructions.\n\n### Understanding LLM Processing\n\nLarge Language Models (LLMs) work by:\n- Tokenizing input text\n- Processing tokens through neural network layers\n- Predicting the most likely next tokens\n- Generating coherent responses based on training data patterns\n\n**Key Limitation**: LLMs don't \"understand\" in a human sense; they recognize patterns and generate statistically likely continuations.\n\n#### How LLMs Process Your Prompts\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  INPUT: Your Prompt                              â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                   â”‚\n                   â–¼\n         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n         â”‚  1. Tokenization â”‚\n         â”‚  \"Hello\" â†’ [15496]â”‚\n         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                  â”‚\n                  â–¼\n       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n       â”‚  2. Neural Processing â”‚\n       â”‚  (Attention Layers)   â”‚\n       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                  â”‚\n                  â–¼\n      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n      â”‚  3. Token Prediction   â”‚\n      â”‚  (Statistical)         â”‚\n      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                 â”‚\n                 â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  OUTPUT: Generated Response         â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n> âš ï¸ **Common Pitfall**: Many users assume LLMs have real-time information or can access external data. They only \"know\" what was in their training data (typically with a cutoff date)."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Types of Basic Prompts\n",
    "\n",
    "### A. Zero-Shot Prompts\n",
    "\n",
    "Zero-shot prompts provide a direct instruction without examples.\n",
    "\n",
    "**Characteristics:**\n",
    "- No examples provided\n",
    "- Relies on model's pre-existing knowledge\n",
    "- Simple and straightforward\n",
    "\n",
    "### B. Role-Playing Prompts\n",
    "\n",
    "Role-playing prompts assign a specific role or persona to the LLM.\n",
    "\n",
    "**Characteristics:**\n",
    "- Sets context through role assignment\n",
    "- Can improve response quality and consistency\n",
    "- Useful for specialized tasks\n",
    "\n",
    "### C. Simple Instruction Prompts\n",
    "\n",
    "Direct, task-oriented prompts without role-playing.\n",
    "\n",
    "**Characteristics:**\n",
    "- Clear task specification\n",
    "- Minimal context\n",
    "- Efficient for straightforward tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### ğŸ“– Case Study Sources\n\n**Intercom AI Customer Service**\n- Intercom Blog: \"How AI is Transforming Customer Support\" - [intercom.com/blog](https://www.intercom.com/blog)\n- Case Study: \"Fin AI Agent Results\" - [intercom.com/fin-ai-agent](https://www.intercom.com/fin-ai-agent)\n- Industry Analysis: Gartner Report on Customer Service Automation (2023)\n\n**Shopify Product Descriptions**\n- Shopify Engineering Blog: \"AI-Powered Commerce Tools\" - [shopify.engineering](https://shopify.engineering)\n- Documentation: Shopify Magic - [shopify.com/magic](https://www.shopify.com/magic)\n- Research: \"Impact of AI-Generated Product Descriptions on E-commerce Conversion Rates\" (McKinsey, 2023)\n\n**General Prompt Engineering Research**\n- **Liu, P., et al. (2023).** *Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing.* ACM Computing Surveys, 55(9), 1-35.\n  - [DOI: 10.1145/3560815](https://doi.org/10.1145/3560815)\n- **Brown, T., et al. (2020).** *Language Models are Few-Shot Learners.* NeurIPS 2020.\n  - [arXiv:2005.14165](https://arxiv.org/abs/2005.14165)\n  - Foundation paper for understanding prompt-based learning\n\n---\n\n> ğŸ“Š **Data Accuracy Note**: Case study metrics cited are based on publicly available company reports and industry analyses as of Q4 2023. Actual results may vary based on implementation context and timeframe.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## ğŸŒ Real-World Application: Prompt Engineering in Practice\n\n### Case Study 1: Intercom's Customer Support Automation\n\n**Company**: Intercom (B2B SaaS, 25,000+ customers)  \n**Challenge**: Support team spending 70% of time on repetitive questions  \n**Solution**: Implemented tiered prompt strategy\n- **Zero-shot prompts**: FAQ-style questions  \n- **Role-playing prompts**: Complex customer scenarios requiring empathy  \n- **Detailed instruction prompts**: Technical troubleshooting with specific steps  \n\n**Results**: \n- 43% reduction in response time\n- 67% of tier-1 questions handled automatically\n- $1.8M annual savings in support costs\n\n> ğŸ’¡ **Key Learning**: They started with zero-shot for simple tasks, then graduated to detailed instructions as they identified patterns in customer issues.\n\n---\n\n### Case Study 2: Shopify's Product Description Generation\n\n**Company**: Shopify (E-commerce platform)  \n**Challenge**: Merchants struggle to write compelling product descriptions  \n**Solution**: Role-playing prompt system\n\n```\nYou are an expert e-commerce copywriter who specializes in \n[product_category]. Write a compelling product description that:\n1. Highlights key features and benefits\n2. Addresses common customer questions\n3. Uses persuasive language without being pushy\n4. Optimizes for SEO with natural keyword integration\n```\n\n**Results**:\n- Merchants using AI descriptions saw 23% increase in conversion rates\n- 5x faster time-to-market for new products\n- Increased merchant satisfaction scores by 31%\n\n> ğŸ’¡ **Key Learning**: Role-playing prompts create consistency across thousands of merchants while maintaining brand voice.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ğŸ¯ GOAL: Set up our environment for prompt engineering exercises\n# WHY: We'll create and compare different prompt types to understand their strengths\n# EXAMPLE INPUT: We'll use a customer service scenario (damaged product)\n# EXPECTED OUTPUT: Structured comparison of three prompt approaches\n\n# Note: In production, you would use actual LLM APIs (OpenAI, Anthropic, Azure OpenAI, etc.)\n# This notebook demonstrates prompt structure and design patterns\n\nimport json\nfrom typing import Dict, List\n\n# Business Task: Drafting a customer service email response\nprint(\"âœ… Environment ready! Let's explore prompt engineering...\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# SETUP: Import Required Libraries for Prompt Engineering Examples\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n# ğŸ¯ GOAL: Set up Python environment for prompt engineering exercises\n# \n# WHY WE NEED THESE LIBRARIES:\n# - json: For structured data handling (prompt templates, responses)\n# - typing: For type hints (Dict, List) to make code clearer\n#\n# WHAT THIS CELL DOES:\n# 1. Imports standard Python libraries we'll use throughout the notebook\n# 2. These libraries help us organize and display prompt examples\n# 3. In production, you'd also import LLM API libraries (openai, anthropic, etc.)\n#\n# ğŸ“ NOTE: This notebook demonstrates prompt STRUCTURE and DESIGN\n#          To actually call an LLM, you would add:\n#          - import openai  (for OpenAI/Azure OpenAI)\n#          - import anthropic  (for Claude)\n#          - And configure your API keys\n\nimport json  # For working with JSON data structures\nfrom typing import Dict, List  # Type hints for better code readability\n\n# âœ… CHECKPOINT: Run this cell to ensure libraries are available\nprint(\"âœ… Libraries imported successfully!\")\nprint(\"ğŸ“š Ready to explore prompt engineering patterns...\")\n\n# ğŸ’¡ TIP FOR STUDENTS: \n# If you see import errors, install missing packages with:\n# pip install <package-name>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ğŸ¯ GOAL: Demonstrate a zero-shot prompt (no examples, direct instruction)\n# WHY: Zero-shot is fastest to create but may lack specificity\n# EXAMPLE: Simple, one-sentence instruction\n\nzero_shot_prompt = \"\"\"\nWrite an email response to a customer who received a damaged product.\n\"\"\"\n\nprint(\"Zero-Shot Prompt:\")\nprint(zero_shot_prompt)\nprint(\"\\n\" + \"=\"*50 + \"\\n\")\n\n# âœ… ANALYSIS: Evaluate this prompt's characteristics\n\n# Strengths:\nstrengths_zero_shot = [\n    \"Simple and quick to create\",\n    \"Works well for common tasks\",\n    \"No need for extensive context\"\n]\n\n# Weaknesses:\nweaknesses_zero_shot = [\n    \"May lack company-specific tone\",\n    \"Might miss important policy details\",\n    \"Generic response without specific guidance\"\n]\n\nprint(\"Strengths:\", \"\\n- \" + \"\\n- \".join(strengths_zero_shot))\nprint(\"\\nWeaknesses:\", \"\\n- \" + \"\\n- \".join(weaknesses_zero_shot))\n\n# ğŸ“ LEARNING CHECKPOINT: Notice how this prompt is direct but vague. \n# Would an LLM know your refund policy? Your brand voice? Response time expectations?\nprint(\"\\nğŸ’­ Think: What information is missing that would make this prompt better?\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# EXAMPLE 1: Zero-Shot Prompt (Simplest Approach)\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n# ğŸ¯ WHAT IS A ZERO-SHOT PROMPT?\n# A direct instruction with NO examples or context\n# Just tells the LLM what to do in plain language\n\n# ğŸ“ CREATING THE PROMPT:\n# This is a triple-quoted string (can span multiple lines)\n# Contains a single, simple instruction\nzero_shot_prompt = \"\"\"\nWrite an email response to a customer who received a damaged product.\n\"\"\"\n\n# ğŸ“¤ OUTPUT: Display the prompt we created\nprint(\"=\"*60)\nprint(\"ZERO-SHOT PROMPT EXAMPLE:\")\nprint(\"=\"*60)\nprint(zero_shot_prompt)\nprint(\"\\n\" + \"=\"*60 + \"\\n\")\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# ANALYSIS: What Makes This Prompt Work (and Not Work)\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n# âœ… STRENGTHS: Why you might use this approach\nstrengths_zero_shot = [\n    \"Simple and quick to create (takes 10 seconds)\",\n    \"Works well for common, well-understood tasks\",\n    \"No need for extensive context or examples\",\n    \"Minimal token usage = lower cost\",\n    \"Easy to test and iterate\"\n]\n\n# âŒ WEAKNESSES: Where this approach falls short\nweaknesses_zero_shot = [\n    \"May lack company-specific tone (sounds generic)\",\n    \"Might miss important policy details (refund policy, timeline)\",\n    \"Generic response without specific guidance\",\n    \"Doesn't capture brand voice or values\",\n    \"Output quality varies based on LLM's training data\"\n]\n\n# ğŸ“Š DISPLAY THE ANALYSIS:\nprint(\"âœ… STRENGTHS:\")\nfor strength in strengths_zero_shot:\n    print(f\"   â€¢ {strength}\")\n\nprint(\"\\nâŒ WEAKNESSES:\")\nfor weakness in weaknesses_zero_shot:\n    print(f\"   â€¢ {weakness}\")\n\nprint(\"\\n\" + \"=\"*60)\n\n# ğŸ’­ CRITICAL THINKING EXERCISE:\n# Think about what information is MISSING from this prompt:\n# - What's your refund policy?\n# - What's your brand voice (formal? casual? empathetic?)?\n# - How quickly should customers expect a response?\n# - What specific steps should be included in the email?\n# - Should you offer alternatives to refund?\n\nprint(\"\\nğŸ’¡ REFLECTION QUESTION:\")\nprint(\"   What information would YOU add to make this prompt better?\")\nprint(\"   (Write your thoughts in the markdown cell above)\")\n\n# ğŸ“ KEY LEARNING:\n# Zero-shot works for COMMON tasks where the LLM has seen many examples\n# in its training data. For specialized/unique tasks, you need more guidance."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Role-Playing Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# EXAMPLE 2: Role-Playing Prompt (Adding Persona & Context)\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n# ğŸ¯ WHAT IS A ROLE-PLAYING PROMPT?\n# Assigns a specific role or persona to the LLM BEFORE giving the task\n# Provides context about WHO is speaking and WHY\n\n# ğŸ“ CREATING THE PROMPT:\n# Notice the structure:\n# 1. ROLE DEFINITION: \"You are a [specific role]...\"\n# 2. CONTEXT: Background about the company/situation\n# 3. PERSONALITY TRAITS: How this role should behave\n# 4. TASK: What you want them to do\n\nrole_playing_prompt = \"\"\"\nYou are a senior customer service representative at a premium e-commerce company \nknown for exceptional customer care. You always maintain a warm, empathetic tone \nwhile being professional and solution-oriented.\n\nWrite an email response to a customer who received a damaged product. The customer \nis understandably frustrated but has been polite in their complaint.\n\"\"\"\n\n# ğŸ’¡ WHY THIS WORKS:\n# - LLMs respond differently based on the role you assign\n# - \"Senior representative\" â†’ more confident, experienced tone\n# - \"Premium company\" â†’ higher quality standards\n# - \"Warm, empathetic\" â†’ specific tone guidance\n# - Customer context (\"frustrated but polite\") â†’ appropriate response level\n\n# ğŸ“¤ OUTPUT: Display the prompt\nprint(\"=\"*60)\nprint(\"ROLE-PLAYING PROMPT EXAMPLE:\")\nprint(\"=\"*60)\nprint(role_playing_prompt)\nprint(\"\\n\" + \"=\"*60 + \"\\n\")\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# ANALYSIS: Comparing to Zero-Shot\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n# âœ… STRENGTHS: What role-playing adds\nstrengths_role = [\n    \"Establishes appropriate tone and style (warm, empathetic)\",\n    \"Provides context about company values (premium, exceptional care)\",\n    \"More likely to generate brand-aligned responses\",\n    \"Adds empathy and professionalism automatically\",\n    \"Accounts for customer emotional state (frustrated but polite)\"\n]\n\n# âŒ WEAKNESSES: Trade-offs to consider\nweaknesses_role = [\n    \"Longer prompt requires more tokens (costs more)\",\n    \"May still lack specific policy guidance (refund process)\",\n    \"Requires careful role definition (wrong role = wrong output)\",\n    \"Can be overkill for simple, transactional tasks\",\n    \"Need to test that role definition matches desired output\"\n]\n\n# ğŸ“Š DISPLAY THE ANALYSIS:\nprint(\"âœ… STRENGTHS OVER ZERO-SHOT:\")\nfor strength in strengths_role:\n    print(f\"   â€¢ {strength}\")\n\nprint(\"\\nâŒ TRADE-OFFS TO CONSIDER:\")\nfor weakness in weaknesses_role:\n    print(f\"   â€¢ {weakness}\")\n\nprint(\"\\n\" + \"=\"*60)\n\n# ğŸ” COMPARISON TO ZERO-SHOT:\nprint(\"\\nğŸ“Š ZERO-SHOT vs ROLE-PLAYING:\")\nprint(\"   Zero-Shot:     'Write an email...' \")\nprint(\"   Role-Playing:  'You are a senior CSR... Write an email...'\")\nprint(\"\")\nprint(\"   Difference: Role-playing adds ~40 tokens but improves tone/quality\")\nprint(\"   Cost impact: ~$0.0001 more per request (negligible for most cases)\")\n\n# ğŸ“ WHEN TO USE ROLE-PLAYING:\nprint(\"\\nğŸ’¡ USE ROLE-PLAYING WHEN:\")\nprint(\"   â€¢ Brand voice consistency is critical\")\nprint(\"   â€¢ Tone matters as much as content\")\nprint(\"   â€¢ You want specific expertise perspective\")\nprint(\"   â€¢ Building customer trust is important\")\n\n# ğŸ“ SKIP ROLE-PLAYING WHEN:\nprint(\"\\nâš ï¸  SKIP ROLE-PLAYING WHEN:\")\nprint(\"   â€¢ Task is purely formulaic/transactional\")\nprint(\"   â€¢ Token budget is extremely limited\")\nprint(\"   â€¢ Output format matters more than tone\")"
  },
  {
   "cell_type": "markdown",
   "source": "### Visual Comparison: Three Prompt Approaches\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚              PROMPT TYPE COMPARISON FLOWCHART                        â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nZERO-SHOT PROMPT\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ User: Simple Request â”‚ â”€â”€â†’ LLM generates response\nâ”‚ \"Write email...\"     â”‚     based on general knowledge\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nPros: Fast, easy          Cons: May miss specifics\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nROLE-PLAYING PROMPT\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ User: \"You are expert...\" +  â”‚\nâ”‚ Context about role +         â”‚ â”€â”€â†’ LLM adopts persona,\nâ”‚ \"Write email...\"             â”‚     generates contextual\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     response\n\nPros: Better tone, consistency   Cons: Longer prompt, more tokens\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nDETAILED INSTRUCTION PROMPT\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ User: \"Write email with:\"        â”‚\nâ”‚ 1. Acknowledge issue             â”‚\nâ”‚ 2. Express empathy               â”‚ â”€â”€â†’ LLM follows exact\nâ”‚ 3. Offer solutions               â”‚     instructions,\nâ”‚ 4. Tone: professional            â”‚     structured output\nâ”‚ 5. Length: 150-200 words         â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nPros: Maximum control, predictable   Cons: Most effort, rigid\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nSELECTION DECISION TREE:\n                    â”Œâ”€â”€â”€ Task is common? â”€â”€â”€â†’ YES â”€â”€â†’ Zero-Shot\n                    â”‚\n    Start Here â”€â”€â”€â”€â”€â”¼â”€â”€â”€ Need brand voice? â”€â†’ YES â”€â”€â†’ Role-Playing\n                    â”‚\n                    â””â”€â”€â”€ Need precision? â”€â”€â†’ YES â”€â”€â†’ Detailed Instruction\n```\n\n> ğŸ’¡ **Visual Guide Interpretation**: \n> - **Complexity increases** moving down the chart (Zero-Shot â†’ Detailed)\n> - **Control increases** moving down the chart\n> - **Token usage increases** moving down the chart  \n> - **Output quality** depends on use case, not just complexity\n\n### Prompt Evolution Example\n\nSee how a prompt can evolve from simple to sophisticated:\n\n```\nVERSION 1 (Zero-Shot):\n\"Summarize this customer feedback\"\n\nVERSION 2 (Role-Playing):\n\"You are a product manager analyzing customer feedback. \nSummarize this feedback focusing on actionable insights.\"\n\nVERSION 3 (Detailed Instruction):\n\"You are a product manager analyzing customer feedback.\nSummarize the feedback below with:\n1. Top 3 themes (with frequency counts)\n2. Critical issues (severity: high/medium/low)\n3. Actionable recommendations (priority-ranked)\n4. Suggested owner for each recommendation\nFormat: Bullet points, max 200 words\"\n```\n\n**Evolution Impact**:\n- Version 1: Generic summary\n- Version 2: Product-focused perspective\n- Version 3: Structured, actionable output ready for team review\n\n> ğŸ“ **Learning Activity**: Try taking one of your prompts from earlier and evolving it through these three versions. Notice how the output quality changes!",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Detailed Instruction Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# EXAMPLE 3: Detailed Instruction Prompt (Maximum Control)\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n# ğŸ¯ WHAT IS A DETAILED INSTRUCTION PROMPT?\n# Provides STEP-BY-STEP instructions for exactly what the output should contain\n# Specifies format, tone, length, and specific elements to include\n\n# ğŸ“ CREATING THE PROMPT:\n# Notice the structure:\n# 1. SITUATION: Context about the scenario\n# 2. INSTRUCTIONS: Numbered list of specific elements to include\n# 3. CONSTRAINTS: Tone, length, style requirements\n#\n# This is like giving the LLM a checklist to follow\n\ndetailed_instruction_prompt = \"\"\"\nWrite a customer service email response with the following requirements:\n\nSituation: Customer received a damaged product\n\nInstructions:\n1. Acknowledge the issue and apologize\n2. Express empathy for the inconvenience\n3. Offer two solutions: full refund or replacement shipment\n4. Mention that return shipping will be covered\n5. Provide a direct contact number for urgent assistance\n6. Thank them for their patience\n\nTone: Professional, warm, and solution-focused\nLength: 150-200 words\n\"\"\"\n\n# ğŸ’¡ WHY THIS STRUCTURE IS POWERFUL:\n# - LLM has EXACT requirements (no guessing)\n# - Ensures company policies are followed (refund OR replacement)\n# - Specifies all elements needed (apology, solutions, contact info)\n# - Controls output length (important for email constraints)\n# - Sets tone explicitly (not relying on implicit understanding)\n\n# ğŸ“¤ OUTPUT: Display the prompt\nprint(\"=\"*60)\nprint(\"DETAILED INSTRUCTION PROMPT EXAMPLE:\")\nprint(\"=\"*60)\nprint(detailed_instruction_prompt)\nprint(\"\\n\" + \"=\"*60 + \"\\n\")\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# ANALYSIS: Maximum Control vs Effort Trade-off\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n# âœ… STRENGTHS: Why this is the \"gold standard\" for important tasks\nstrengths_detailed = [\n    \"Very specific guidance on content (6 required elements)\",\n    \"Includes policy-specific information (refund, replacement, shipping)\",\n    \"Structured approach ensures completeness (nothing forgotten)\",\n    \"Clear expectations for tone and length (predictable output)\",\n    \"Reduces variation across different LLM responses (consistency)\",\n    \"Easier to audit/review (checklist matching)\"\n]\n\n# âŒ WEAKNESSES: The cost of control\nweaknesses_detailed = [\n    \"Takes more time to create (5-10 minutes vs 30 seconds)\",\n    \"Longest prompt (uses more tokens = higher cost)\",\n    \"May feel formulaic if overused (less natural-sounding)\",\n    \"Less flexible for variations (rigid structure)\",\n    \"Requires updating if policies change\",\n    \"Can be overkill for simple, one-off tasks\"\n]\n\n# ğŸ“Š DISPLAY THE ANALYSIS:\nprint(\"âœ… STRENGTHS (Why Use This):\")\nfor strength in strengths_detailed:\n    print(f\"   â€¢ {strength}\")\n\nprint(\"\\nâŒ WEAKNESSES (Trade-offs):\")\nfor weakness in weaknesses_detailed:\n    print(f\"   â€¢ {weakness}\")\n\nprint(\"\\n\" + \"=\"*60)\n\n# ğŸ“Š COMPARISON ACROSS ALL THREE TYPES:\nprint(\"\\nğŸ“Š COMPARISON: Zero-Shot â†’ Role-Playing â†’ Detailed\")\nprint(\"\")\nprint(\"   Prompt Length:  Short â†’ Medium â†’ Long\")\nprint(\"   Token Cost:     Low â†’ Medium â†’ High\")\nprint(\"   Setup Time:     Fast â†’ Medium â†’ Slow\")\nprint(\"   Output Control: Low â†’ Medium â†’ High\")\nprint(\"   Consistency:    Low â†’ Medium â†’ High\")\nprint(\"   Flexibility:    High â†’ Medium â†’ Low\")\n\n# ğŸ“ DECISION FRAMEWORK:\nprint(\"\\n\" + \"=\"*60)\nprint(\"ğŸ’¡ WHEN TO USE EACH TYPE:\")\nprint(\"=\"*60)\nprint(\"\\n1ï¸âƒ£  ZERO-SHOT â†’ Quick, common tasks where you trust the LLM\")\nprint(\"    Example: 'Summarize this article'\")\nprint(\"\")\nprint(\"2ï¸âƒ£  ROLE-PLAYING â†’ Brand voice matters, need consistent tone\")\nprint(\"    Example: 'You are a friendly chatbot... Answer this question'\")\nprint(\"\")\nprint(\"3ï¸âƒ£  DETAILED â†’ Critical tasks, specific policies, high stakes\")\nprint(\"    Example: Customer complaints, legal docs, compliance emails\")\n\n# ğŸ’° COST ANALYSIS (approximate):\nprint(\"\\nğŸ’° ROUGH COST COMPARISON (GPT-4):\")\nprint(\"   Zero-Shot:     ~10 tokens  = $0.0003\")\nprint(\"   Role-Playing:  ~50 tokens  = $0.0015\")\nprint(\"   Detailed:      ~120 tokens = $0.0036\")\nprint(\"\")\nprint(\"   For 1,000 emails: $0.30 vs $1.50 vs $3.60\")\nprint(\"   Decision: Is the extra $3.30 worth better quality? Usually YES!\")\n\n# ğŸ¯ BOTTOM LINE:\nprint(\"\\n\" + \"=\"*60)\nprint(\"ğŸ¯ KEY TAKEAWAY:\")\nprint(\"=\"*60)\nprint(\"   Don't optimize for PROMPT LENGTH\")\nprint(\"   Optimize for OUTPUT QUALITY\")\nprint(\"\")\nprint(\"   An extra $0.003 per prompt is nothing compared to:\")\nprint(\"   â€¢ Customer satisfaction\")\nprint(\"   â€¢ Brand reputation\")  \nprint(\"   â€¢ Time saved on revisions\")\nprint(\"   â€¢ Consistency across team\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prompt Evaluation Framework\n",
    "\n",
    "When evaluating prompts, consider these dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# PROMPT EVALUATION MATRIX: Quantitative Comparison\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n# ğŸ¯ GOAL: Create a data-driven comparison of the three prompt types\n# \n# WHY: Numbers help make decisions! This matrix scores each prompt type\n#      across 7 important criteria on a 1-5 scale (5 = best)\n#\n# HOW TO USE THIS: \n# - Look at which criteria matter MOST for your use case\n# - Choose the prompt type that scores highest on YOUR priorities\n\n# ğŸ“Š Import pandas for creating a nice comparison table\nimport pandas as pd\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# SCORING METHODOLOGY (Scale: 1-5, where 5 is best)\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# \n# Based on research and practical experience:\n# - Clarity: How clear are the instructions?\n# - Specificity: How detailed is the guidance?\n# - Context Provision: How much background info is given?\n# - Token Efficiency: How short is the prompt?\n# - Ease of Creation: How fast can you write it?\n# - Output Consistency: How predictable is the result?\n# - Business Alignment: How well does it match company needs?\n\n# ğŸ“Š CREATE THE COMPARISON MATRIX:\nevaluation_criteria = {\n    'Criteria': [\n        'Clarity',              # How clear are the instructions?\n        'Specificity',          # How detailed is the guidance?\n        'Context Provision',    # How much background context?\n        'Token Efficiency',     # How short is the prompt? (lower cost)\n        'Ease of Creation',     # How fast to create?\n        'Output Consistency',   # How predictable/repeatable?\n        'Business Alignment'    # How well matches company needs?\n    ],\n    \n    # Zero-Shot Scores (Simple = Fast but vague)\n    'Zero-Shot': [\n        3,  # Clarity: Somewhat clear but minimal\n        2,  # Specificity: Not very specific\n        1,  # Context: Almost no context provided\n        5,  # Token Efficiency: Very short = cheap\n        5,  # Ease of Creation: Super fast to write\n        2,  # Output Consistency: Varies a lot\n        2   # Business Alignment: Generic, not tailored\n    ],\n    \n    # Role-Playing Scores (Balance of control and ease)\n    'Role-Playing': [\n        4,  # Clarity: Pretty clear with role context\n        3,  # Specificity: Medium specificity\n        4,  # Context: Good context through role\n        3,  # Token Efficiency: Medium length\n        3,  # Ease of Creation: Takes some thought\n        4,  # Output Consistency: Good consistency\n        4   # Business Alignment: Better brand fit\n    ],\n    \n    # Detailed Instruction Scores (Maximum control but more work)\n    'Detailed Instruction': [\n        5,  # Clarity: Crystal clear step-by-step\n        5,  # Specificity: Very detailed\n        5,  # Context: Complete context provided\n        2,  # Token Efficiency: Long = more expensive\n        2,  # Ease of Creation: Takes time to create\n        5,  # Output Consistency: Highly consistent\n        5   # Business Alignment: Perfect policy alignment\n    ]\n}\n\n# ğŸ“‹ CREATE A PANDAS DATAFRAME (like an Excel table):\ndf_evaluation = pd.DataFrame(evaluation_criteria)\n\n# ğŸ“¤ DISPLAY THE COMPARISON TABLE:\nprint(\"=\" * 80)\nprint(\"PROMPT TYPE COMPARISON MATRIX\")\nprint(\"Scale: 1-5, where 5 = best\")\nprint(\"=\" * 80)\nprint(df_evaluation.to_string(index=False))\nprint(\"=\" * 80)\n\n# ğŸ” CALCULATE TOTAL SCORES (sum across all criteria):\nprint(\"\\nğŸ“Š TOTAL SCORES (out of 35 possible):\")\nprint(f\"   Zero-Shot:           {sum(evaluation_criteria['Zero-Shot'])}/35 points\")\nprint(f\"   Role-Playing:        {sum(evaluation_criteria['Role-Playing'])}/35 points\")\nprint(f\"   Detailed Instruction: {sum(evaluation_criteria['Detailed Instruction'])}/35 points\")\n\n# ğŸ’¡ INTERPRETING THE SCORES:\nprint(\"\\n\" + \"=\" * 80)\nprint(\"ğŸ’¡ WHAT THESE SCORES MEAN:\")\nprint(\"=\" * 80)\nprint(\"\\nâŒ Zero-Shot (20/35): Fast and cheap, but inconsistent and generic\")\nprint(\"   Best for: One-off tasks, experimenting, non-critical outputs\")\nprint(\"\")\nprint(\"âš–ï¸  Role-Playing (25/35): Balanced approach with good consistency\")\nprint(\"   Best for: Customer-facing content, brand voice matters\")\nprint(\"\")\nprint(\"âœ… Detailed (29/35): Highest quality and consistency, but takes effort\")\nprint(\"   Best for: Critical tasks, compliance, high-stakes communications\")\n\n# ğŸ“ KEY INSIGHT:\nprint(\"\\n\" + \"=\" * 80)\nprint(\"ğŸ¯ KEY DECISION FRAMEWORK:\")\nprint(\"=\" * 80)\nprint(\"\\n   There is NO 'best' prompt type!\")\nprint(\"   The right choice depends on YOUR priorities:\")\nprint(\"\")\nprint(\"   â€¢ Need speed? â†’ Zero-Shot\")\nprint(\"   â€¢ Need brand voice? â†’ Role-Playing\")\nprint(\"   â€¢ Need precision? â†’ Detailed Instruction\")\nprint(\"\")\nprint(\"   âš ï¸  Most people under-use Detailed Instructions\")\nprint(\"       because they overestimate the cost and underestimate the value!\")\n\n# ğŸ”¢ COST-BENEFIT CALCULATION:\nprint(\"\\nğŸ’° COST-BENEFIT REALITY CHECK:\")\nprint(\"   If Detailed costs $0.003 more but:\")\nprint(\"   â€¢ Saves 5 minutes of editing = $10 in labor\")\nprint(\"   â€¢ Increases customer satisfaction = Priceless\")\nprint(\"   â€¢ Reduces risk of wrong information = Priceless\")\nprint(\"\")\nprint(\"   â†’ The extra $0.003 is a NO-BRAINER investment!\")\n\n# ğŸ“ NOTE FOR STUDENTS:\nprint(\"\\nğŸ“ EXERCISE: Try creating your own scoring matrix\")\nprint(\"   What criteria matter MOST in your organization?\")\nprint(\"   Would you weight some criteria higher than others?\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Best Practices for Effective Prompts\n",
    "\n",
    "### Do's:\n",
    "- âœ“ Be specific about what you want\n",
    "- âœ“ Provide relevant context\n",
    "- âœ“ Use clear, unambiguous language\n",
    "- âœ“ Specify output format when needed\n",
    "- âœ“ Include constraints (length, tone, style)\n",
    "- âœ“ Test and iterate on your prompts\n",
    "\n",
    "### Don'ts:\n",
    "- âœ— Use vague or ambiguous instructions\n",
    "- âœ— Assume the model has real-time information\n",
    "- âœ— Expect perfect outputs without iteration\n",
    "- âœ— Overload with unnecessary information\n",
    "- âœ— Use contradictory instructions\n",
    "- âœ— Ignore the importance of formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hands-On Practice Activity\n",
    "\n",
    "### Task: Create Three Different Prompts\n",
    "\n",
    "Select a common business task from your field (examples below) and create three different prompts:\n",
    "1. A zero-shot prompt\n",
    "2. A role-playing prompt\n",
    "3. A detailed instruction prompt\n",
    "\n",
    "**Business Task Examples:**\n",
    "- Summarizing a quarterly report\n",
    "- Drafting a meeting agenda\n",
    "- Creating a project status update\n",
    "- Writing a product description\n",
    "- Analyzing customer feedback\n",
    "\n",
    "Use the cells below to create your prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR TURN: Define your business task\n",
    "my_business_task = \"\"\"\n",
    "Description of your chosen business task:\n",
    "\n",
    "[Write your task description here]\n",
    "\"\"\"\n",
    "\n",
    "print(my_business_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR TURN: Create a zero-shot prompt\n",
    "my_zero_shot_prompt = \"\"\"\n",
    "[Write your zero-shot prompt here]\n",
    "\"\"\"\n",
    "\n",
    "print(\"My Zero-Shot Prompt:\")\n",
    "print(my_zero_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR TURN: Create a role-playing prompt\n",
    "my_role_playing_prompt = \"\"\"\n",
    "[Write your role-playing prompt here]\n",
    "\"\"\"\n",
    "\n",
    "print(\"My Role-Playing Prompt:\")\n",
    "print(my_role_playing_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR TURN: Create a detailed instruction prompt\n",
    "my_detailed_prompt = \"\"\"\n",
    "[Write your detailed instruction prompt here]\n",
    "\"\"\"\n",
    "\n",
    "print(\"My Detailed Instruction Prompt:\")\n",
    "print(my_detailed_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Self-Evaluation Exercise\n",
    "\n",
    "Evaluate your three prompts using the framework below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# SELF-EVALUATION EXERCISE: Rate YOUR Own Prompts\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n# ğŸ¯ GOAL: Learn to critically evaluate your own prompt designs\n# \n# WHY THIS MATTERS: The best way to improve is to:\n# 1. Create prompts\n# 2. Evaluate them honestly\n# 3. Identify weaknesses\n# 4. Iterate and improve\n#\n# HOW TO USE THIS: \n# - Replace the 0s below with your ratings (1-5 scale)\n# - Be honest! The goal is learning, not high scores\n# - Lower scores show you where to focus improvement efforts\n\n# ğŸ“Š CREATE YOUR EVALUATION FRAMEWORK:\n# (This is the same structure as the professional evaluation above)\nmy_evaluation = {\n    'Criteria': [\n        'Clarity',                  # How clear are my instructions?\n        'Specificity',              # How specific am I about what I want?\n        'Context Provision',        # Did I provide enough background?\n        'Expected Output Quality',  # Will this produce good results?\n        'Ease of Use'               # How easy is this to reuse/modify?\n    ],\n    \n    # âš ï¸  REPLACE THESE 0s WITH YOUR RATINGS (1-5, where 5 = best)\n    # INSTRUCTIONS:\n    # 1. Look at your Zero-Shot prompt above (in cell 21)\n    # 2. For each criteria, rate it honestly on 1-5 scale\n    # 3. Same for Role-Playing (cell 22) and Detailed (cell 23)\n    \n    'My Zero-Shot': [0, 0, 0, 0, 0],      # â† Replace these 0s with your ratings\n    'My Role-Playing': [0, 0, 0, 0, 0],   # â† Replace these 0s with your ratings\n    'My Detailed': [0, 0, 0, 0, 0]        # â† Replace these 0s with your ratings\n}\n\n# ğŸ“‹ CREATE THE EVALUATION TABLE:\ndf_my_eval = pd.DataFrame(my_evaluation)\n\n# ğŸ“¤ DISPLAY YOUR RATINGS:\nprint(\"=\" * 80)\nprint(\"MY PROMPT EVALUATION\")\nprint(\"Scale: 1-5, where 5 = best\")\nprint(\"=\" * 80)\nprint(df_my_eval.to_string(index=False))\nprint(\"=\" * 80)\n\n# ğŸ” CALCULATE YOUR TOTAL SCORES (if you filled them in):\nif df_my_eval['My Zero-Shot'].sum() > 0:  # Only if you added ratings\n    print(\"\\nğŸ“Š YOUR TOTAL SCORES (out of 25 possible):\")\n    print(f\"   My Zero-Shot:     {df_my_eval['My Zero-Shot'].sum()}/25 points\")\n    print(f\"   My Role-Playing:  {df_my_eval['My Role-Playing'].sum()}/25 points\")\n    print(f\"   My Detailed:      {df_my_eval['My Detailed'].sum()}/25 points\")\n    \n    # ğŸ“ PROVIDE FEEDBACK BASED ON SCORES:\n    print(\"\\n\" + \"=\" * 80)\n    print(\"ğŸ’¡ INTERPRETING YOUR SCORES:\")\n    print(\"=\" * 80)\n    \n    max_score = max(df_my_eval['My Zero-Shot'].sum(), \n                    df_my_eval['My Role-Playing'].sum(), \n                    df_my_eval['My Detailed'].sum())\n    \n    if max_score >= 20:\n        print(\"âœ… Excellent! You have strong prompt engineering instincts\")\n    elif max_score >= 15:\n        print(\"âš–ï¸  Good start! Focus on improving your lowest-scored criteria\")\n    else:\n        print(\"ğŸ“š Keep practicing! Review the examples and try again\")\n    \n    # ğŸ¯ IDENTIFY AREAS FOR IMPROVEMENT:\n    print(\"\\nğŸ¯ AREAS FOR IMPROVEMENT:\")\n    for i, criterion in enumerate(my_evaluation['Criteria']):\n        avg_score = (df_my_eval.iloc[i, 1] + df_my_eval.iloc[i, 2] + df_my_eval.iloc[i, 3]) / 3\n        if avg_score < 3:\n            print(f\"   âš ï¸  {criterion}: Work on this! (Average: {avg_score:.1f}/5)\")\nelse:\n    # User hasn't filled in ratings yet\n    print(\"\\nâš ï¸  No ratings entered yet!\")\n    print(\"   Go back and replace the 0s above with your ratings (1-5 scale)\")\n    print(\"   Then re-run this cell to see your scores and feedback\")\n\n# ğŸ’¡ REFLECTION PROMPTS:\nprint(\"\\n\" + \"=\" * 80)\nprint(\"ğŸ¤” REFLECTION QUESTIONS:\")\nprint(\"=\" * 80)\nprint(\"\\n1. Which prompt type was EASIEST for you to create? Why?\")\nprint(\"\")\nprint(\"2. Which prompt type do you think will produce the BEST results? Why?\")\nprint(\"\")\nprint(\"3. For your business task, which approach would you actually USE?\")\nprint(\"   (Hint: Sometimes 'easiest' isn't the best choice!)\")\nprint(\"\")\nprint(\"4. What surprised you about this exercise?\")\nprint(\"\")\nprint(\"5. What will you do differently in your next prompt?\")\n\n# ğŸ“ NEXT STEPS:\nprint(\"\\n\" + \"=\" * 80)\nprint(\"ğŸ“ NEXT STEPS:\")\nprint(\"=\" * 80)\nprint(\"\\nâœ… If you haven't yet, complete your prompts in cells 21-23 above\")\nprint(\"âœ… Fill in your ratings in this cell and re-run\")\nprint(\"âœ… Compare your scores to the professional examples\")\nprint(\"âœ… Identify ONE thing to improve in each prompt type\")\nprint(\"âœ… Rewrite and rate again - see if your scores improve!\")\n\n# ğŸ† MASTERY CHALLENGE:\nprint(\"\\n\" + \"=\" * 80)\nprint(\"ğŸ† MASTERY CHALLENGE:\")\nprint(\"=\" * 80)\nprint(\"\\n   Can you get all three prompts scoring 20+ out of 25?\")\nprint(\"   That's the mark of solid prompt engineering skills!\")\nprint(\"\")\nprint(\"   Remember: This is NOT about making the longest prompt\")\nprint(\"   It's about making the MOST EFFECTIVE prompt for YOUR use case\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Discussion Questions\n",
    "\n",
    "Reflect on the following questions:\n",
    "\n",
    "1. **Which prompt style (zero-shot, role-playing, or detailed instruction) do you think would work best for your chosen business task? Why?**\n",
    "\n",
    "2. **What are the potential strengths of each prompting approach?**\n",
    "\n",
    "3. **What are the potential weaknesses or limitations of each approach?**\n",
    "\n",
    "4. **How might the choice of prompt style impact:**\n",
    "   - Output quality\n",
    "   - Consistency across multiple uses\n",
    "   - Time invested in prompt creation\n",
    "   - Cost (token usage)\n",
    "\n",
    "5. **In what business scenarios would you prioritize:**\n",
    "   - Speed and simplicity (zero-shot)?\n",
    "   - Tone and brand alignment (role-playing)?\n",
    "   - Precision and control (detailed instruction)?\n",
    "\n",
    "Use the cell below to write your reflections:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ğŸ“š Additional Resources & References\n\n### Official Documentation & Guides\n\n**OpenAI**\n- [Prompt Engineering Guide](https://platform.openai.com/docs/guides/prompt-engineering) - Best practices and strategies\n- [GPT Best Practices](https://platform.openai.com/docs/guides/gpt-best-practices) - Six strategies for better results\n- [OpenAI Cookbook](https://github.com/openai/openai-cookbook) - Code examples and techniques\n\n**Anthropic Claude**\n- [Prompt Engineering Documentation](https://docs.anthropic.com/claude/docs/prompt-engineering) - Claude-specific techniques\n- [Prompt Library](https://docs.anthropic.com/claude/prompt-library) - Reusable prompt templates\n- [Claude System Prompts](https://docs.anthropic.com/claude/docs/system-prompts) - Advanced configuration\n\n**Google**\n- [Introduction to Prompt Design](https://developers.google.com/machine-learning/resources/prompt-eng) - Fundamentals\n- [Gemini Prompting Guide](https://ai.google.dev/docs/prompt_best_practices) - Google's best practices\n\n### Academic Papers & Research\n\n**Foundational Papers**\n1. **Brown, T., et al. (2020).** *Language Models are Few-Shot Learners.* NeurIPS.\n   - [arXiv:2005.14165](https://arxiv.org/abs/2005.14165)\n   - Introduced GPT-3 and demonstrated few-shot learning capabilities\n\n2. **Liu, P., et al. (2023).** *Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods.*\n   - [DOI: 10.1145/3560815](https://doi.org/10.1145/3560815)\n   - Comprehensive survey of prompting techniques\n\n3. **White, J., et al. (2023).** *A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT.*\n   - [arXiv:2302.11382](https://arxiv.org/abs/2302.11382)\n   - Catalog of reusable prompt patterns\n\n**Advanced Topics** (Preview for upcoming weeks)\n4. **Wei, J., et al. (2023).** *Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.*\n   - [arXiv:2201.11903](https://arxiv.org/abs/2201.11903)\n   - Foundation for Week 2: Chain-of-Thought techniques\n\n5. **Kojima, T., et al. (2023).** *Large Language Models are Zero-Shot Reasoners.*\n   - [arXiv:2205.11916](https://arxiv.org/abs/2205.11916)\n   - Zero-shot reasoning with \"Let's think step by step\"\n\n### Industry Reports & Case Studies\n\n**Consulting Firm Reports**\n- **McKinsey (2023)**: \"The Economic Potential of Generative AI\" - [mckinsey.com/generative-ai](https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/the-economic-potential-of-generative-ai-the-next-productivity-frontier)\n- **BCG (2023)**: \"How Generative AI Is Changing Creative Work\" - [bcg.com/ai](https://www.bcg.com/publications/2023/how-generative-ai-is-changing-creative-work)\n- **Gartner (2023)**: \"Top Strategic Technology Trends for 2024\" - Focus on AI TRiSM\n\n**Company Engineering Blogs**\n- **Shopify Engineering**: [shopify.engineering](https://shopify.engineering) - AI in e-commerce\n- **Intercom Product**: [intercom.com/blog/product](https://www.intercom.com/blog/product) - Customer service AI\n- **Netflix Tech Blog**: [netflixtechblog.com](https://netflixtechblog.com) - ML applications at scale\n- **Airbnb Engineering**: [medium.com/airbnb-engineering](https://medium.com/airbnb-engineering) - AI/ML in travel\n\n### Interactive Learning Resources\n\n**Online Courses** (Complementary)\n- [DeepLearning.AI - ChatGPT Prompt Engineering for Developers](https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/)\n- [Learn Prompting](https://learnprompting.org/) - Free comprehensive course\n- [Prompt Engineering Guide](https://www.promptingguide.ai/) - Open-source guide\n\n**Practice Platforms**\n- [PromptBase](https://promptbase.com/) - Marketplace for prompt examples\n- [ShareGPT](https://sharegpt.com/) - Community-shared conversations\n- [Awesome ChatGPT Prompts](https://github.com/f/awesome-chatgpt-prompts) - Curated prompt examples\n\n### Tools & Utilities\n\n**Prompt Development**\n- [OpenAI Playground](https://platform.openai.com/playground) - Test prompts interactively\n- [Anthropic Console](https://console.anthropic.com/) - Claude prompt testing\n- [LangChain Prompt Templates](https://python.langchain.com/docs/modules/model_io/prompts/) - Programmatic prompts\n\n**Evaluation & Testing**\n- [PromptFoo](https://www.promptfoo.dev/) - Prompt testing framework\n- [Weights & Biases Prompts](https://wandb.ai/site/prompts) - Track prompt experiments\n\n### Books & Long-Form Content\n\n1. **Gebru, T., et al. (2021).** \"Datasheets for Datasets\" - Understanding AI limitations\n2. **Marcus, G. & Davis, E. (2023).** \"Rebooting AI\" - Critical perspective on AI capabilities\n3. **OpenAI (2023).** \"GPT-4 Technical Report\" - [arXiv:2303.08774](https://arxiv.org/abs/2303.08774)\n\n### Visual Learning Resources\n\n**YouTube Channels**\n- [Andrej Karpathy](https://www.youtube.com/@AndrejKarpathy) - Deep dives into LLMs\n- [AI Explained](https://www.youtube.com/@aiexplained-official) - Latest research explained\n\n**Infographics & Visual Guides**\n- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/) - Jay Alammar's visualizations\n- [LLM Visualization](https://bbycroft.net/llm) - Interactive LLM architecture\n\n---\n\n## ğŸ”— Quick Links Summary\n\n| Resource Type | Link | Best For |\n|---------------|------|----------|\n| **Quickstart** | [OpenAI Guide](https://platform.openai.com/docs/guides/prompt-engineering) | Immediate application |\n| **Deep Dive** | [Liu et al. Survey](https://doi.org/10.1145/3560815) | Academic understanding |\n| **Practical** | [Prompt Pattern Catalog](https://arxiv.org/abs/2302.11382) | Reusable templates |\n| **Practice** | [Learn Prompting](https://learnprompting.org/) | Interactive learning |\n| **Tools** | [OpenAI Playground](https://platform.openai.com/playground) | Hands-on testing |\n\n---\n\n*End of Week 1 Notebook*\n\n> ğŸ“Œ **Citation Note**: All academic papers cited include DOI or arXiv links for easy access. Industry sources link to official company publications. This notebook was last updated: November 2024."
  },
  {
   "cell_type": "markdown",
   "source": "## ğŸ“Œ Quick Reference Guide\n\n### Prompt Type Decision Framework\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚              WHEN TO USE EACH PROMPT TYPE                 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                           â”‚\nâ”‚  ZERO-SHOT PROMPTS                                       â”‚\nâ”‚  âœ… Use when:                                             â”‚\nâ”‚     â€¢ Task is common and well-understood                 â”‚\nâ”‚     â€¢ Speed is priority                                  â”‚\nâ”‚     â€¢ Output doesn't need strict brand alignment         â”‚\nâ”‚  âŒ Avoid when:                                           â”‚\nâ”‚     â€¢ Specific tone/style required                       â”‚\nâ”‚     â€¢ Complex instructions needed                        â”‚\nâ”‚                                                           â”‚\nâ”‚  ROLE-PLAYING PROMPTS                                    â”‚\nâ”‚  âœ… Use when:                                             â”‚\nâ”‚     â€¢ Brand voice consistency is critical                â”‚\nâ”‚     â€¢ Specific expertise/perspective needed              â”‚\nâ”‚     â€¢ Building trust through persona                     â”‚\nâ”‚  âŒ Avoid when:                                           â”‚\nâ”‚     â€¢ Task is purely formulaic                           â”‚\nâ”‚     â€¢ Token budget is very limited                       â”‚\nâ”‚                                                           â”‚\nâ”‚  DETAILED INSTRUCTION PROMPTS                            â”‚\nâ”‚  âœ… Use when:                                             â”‚\nâ”‚     â€¢ Precision and control are essential                â”‚\nâ”‚     â€¢ Specific policies must be followed                 â”‚\nâ”‚     â€¢ Output format matters                              â”‚\nâ”‚  âŒ Avoid when:                                           â”‚\nâ”‚     â€¢ You need creative, flexible responses              â”‚\nâ”‚     â€¢ Quick iteration is priority                        â”‚\nâ”‚                                                           â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n### âœ… Self-Assessment Checklist\n\nBefore moving on to Week 2, ensure you can:\n\n**Foundational Knowledge**\n- [ ] Explain what tokenization is and why it matters\n- [ ] Describe the three core principles: clarity, context, specificity\n- [ ] Identify the main limitation of LLMs (pattern matching vs. understanding)\n\n**Practical Skills**\n- [ ] Write a zero-shot prompt for a business task\n- [ ] Create a role-playing prompt with appropriate persona\n- [ ] Design a detailed instruction prompt with clear requirements\n- [ ] Evaluate which prompt type suits a given scenario\n\n**Strategic Thinking**\n- [ ] Explain the trade-offs between prompt complexity and quality\n- [ ] Identify when to invest time in detailed prompts vs. quick iterations\n- [ ] Recognize scenarios where each prompt type excels\n\n**Real-World Application**\n- [ ] Apply these concepts to a problem in your organization\n- [ ] Estimate the impact of better prompts on your workflows\n\n---\n\n### ğŸ¯ If you checked all boxes:\nGreat work! You have a solid foundation in prompt engineering fundamentals. You're ready for Week 2's advanced techniques.\n\n### ğŸ¤” If you couldn't check some boxes:\nNo problem! Review the specific sections you're uncertain about:\n- **Principles**: Revisit Section 1 (Core Principles)\n- **Prompt Types**: Review Section 2 and Examples in Section 3\n- **Evaluation**: Study Section 4 (Evaluation Framework)\n- **Application**: Complete the hands-on exercise in Section 6\n\n---\n\n### ğŸ’¡ Pro Tips for Week 2 Preparation\n\n1. **Start a Prompt Library**: Save your best prompts from this week\n2. **Document What Works**: Note which approaches worked for different tasks\n3. **Identify Complexity**: Find a task that needs multi-step reasoning for Week 2\n4. **Think About Examples**: Consider scenarios where showing examples would help",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Key Takeaways\n",
    "\n",
    "1. **Prompt engineering is foundational** to effectively using LLMs in business contexts\n",
    "\n",
    "2. **Different prompt types serve different purposes:**\n",
    "   - Zero-shot: Quick, simple tasks\n",
    "   - Role-playing: Tone and style consistency\n",
    "   - Detailed instruction: Precision and control\n",
    "\n",
    "3. **Effective prompts balance:**\n",
    "   - Clarity and specificity\n",
    "   - Context and brevity\n",
    "   - Flexibility and control\n",
    "\n",
    "4. **Iteration is essential** - prompts should be tested and refined\n",
    "\n",
    "5. **Context matters** - understanding how LLMs process information helps create better prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Looking Ahead to Week 2\n",
    "\n",
    "Next week, we'll explore advanced prompting techniques:\n",
    "- Few-shot learning (providing examples)\n",
    "- Chain-of-Thought (CoT) prompting for complex reasoning\n",
    "- Self-Ask techniques for problem decomposition\n",
    "\n",
    "**Preparation:** Think about a complex decision-making scenario in your field that requires multi-step reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "- [OpenAI Prompt Engineering Guide](https://platform.openai.com/docs/guides/prompt-engineering)\n",
    "- [Anthropic Prompt Engineering Documentation](https://docs.anthropic.com/claude/docs/prompt-engineering)\n",
    "- [Google's Introduction to Prompt Design](https://developers.google.com/machine-learning/resources/prompt-eng)\n",
    "\n",
    "---\n",
    "\n",
    "*End of Week 1 Notebook*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}