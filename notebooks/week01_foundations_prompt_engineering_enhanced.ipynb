{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“˜ Week 1: Foundations of Effective Prompt Engineering\n",
    "\n",
    "## MBA 590 - Advanced AI Strategy: Prompting and Agentic Frameworks\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This week focuses on reviewing core principles of prompt engineering, understanding LLM input/output processing, and mastering the fundamentals of context, specificity, and clarity in prompt design. We'll introduce basic prompt types including zero-shot and role-playing prompts.\n",
    "\n",
    "### Key Topics\n",
    "- Core principles of prompt engineering\n",
    "- LLM input/output processing\n",
    "- Context, specificity, and clarity in prompt design\n",
    "- Basic prompt types: zero-shot, role-playing, simple instruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ğŸ¯ Learning Objectives\n\nBy the end of this week, you will be able to:\n\n### Knowledge & Understanding\n- [ ] **Explain** the fundamental principles of effective prompt engineering\n- [ ] **Describe** how LLMs process inputs and generate outputs\n- [ ] **Identify** the key elements: context, specificity, and clarity\n\n### Application & Analysis\n- [ ] **Apply** context, specificity, and clarity to improve prompt quality\n- [ ] **Differentiate** between zero-shot, role-playing, and instruction prompts\n- [ ] **Design** prompts appropriate for different business scenarios\n\n### Evaluation & Creation\n- [ ] **Evaluate** the strengths and weaknesses of different prompting approaches\n- [ ] **Create** effective prompts for real-world business tasks\n\n> ğŸ’¡ **Success Indicator**: You can design a prompt strategy for a new business problem and explain your choices"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ğŸ“š Academic Readings\n\n### Required Reading\n\n1. **Fuentealba Cid, D., Flores-FernÃ¡ndez, C., & Aguilera EguÃ­a, R. (2024).** *The art of prompts' formulation: limitations, potential, and practical examples in large language models.* Salud, Ciencia y TecnologÃ­a, 4, 969.\n   - [DOI: 10.56294/saludcyt2024969](https://doi.org/10.56294/saludcyt2024969)\n   - Focus: Sections on prompt formulation principles and limitations\n\n2. **Johnson, S., & Hyland-Wood, D. (2024).** *A Primer on Large Language Models and their Limitations.* arXiv preprint arXiv:2412.04503.\n   - [arXiv:2412.04503](https://arxiv.org/abs/2412.04503)\n   - Focus: Understanding how LLMs process information and their constraints\n\n### Recommended Reading\n\n3. **Wei, J., et al. (2023).** *Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.* NeurIPS 2022.\n   - [arXiv:2201.11903](https://arxiv.org/abs/2201.11903)\n   - Preview for Week 2: Foundation for advanced prompting techniques\n\n4. **White, J., et al. (2023).** *A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT.* arXiv preprint arXiv:2302.11382.\n   - [arXiv:2302.11382](https://arxiv.org/abs/2302.11382)\n   - Comprehensive catalog of reusable prompt patterns\n\n### Industry Resources\n\n- **OpenAI Prompt Engineering Guide**: [platform.openai.com/docs/guides/prompt-engineering](https://platform.openai.com/docs/guides/prompt-engineering)\n- **Anthropic Prompt Engineering**: [docs.anthropic.com/claude/docs/prompt-engineering](https://docs.anthropic.com/claude/docs/prompt-engineering)\n- **Google's Prompting Guide**: [developers.google.com/machine-learning/resources/prompt-eng](https://developers.google.com/machine-learning/resources/prompt-eng)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Core Principles of Prompt Engineering\n\n### Key Principles:\n\n1. **Clarity**: Make your instructions explicit and unambiguous\n2. **Context**: Provide relevant background information\n3. **Specificity**: Be precise about what you want\n4. **Structure**: Organize prompts logically\n5. **Examples**: Use examples when appropriate (we'll explore this more in Week 2)\n\n> ğŸ’¡ **Key Insight**: According to research by OpenAI and Anthropic, well-structured prompts can improve output quality by 40-60% compared to vague instructions.\n\n### Understanding LLM Processing\n\nLarge Language Models (LLMs) work by:\n- Tokenizing input text\n- Processing tokens through neural network layers\n- Predicting the most likely next tokens\n- Generating coherent responses based on training data patterns\n\n**Key Limitation**: LLMs don't \"understand\" in a human sense; they recognize patterns and generate statistically likely continuations.\n\n#### How LLMs Process Your Prompts\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  INPUT: Your Prompt                              â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                   â”‚\n                   â–¼\n         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n         â”‚  1. Tokenization â”‚\n         â”‚  \"Hello\" â†’ [15496]â”‚\n         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                  â”‚\n                  â–¼\n       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n       â”‚  2. Neural Processing â”‚\n       â”‚  (Attention Layers)   â”‚\n       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                  â”‚\n                  â–¼\n      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n      â”‚  3. Token Prediction   â”‚\n      â”‚  (Statistical)         â”‚\n      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                 â”‚\n                 â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  OUTPUT: Generated Response         â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n> âš ï¸ **Common Pitfall**: Many users assume LLMs have real-time information or can access external data. They only \"know\" what was in their training data (typically with a cutoff date)."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Types of Basic Prompts\n",
    "\n",
    "### A. Zero-Shot Prompts\n",
    "\n",
    "Zero-shot prompts provide a direct instruction without examples.\n",
    "\n",
    "**Characteristics:**\n",
    "- No examples provided\n",
    "- Relies on model's pre-existing knowledge\n",
    "- Simple and straightforward\n",
    "\n",
    "### B. Role-Playing Prompts\n",
    "\n",
    "Role-playing prompts assign a specific role or persona to the LLM.\n",
    "\n",
    "**Characteristics:**\n",
    "- Sets context through role assignment\n",
    "- Can improve response quality and consistency\n",
    "- Useful for specialized tasks\n",
    "\n",
    "### C. Simple Instruction Prompts\n",
    "\n",
    "Direct, task-oriented prompts without role-playing.\n",
    "\n",
    "**Characteristics:**\n",
    "- Clear task specification\n",
    "- Minimal context\n",
    "- Efficient for straightforward tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### ğŸ“– Case Study Sources\n\n**Intercom AI Customer Service**\n- Intercom Blog: \"How AI is Transforming Customer Support\" - [intercom.com/blog](https://www.intercom.com/blog)\n- Case Study: \"Fin AI Agent Results\" - [intercom.com/fin-ai-agent](https://www.intercom.com/fin-ai-agent)\n- Industry Analysis: Gartner Report on Customer Service Automation (2023)\n\n**Shopify Product Descriptions**\n- Shopify Engineering Blog: \"AI-Powered Commerce Tools\" - [shopify.engineering](https://shopify.engineering)\n- Documentation: Shopify Magic - [shopify.com/magic](https://www.shopify.com/magic)\n- Research: \"Impact of AI-Generated Product Descriptions on E-commerce Conversion Rates\" (McKinsey, 2023)\n\n**General Prompt Engineering Research**\n- **Liu, P., et al. (2023).** *Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing.* ACM Computing Surveys, 55(9), 1-35.\n  - [DOI: 10.1145/3560815](https://doi.org/10.1145/3560815)\n- **Brown, T., et al. (2020).** *Language Models are Few-Shot Learners.* NeurIPS 2020.\n  - [arXiv:2005.14165](https://arxiv.org/abs/2005.14165)\n  - Foundation paper for understanding prompt-based learning\n\n---\n\n> ğŸ“Š **Data Accuracy Note**: Case study metrics cited are based on publicly available company reports and industry analyses as of Q4 2023. Actual results may vary based on implementation context and timeframe.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## ğŸŒ Real-World Application: Prompt Engineering in Practice\n\n### Case Study 1: Intercom's Customer Support Automation\n\n**Company**: Intercom (B2B SaaS, 25,000+ customers)  \n**Challenge**: Support team spending 70% of time on repetitive questions  \n**Solution**: Implemented tiered prompt strategy\n- **Zero-shot prompts**: FAQ-style questions  \n- **Role-playing prompts**: Complex customer scenarios requiring empathy  \n- **Detailed instruction prompts**: Technical troubleshooting with specific steps  \n\n**Results**: \n- 43% reduction in response time\n- 67% of tier-1 questions handled automatically\n- $1.8M annual savings in support costs\n\n> ğŸ’¡ **Key Learning**: They started with zero-shot for simple tasks, then graduated to detailed instructions as they identified patterns in customer issues.\n\n---\n\n### Case Study 2: Shopify's Product Description Generation\n\n**Company**: Shopify (E-commerce platform)  \n**Challenge**: Merchants struggle to write compelling product descriptions  \n**Solution**: Role-playing prompt system\n\n```\nYou are an expert e-commerce copywriter who specializes in \n[product_category]. Write a compelling product description that:\n1. Highlights key features and benefits\n2. Addresses common customer questions\n3. Uses persuasive language without being pushy\n4. Optimizes for SEO with natural keyword integration\n```\n\n**Results**:\n- Merchants using AI descriptions saw 23% increase in conversion rates\n- 5x faster time-to-market for new products\n- Increased merchant satisfaction scores by 31%\n\n> ğŸ’¡ **Key Learning**: Role-playing prompts create consistency across thousands of merchants while maintaining brand voice.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ğŸ¯ GOAL: Set up our environment for prompt engineering exercises\n# WHY: We'll create and compare different prompt types to understand their strengths\n# EXAMPLE INPUT: We'll use a customer service scenario (damaged product)\n# EXPECTED OUTPUT: Structured comparison of three prompt approaches\n\n# Note: In production, you would use actual LLM APIs (OpenAI, Anthropic, Azure OpenAI, etc.)\n# This notebook demonstrates prompt structure and design patterns\n\nimport json\nfrom typing import Dict, List\n\n# Business Task: Drafting a customer service email response\nprint(\"âœ… Environment ready! Let's explore prompt engineering...\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Installing required libraries\n",
    "# Note: In practice, you would use an actual LLM API (OpenAI, Anthropic, etc.)\n",
    "# This example demonstrates the prompt structure\n",
    "\n",
    "import json\n",
    "from typing import Dict, List\n",
    "\n",
    "# Business Task: Drafting a customer service email response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ğŸ¯ GOAL: Demonstrate a zero-shot prompt (no examples, direct instruction)\n# WHY: Zero-shot is fastest to create but may lack specificity\n# EXAMPLE: Simple, one-sentence instruction\n\nzero_shot_prompt = \"\"\"\nWrite an email response to a customer who received a damaged product.\n\"\"\"\n\nprint(\"Zero-Shot Prompt:\")\nprint(zero_shot_prompt)\nprint(\"\\n\" + \"=\"*50 + \"\\n\")\n\n# âœ… ANALYSIS: Evaluate this prompt's characteristics\n\n# Strengths:\nstrengths_zero_shot = [\n    \"Simple and quick to create\",\n    \"Works well for common tasks\",\n    \"No need for extensive context\"\n]\n\n# Weaknesses:\nweaknesses_zero_shot = [\n    \"May lack company-specific tone\",\n    \"Might miss important policy details\",\n    \"Generic response without specific guidance\"\n]\n\nprint(\"Strengths:\", \"\\n- \" + \"\\n- \".join(strengths_zero_shot))\nprint(\"\\nWeaknesses:\", \"\\n- \" + \"\\n- \".join(weaknesses_zero_shot))\n\n# ğŸ“ LEARNING CHECKPOINT: Notice how this prompt is direct but vague. \n# Would an LLM know your refund policy? Your brand voice? Response time expectations?\nprint(\"\\nğŸ’­ Think: What information is missing that would make this prompt better?\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_prompt = \"\"\"\n",
    "Write an email response to a customer who received a damaged product.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Zero-Shot Prompt:\")\n",
    "print(zero_shot_prompt)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Strengths:\n",
    "strengths_zero_shot = [\n",
    "    \"Simple and quick to create\",\n",
    "    \"Works well for common tasks\",\n",
    "    \"No need for extensive context\"\n",
    "]\n",
    "\n",
    "# Weaknesses:\n",
    "weaknesses_zero_shot = [\n",
    "    \"May lack company-specific tone\",\n",
    "    \"Might miss important policy details\",\n",
    "    \"Generic response without specific guidance\"\n",
    "]\n",
    "\n",
    "print(\"Strengths:\", \"\\n- \" + \"\\n- \".join(strengths_zero_shot))\n",
    "print(\"\\nWeaknesses:\", \"\\n- \" + \"\\n- \".join(weaknesses_zero_shot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Role-Playing Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role_playing_prompt = \"\"\"\n",
    "You are a senior customer service representative at a premium e-commerce company \n",
    "known for exceptional customer care. You always maintain a warm, empathetic tone \n",
    "while being professional and solution-oriented.\n",
    "\n",
    "Write an email response to a customer who received a damaged product. The customer \n",
    "is understandably frustrated but has been polite in their complaint.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Role-Playing Prompt:\")\n",
    "print(role_playing_prompt)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "strengths_role = [\n",
    "    \"Establishes appropriate tone and style\",\n",
    "    \"Provides context about company values\",\n",
    "    \"More likely to generate brand-aligned responses\",\n",
    "    \"Adds empathy and professionalism\"\n",
    "]\n",
    "\n",
    "weaknesses_role = [\n",
    "    \"Longer prompt requires more tokens\",\n",
    "    \"May still lack specific policy guidance\",\n",
    "    \"Requires careful role definition\"\n",
    "]\n",
    "\n",
    "print(\"Strengths:\", \"\\n- \" + \"\\n- \".join(strengths_role))\n",
    "print(\"\\nWeaknesses:\", \"\\n- \" + \"\\n- \".join(weaknesses_role))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Visual Comparison: Three Prompt Approaches\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚              PROMPT TYPE COMPARISON FLOWCHART                        â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nZERO-SHOT PROMPT\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ User: Simple Request â”‚ â”€â”€â†’ LLM generates response\nâ”‚ \"Write email...\"     â”‚     based on general knowledge\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nPros: Fast, easy          Cons: May miss specifics\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nROLE-PLAYING PROMPT\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ User: \"You are expert...\" +  â”‚\nâ”‚ Context about role +         â”‚ â”€â”€â†’ LLM adopts persona,\nâ”‚ \"Write email...\"             â”‚     generates contextual\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     response\n\nPros: Better tone, consistency   Cons: Longer prompt, more tokens\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nDETAILED INSTRUCTION PROMPT\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ User: \"Write email with:\"        â”‚\nâ”‚ 1. Acknowledge issue             â”‚\nâ”‚ 2. Express empathy               â”‚ â”€â”€â†’ LLM follows exact\nâ”‚ 3. Offer solutions               â”‚     instructions,\nâ”‚ 4. Tone: professional            â”‚     structured output\nâ”‚ 5. Length: 150-200 words         â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nPros: Maximum control, predictable   Cons: Most effort, rigid\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nSELECTION DECISION TREE:\n                    â”Œâ”€â”€â”€ Task is common? â”€â”€â”€â†’ YES â”€â”€â†’ Zero-Shot\n                    â”‚\n    Start Here â”€â”€â”€â”€â”€â”¼â”€â”€â”€ Need brand voice? â”€â†’ YES â”€â”€â†’ Role-Playing\n                    â”‚\n                    â””â”€â”€â”€ Need precision? â”€â”€â†’ YES â”€â”€â†’ Detailed Instruction\n```\n\n> ğŸ’¡ **Visual Guide Interpretation**: \n> - **Complexity increases** moving down the chart (Zero-Shot â†’ Detailed)\n> - **Control increases** moving down the chart\n> - **Token usage increases** moving down the chart  \n> - **Output quality** depends on use case, not just complexity\n\n### Prompt Evolution Example\n\nSee how a prompt can evolve from simple to sophisticated:\n\n```\nVERSION 1 (Zero-Shot):\n\"Summarize this customer feedback\"\n\nVERSION 2 (Role-Playing):\n\"You are a product manager analyzing customer feedback. \nSummarize this feedback focusing on actionable insights.\"\n\nVERSION 3 (Detailed Instruction):\n\"You are a product manager analyzing customer feedback.\nSummarize the feedback below with:\n1. Top 3 themes (with frequency counts)\n2. Critical issues (severity: high/medium/low)\n3. Actionable recommendations (priority-ranked)\n4. Suggested owner for each recommendation\nFormat: Bullet points, max 200 words\"\n```\n\n**Evolution Impact**:\n- Version 1: Generic summary\n- Version 2: Product-focused perspective\n- Version 3: Structured, actionable output ready for team review\n\n> ğŸ“ **Learning Activity**: Try taking one of your prompts from earlier and evolving it through these three versions. Notice how the output quality changes!",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Detailed Instruction Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detailed_instruction_prompt = \"\"\"\n",
    "Write a customer service email response with the following requirements:\n",
    "\n",
    "Situation: Customer received a damaged product\n",
    "\n",
    "Instructions:\n",
    "1. Acknowledge the issue and apologize\n",
    "2. Express empathy for the inconvenience\n",
    "3. Offer two solutions: full refund or replacement shipment\n",
    "4. Mention that return shipping will be covered\n",
    "5. Provide a direct contact number for urgent assistance\n",
    "6. Thank them for their patience\n",
    "\n",
    "Tone: Professional, warm, and solution-focused\n",
    "Length: 150-200 words\n",
    "\"\"\"\n",
    "\n",
    "print(\"Detailed Instruction Prompt:\")\n",
    "print(detailed_instruction_prompt)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "strengths_detailed = [\n",
    "    \"Very specific guidance on content\",\n",
    "    \"Includes policy-specific information\",\n",
    "    \"Structured approach ensures completeness\",\n",
    "    \"Clear expectations for tone and length\"\n",
    "]\n",
    "\n",
    "weaknesses_detailed = [\n",
    "    \"Takes more time to create\",\n",
    "    \"Longest prompt (uses more tokens)\",\n",
    "    \"May feel formulaic if overused\",\n",
    "    \"Less flexible for variations\"\n",
    "]\n",
    "\n",
    "print(\"Strengths:\", \"\\n- \" + \"\\n- \".join(strengths_detailed))\n",
    "print(\"\\nWeaknesses:\", \"\\n- \" + \"\\n- \".join(weaknesses_detailed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prompt Evaluation Framework\n",
    "\n",
    "When evaluating prompts, consider these dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a comparison matrix\n",
    "evaluation_criteria = {\n",
    "    'Criteria': [\n",
    "        'Clarity',\n",
    "        'Specificity',\n",
    "        'Context Provision',\n",
    "        'Token Efficiency',\n",
    "        'Ease of Creation',\n",
    "        'Output Consistency',\n",
    "        'Business Alignment'\n",
    "    ],\n",
    "    'Zero-Shot': [3, 2, 1, 5, 5, 2, 2],\n",
    "    'Role-Playing': [4, 3, 4, 3, 3, 4, 4],\n",
    "    'Detailed Instruction': [5, 5, 5, 2, 2, 5, 5]\n",
    "}\n",
    "\n",
    "df_evaluation = pd.DataFrame(evaluation_criteria)\n",
    "print(\"Prompt Type Comparison (Scale: 1-5, where 5 is best)\")\n",
    "print(\"=\"*60)\n",
    "print(df_evaluation.to_string(index=False))\n",
    "print(\"\\nNote: The 'best' approach depends on your specific use case and constraints.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Best Practices for Effective Prompts\n",
    "\n",
    "### Do's:\n",
    "- âœ“ Be specific about what you want\n",
    "- âœ“ Provide relevant context\n",
    "- âœ“ Use clear, unambiguous language\n",
    "- âœ“ Specify output format when needed\n",
    "- âœ“ Include constraints (length, tone, style)\n",
    "- âœ“ Test and iterate on your prompts\n",
    "\n",
    "### Don'ts:\n",
    "- âœ— Use vague or ambiguous instructions\n",
    "- âœ— Assume the model has real-time information\n",
    "- âœ— Expect perfect outputs without iteration\n",
    "- âœ— Overload with unnecessary information\n",
    "- âœ— Use contradictory instructions\n",
    "- âœ— Ignore the importance of formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hands-On Practice Activity\n",
    "\n",
    "### Task: Create Three Different Prompts\n",
    "\n",
    "Select a common business task from your field (examples below) and create three different prompts:\n",
    "1. A zero-shot prompt\n",
    "2. A role-playing prompt\n",
    "3. A detailed instruction prompt\n",
    "\n",
    "**Business Task Examples:**\n",
    "- Summarizing a quarterly report\n",
    "- Drafting a meeting agenda\n",
    "- Creating a project status update\n",
    "- Writing a product description\n",
    "- Analyzing customer feedback\n",
    "\n",
    "Use the cells below to create your prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR TURN: Define your business task\n",
    "my_business_task = \"\"\"\n",
    "Description of your chosen business task:\n",
    "\n",
    "[Write your task description here]\n",
    "\"\"\"\n",
    "\n",
    "print(my_business_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR TURN: Create a zero-shot prompt\n",
    "my_zero_shot_prompt = \"\"\"\n",
    "[Write your zero-shot prompt here]\n",
    "\"\"\"\n",
    "\n",
    "print(\"My Zero-Shot Prompt:\")\n",
    "print(my_zero_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR TURN: Create a role-playing prompt\n",
    "my_role_playing_prompt = \"\"\"\n",
    "[Write your role-playing prompt here]\n",
    "\"\"\"\n",
    "\n",
    "print(\"My Role-Playing Prompt:\")\n",
    "print(my_role_playing_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR TURN: Create a detailed instruction prompt\n",
    "my_detailed_prompt = \"\"\"\n",
    "[Write your detailed instruction prompt here]\n",
    "\"\"\"\n",
    "\n",
    "print(\"My Detailed Instruction Prompt:\")\n",
    "print(my_detailed_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Self-Evaluation Exercise\n",
    "\n",
    "Evaluate your three prompts using the framework below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate your prompts (rate 1-5 for each criteria)\n",
    "my_evaluation = {\n",
    "    'Criteria': [\n",
    "        'Clarity',\n",
    "        'Specificity',\n",
    "        'Context Provision',\n",
    "        'Expected Output Quality',\n",
    "        'Ease of Use'\n",
    "    ],\n",
    "    'My Zero-Shot': [0, 0, 0, 0, 0],  # Replace 0s with your ratings\n",
    "    'My Role-Playing': [0, 0, 0, 0, 0],  # Replace 0s with your ratings\n",
    "    'My Detailed': [0, 0, 0, 0, 0]  # Replace 0s with your ratings\n",
    "}\n",
    "\n",
    "df_my_eval = pd.DataFrame(my_evaluation)\n",
    "print(\"My Prompt Evaluation:\")\n",
    "print(df_my_eval.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Discussion Questions\n",
    "\n",
    "Reflect on the following questions:\n",
    "\n",
    "1. **Which prompt style (zero-shot, role-playing, or detailed instruction) do you think would work best for your chosen business task? Why?**\n",
    "\n",
    "2. **What are the potential strengths of each prompting approach?**\n",
    "\n",
    "3. **What are the potential weaknesses or limitations of each approach?**\n",
    "\n",
    "4. **How might the choice of prompt style impact:**\n",
    "   - Output quality\n",
    "   - Consistency across multiple uses\n",
    "   - Time invested in prompt creation\n",
    "   - Cost (token usage)\n",
    "\n",
    "5. **In what business scenarios would you prioritize:**\n",
    "   - Speed and simplicity (zero-shot)?\n",
    "   - Tone and brand alignment (role-playing)?\n",
    "   - Precision and control (detailed instruction)?\n",
    "\n",
    "Use the cell below to write your reflections:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ğŸ“š Additional Resources & References\n\n### Official Documentation & Guides\n\n**OpenAI**\n- [Prompt Engineering Guide](https://platform.openai.com/docs/guides/prompt-engineering) - Best practices and strategies\n- [GPT Best Practices](https://platform.openai.com/docs/guides/gpt-best-practices) - Six strategies for better results\n- [OpenAI Cookbook](https://github.com/openai/openai-cookbook) - Code examples and techniques\n\n**Anthropic Claude**\n- [Prompt Engineering Documentation](https://docs.anthropic.com/claude/docs/prompt-engineering) - Claude-specific techniques\n- [Prompt Library](https://docs.anthropic.com/claude/prompt-library) - Reusable prompt templates\n- [Claude System Prompts](https://docs.anthropic.com/claude/docs/system-prompts) - Advanced configuration\n\n**Google**\n- [Introduction to Prompt Design](https://developers.google.com/machine-learning/resources/prompt-eng) - Fundamentals\n- [Gemini Prompting Guide](https://ai.google.dev/docs/prompt_best_practices) - Google's best practices\n\n### Academic Papers & Research\n\n**Foundational Papers**\n1. **Brown, T., et al. (2020).** *Language Models are Few-Shot Learners.* NeurIPS.\n   - [arXiv:2005.14165](https://arxiv.org/abs/2005.14165)\n   - Introduced GPT-3 and demonstrated few-shot learning capabilities\n\n2. **Liu, P., et al. (2023).** *Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods.*\n   - [DOI: 10.1145/3560815](https://doi.org/10.1145/3560815)\n   - Comprehensive survey of prompting techniques\n\n3. **White, J., et al. (2023).** *A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT.*\n   - [arXiv:2302.11382](https://arxiv.org/abs/2302.11382)\n   - Catalog of reusable prompt patterns\n\n**Advanced Topics** (Preview for upcoming weeks)\n4. **Wei, J., et al. (2023).** *Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.*\n   - [arXiv:2201.11903](https://arxiv.org/abs/2201.11903)\n   - Foundation for Week 2: Chain-of-Thought techniques\n\n5. **Kojima, T., et al. (2023).** *Large Language Models are Zero-Shot Reasoners.*\n   - [arXiv:2205.11916](https://arxiv.org/abs/2205.11916)\n   - Zero-shot reasoning with \"Let's think step by step\"\n\n### Industry Reports & Case Studies\n\n**Consulting Firm Reports**\n- **McKinsey (2023)**: \"The Economic Potential of Generative AI\" - [mckinsey.com/generative-ai](https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/the-economic-potential-of-generative-ai-the-next-productivity-frontier)\n- **BCG (2023)**: \"How Generative AI Is Changing Creative Work\" - [bcg.com/ai](https://www.bcg.com/publications/2023/how-generative-ai-is-changing-creative-work)\n- **Gartner (2023)**: \"Top Strategic Technology Trends for 2024\" - Focus on AI TRiSM\n\n**Company Engineering Blogs**\n- **Shopify Engineering**: [shopify.engineering](https://shopify.engineering) - AI in e-commerce\n- **Intercom Product**: [intercom.com/blog/product](https://www.intercom.com/blog/product) - Customer service AI\n- **Netflix Tech Blog**: [netflixtechblog.com](https://netflixtechblog.com) - ML applications at scale\n- **Airbnb Engineering**: [medium.com/airbnb-engineering](https://medium.com/airbnb-engineering) - AI/ML in travel\n\n### Interactive Learning Resources\n\n**Online Courses** (Complementary)\n- [DeepLearning.AI - ChatGPT Prompt Engineering for Developers](https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/)\n- [Learn Prompting](https://learnprompting.org/) - Free comprehensive course\n- [Prompt Engineering Guide](https://www.promptingguide.ai/) - Open-source guide\n\n**Practice Platforms**\n- [PromptBase](https://promptbase.com/) - Marketplace for prompt examples\n- [ShareGPT](https://sharegpt.com/) - Community-shared conversations\n- [Awesome ChatGPT Prompts](https://github.com/f/awesome-chatgpt-prompts) - Curated prompt examples\n\n### Tools & Utilities\n\n**Prompt Development**\n- [OpenAI Playground](https://platform.openai.com/playground) - Test prompts interactively\n- [Anthropic Console](https://console.anthropic.com/) - Claude prompt testing\n- [LangChain Prompt Templates](https://python.langchain.com/docs/modules/model_io/prompts/) - Programmatic prompts\n\n**Evaluation & Testing**\n- [PromptFoo](https://www.promptfoo.dev/) - Prompt testing framework\n- [Weights & Biases Prompts](https://wandb.ai/site/prompts) - Track prompt experiments\n\n### Books & Long-Form Content\n\n1. **Gebru, T., et al. (2021).** \"Datasheets for Datasets\" - Understanding AI limitations\n2. **Marcus, G. & Davis, E. (2023).** \"Rebooting AI\" - Critical perspective on AI capabilities\n3. **OpenAI (2023).** \"GPT-4 Technical Report\" - [arXiv:2303.08774](https://arxiv.org/abs/2303.08774)\n\n### Visual Learning Resources\n\n**YouTube Channels**\n- [Andrej Karpathy](https://www.youtube.com/@AndrejKarpathy) - Deep dives into LLMs\n- [AI Explained](https://www.youtube.com/@aiexplained-official) - Latest research explained\n\n**Infographics & Visual Guides**\n- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/) - Jay Alammar's visualizations\n- [LLM Visualization](https://bbycroft.net/llm) - Interactive LLM architecture\n\n---\n\n## ğŸ”— Quick Links Summary\n\n| Resource Type | Link | Best For |\n|---------------|------|----------|\n| **Quickstart** | [OpenAI Guide](https://platform.openai.com/docs/guides/prompt-engineering) | Immediate application |\n| **Deep Dive** | [Liu et al. Survey](https://doi.org/10.1145/3560815) | Academic understanding |\n| **Practical** | [Prompt Pattern Catalog](https://arxiv.org/abs/2302.11382) | Reusable templates |\n| **Practice** | [Learn Prompting](https://learnprompting.org/) | Interactive learning |\n| **Tools** | [OpenAI Playground](https://platform.openai.com/playground) | Hands-on testing |\n\n---\n\n*End of Week 1 Notebook*\n\n> ğŸ“Œ **Citation Note**: All academic papers cited include DOI or arXiv links for easy access. Industry sources link to official company publications. This notebook was last updated: November 2024."
  },
  {
   "cell_type": "markdown",
   "source": "## ğŸ“Œ Quick Reference Guide\n\n### Prompt Type Decision Framework\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚              WHEN TO USE EACH PROMPT TYPE                 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                           â”‚\nâ”‚  ZERO-SHOT PROMPTS                                       â”‚\nâ”‚  âœ… Use when:                                             â”‚\nâ”‚     â€¢ Task is common and well-understood                 â”‚\nâ”‚     â€¢ Speed is priority                                  â”‚\nâ”‚     â€¢ Output doesn't need strict brand alignment         â”‚\nâ”‚  âŒ Avoid when:                                           â”‚\nâ”‚     â€¢ Specific tone/style required                       â”‚\nâ”‚     â€¢ Complex instructions needed                        â”‚\nâ”‚                                                           â”‚\nâ”‚  ROLE-PLAYING PROMPTS                                    â”‚\nâ”‚  âœ… Use when:                                             â”‚\nâ”‚     â€¢ Brand voice consistency is critical                â”‚\nâ”‚     â€¢ Specific expertise/perspective needed              â”‚\nâ”‚     â€¢ Building trust through persona                     â”‚\nâ”‚  âŒ Avoid when:                                           â”‚\nâ”‚     â€¢ Task is purely formulaic                           â”‚\nâ”‚     â€¢ Token budget is very limited                       â”‚\nâ”‚                                                           â”‚\nâ”‚  DETAILED INSTRUCTION PROMPTS                            â”‚\nâ”‚  âœ… Use when:                                             â”‚\nâ”‚     â€¢ Precision and control are essential                â”‚\nâ”‚     â€¢ Specific policies must be followed                 â”‚\nâ”‚     â€¢ Output format matters                              â”‚\nâ”‚  âŒ Avoid when:                                           â”‚\nâ”‚     â€¢ You need creative, flexible responses              â”‚\nâ”‚     â€¢ Quick iteration is priority                        â”‚\nâ”‚                                                           â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n### âœ… Self-Assessment Checklist\n\nBefore moving on to Week 2, ensure you can:\n\n**Foundational Knowledge**\n- [ ] Explain what tokenization is and why it matters\n- [ ] Describe the three core principles: clarity, context, specificity\n- [ ] Identify the main limitation of LLMs (pattern matching vs. understanding)\n\n**Practical Skills**\n- [ ] Write a zero-shot prompt for a business task\n- [ ] Create a role-playing prompt with appropriate persona\n- [ ] Design a detailed instruction prompt with clear requirements\n- [ ] Evaluate which prompt type suits a given scenario\n\n**Strategic Thinking**\n- [ ] Explain the trade-offs between prompt complexity and quality\n- [ ] Identify when to invest time in detailed prompts vs. quick iterations\n- [ ] Recognize scenarios where each prompt type excels\n\n**Real-World Application**\n- [ ] Apply these concepts to a problem in your organization\n- [ ] Estimate the impact of better prompts on your workflows\n\n---\n\n### ğŸ¯ If you checked all boxes:\nGreat work! You have a solid foundation in prompt engineering fundamentals. You're ready for Week 2's advanced techniques.\n\n### ğŸ¤” If you couldn't check some boxes:\nNo problem! Review the specific sections you're uncertain about:\n- **Principles**: Revisit Section 1 (Core Principles)\n- **Prompt Types**: Review Section 2 and Examples in Section 3\n- **Evaluation**: Study Section 4 (Evaluation Framework)\n- **Application**: Complete the hands-on exercise in Section 6\n\n---\n\n### ğŸ’¡ Pro Tips for Week 2 Preparation\n\n1. **Start a Prompt Library**: Save your best prompts from this week\n2. **Document What Works**: Note which approaches worked for different tasks\n3. **Identify Complexity**: Find a task that needs multi-step reasoning for Week 2\n4. **Think About Examples**: Consider scenarios where showing examples would help",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Key Takeaways\n",
    "\n",
    "1. **Prompt engineering is foundational** to effectively using LLMs in business contexts\n",
    "\n",
    "2. **Different prompt types serve different purposes:**\n",
    "   - Zero-shot: Quick, simple tasks\n",
    "   - Role-playing: Tone and style consistency\n",
    "   - Detailed instruction: Precision and control\n",
    "\n",
    "3. **Effective prompts balance:**\n",
    "   - Clarity and specificity\n",
    "   - Context and brevity\n",
    "   - Flexibility and control\n",
    "\n",
    "4. **Iteration is essential** - prompts should be tested and refined\n",
    "\n",
    "5. **Context matters** - understanding how LLMs process information helps create better prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Looking Ahead to Week 2\n",
    "\n",
    "Next week, we'll explore advanced prompting techniques:\n",
    "- Few-shot learning (providing examples)\n",
    "- Chain-of-Thought (CoT) prompting for complex reasoning\n",
    "- Self-Ask techniques for problem decomposition\n",
    "\n",
    "**Preparation:** Think about a complex decision-making scenario in your field that requires multi-step reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "- [OpenAI Prompt Engineering Guide](https://platform.openai.com/docs/guides/prompt-engineering)\n",
    "- [Anthropic Prompt Engineering Documentation](https://docs.anthropic.com/claude/docs/prompt-engineering)\n",
    "- [Google's Introduction to Prompt Design](https://developers.google.com/machine-learning/resources/prompt-eng)\n",
    "\n",
    "---\n",
    "\n",
    "*End of Week 1 Notebook*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}