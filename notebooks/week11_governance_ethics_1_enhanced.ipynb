{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“˜ Week 11: Technology Governance & Ethics I - Frameworks & Principles\n",
    "\n",
    "## MBA 590 - Advanced AI Strategy: Prompting and Agentic Frameworks\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "As organizations deploy advanced technologies like LLMs and agentic systems, ethical considerations and robust governance become critical. This week introduces the fundamental principles and frameworks for responsible technology development and deployment. We'll explore established frameworks like NIST AI RMF, examine core ethical principles, and learn how to translate these into actionable organizational policies.\n",
    "\n",
    "### Key Topics\n",
    "- Ethical risks in AI and advanced technologies (bias, fairness, transparency, accountability)\n",
    "- Overview of responsible AI frameworks (NIST, OECD, IEEE)\n",
    "- Core principles for ethical technology development\n",
    "- Translating principles into concrete policies\n",
    "- Risk assessment and mitigation strategies\n",
    "- Stakeholder considerations in governance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "By the end of this week, you will be able to:\n",
    "\n",
    "1. Identify and assess key ethical risks in AI and advanced technology systems\n",
    "2. Compare and contrast major responsible AI frameworks (NIST, OECD, IEEE)\n",
    "3. Apply core ethical principles to technology decision-making\n",
    "4. Translate abstract ethical principles into concrete organizational policies\n",
    "5. Conduct risk assessments for AI/technology initiatives\n",
    "6. Design governance structures appropriate for your organization\n",
    "7. Balance innovation with ethical responsibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Academic Readings\n",
    "\n",
    "1. **National Institute of Standards and Technology (NIST). (2023).** *AI Risk Management Framework (AI RMF 1.0).* (Focus on core principles and functions)\n",
    "\n",
    "2. **Floridi, L., & Cowls, J. (2019).** *A Unified Framework of Five Principles for AI in Society.* Harvard Data Science Review, 1(1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Tuple\n",
    "import json\n",
    "\n",
    "# Set style for visualizations\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('Set2')\n",
    "\n",
    "print('Libraries imported successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Key Ethical Risks in AI Systems\n",
    "\n",
    "### Primary Risk Categories\n",
    "\n",
    "#### A. Bias and Fairness\n",
    "- **Data bias**: Historical biases in training data\n",
    "- **Algorithmic bias**: Biases introduced by model design\n",
    "- **Deployment bias**: Differential impact across user groups\n",
    "- **Impact**: Discrimination, unequal treatment, legal liability\n",
    "\n",
    "#### B. Transparency and Explainability\n",
    "- **Black box problem**: Difficulty understanding model decisions\n",
    "- **Stakeholder needs**: Different audiences need different explanations\n",
    "- **Regulatory requirements**: Explainability mandates (e.g., GDPR \"right to explanation\")\n",
    "- **Impact**: Lack of trust, compliance issues, poor decision-making\n",
    "\n",
    "#### C. Accountability and Responsibility\n",
    "- **Decision attribution**: Who is responsible when AI makes mistakes?\n",
    "- **Liability chains**: Complex systems with multiple stakeholders\n",
    "- **Human oversight**: Ensuring meaningful human control\n",
    "- **Impact**: Legal liability, reputational damage, regulatory penalties\n",
    "\n",
    "#### D. Privacy and Data Protection\n",
    "- **Data collection**: Over-collection, unauthorized use\n",
    "- **Data retention**: Storing data longer than necessary\n",
    "- **Re-identification**: De-anonymized data risks\n",
    "- **Impact**: Privacy violations, regulatory fines, loss of trust\n",
    "\n",
    "#### E. Security and Safety\n",
    "- **Adversarial attacks**: Manipulating model inputs/outputs\n",
    "- **Model theft**: IP protection of AI systems\n",
    "- **System failures**: AI making harmful decisions\n",
    "- **Impact**: Security breaches, physical harm, financial losses\n",
    "\n",
    "#### F. Environmental and Social Impact\n",
    "- **Computational costs**: Energy consumption of large models\n",
    "- **Labor displacement**: Job losses from automation\n",
    "- **Societal polarization**: Amplification of misinformation\n",
    "- **Impact**: Environmental damage, social inequality, democratic erosion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Risk assessment framework\n",
    "\n",
    "ethical_risks = {\n",
    "    'Risk_Category': [\n",
    "        'Bias & Fairness',\n",
    "        'Transparency & Explainability',\n",
    "        'Accountability',\n",
    "        'Privacy & Data Protection',\n",
    "        'Security & Safety',\n",
    "        'Environmental Impact',\n",
    "        'Social Impact'\n",
    "    ],\n",
    "    'Likelihood': ['High', 'High', 'Medium', 'High', 'Medium', 'Medium', 'Medium'],\n",
    "    'Impact': ['High', 'Medium', 'High', 'Critical', 'High', 'Medium', 'High'],\n",
    "    'Regulatory_Pressure': ['High', 'High', 'Medium', 'Critical', 'High', 'Medium', 'Low'],\n",
    "    'Mitigation_Difficulty': ['High', 'Medium', 'Medium', 'Medium', 'High', 'Low', 'High']\n",
    "}\n",
    "\n",
    "df_risks = pd.DataFrame(ethical_risks)\n",
    "\n",
    "# Convert to numeric for risk scoring\n",
    "level_map = {'Low': 1, 'Medium': 2, 'High': 3, 'Critical': 4}\n",
    "df_risks['Likelihood_Score'] = df_risks['Likelihood'].map(level_map)\n",
    "df_risks['Impact_Score'] = df_risks['Impact'].map(level_map)\n",
    "df_risks['Risk_Score'] = df_risks['Likelihood_Score'] * df_risks['Impact_Score']\n",
    "\n",
    "print(\"ETHICAL RISK ASSESSMENT\")\n",
    "print(\"=\"*80)\n",
    "print(df_risks[['Risk_Category', 'Likelihood', 'Impact', 'Risk_Score']].to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"HIGHEST PRIORITY RISKS (Risk Score â‰¥ 9):\")\n",
    "high_risks = df_risks[df_risks['Risk_Score'] >= 9].sort_values('Risk_Score', ascending=False)\n",
    "print(high_risks[['Risk_Category', 'Risk_Score', 'Regulatory_Pressure']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize risk landscape\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Risk matrix\n",
    "scatter = ax1.scatter(df_risks['Likelihood_Score'], df_risks['Impact_Score'], \n",
    "                     s=df_risks['Risk_Score']*50, alpha=0.6, c=df_risks['Risk_Score'], \n",
    "                     cmap='YlOrRd', edgecolors='black', linewidth=1.5)\n",
    "\n",
    "for idx, row in df_risks.iterrows():\n",
    "    ax1.annotate(row['Risk_Category'], \n",
    "                (row['Likelihood_Score'], row['Impact_Score']),\n",
    "                fontsize=9, ha='center', va='bottom')\n",
    "\n",
    "ax1.set_xlabel('Likelihood', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Impact', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Ethical Risk Matrix\\n(Size = Risk Score)', fontsize=14, fontweight='bold')\n",
    "ax1.set_xticks([1, 2, 3, 4])\n",
    "ax1.set_xticklabels(['Low', 'Medium', 'High', 'Critical'])\n",
    "ax1.set_yticks([1, 2, 3, 4])\n",
    "ax1.set_yticklabels(['Low', 'Medium', 'High', 'Critical'])\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xlim(0.5, 4.5)\n",
    "ax1.set_ylim(0.5, 4.5)\n",
    "\n",
    "# Add risk zones\n",
    "ax1.axhline(y=2.5, color='orange', linestyle='--', alpha=0.3)\n",
    "ax1.axvline(x=2.5, color='orange', linestyle='--', alpha=0.3)\n",
    "\n",
    "# Risk scores comparison\n",
    "colors = ['green' if x < 6 else 'orange' if x < 9 else 'red' for x in df_risks['Risk_Score']]\n",
    "bars = ax2.barh(df_risks['Risk_Category'], df_risks['Risk_Score'], color=colors, alpha=0.7)\n",
    "ax2.set_xlabel('Risk Score (Likelihood Ã— Impact)', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Risk Score Rankings', fontsize=14, fontweight='bold')\n",
    "ax2.axvline(x=6, color='orange', linestyle='--', alpha=0.5, label='Medium Risk')\n",
    "ax2.axvline(x=9, color='red', linestyle='--', alpha=0.5, label='High Risk')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Major Responsible AI Frameworks\n",
    "\n",
    "### A. NIST AI Risk Management Framework (AI RMF 1.0)\n",
    "\n",
    "**Purpose**: Voluntary framework to help organizations manage AI risks\n",
    "\n",
    "**Core Functions**:\n",
    "1. **GOVERN**: Establish and nurture a culture of risk management\n",
    "2. **MAP**: Understand context and categorize AI risks\n",
    "3. **MEASURE**: Assess, analyze, and track AI risks\n",
    "4. **MANAGE**: Allocate resources to mapped and measured risks\n",
    "\n",
    "**Key Characteristics**:\n",
    "- Valid: Technical accuracy and effectiveness\n",
    "- Reliable: Consistent performance\n",
    "- Safe: No unacceptable harm\n",
    "- Secure: Protected from threats\n",
    "- Resilient: Handles exceptional conditions\n",
    "- Accountable: Clear responsibility\n",
    "- Transparent: Documented and explainable\n",
    "- Explainable: Understandable to stakeholders\n",
    "- Interpretable: Clear meaning of outputs\n",
    "- Privacy-enhanced: Protects personal data\n",
    "- Fair: No harmful bias\n",
    "\n",
    "### B. OECD AI Principles\n",
    "\n",
    "**Five Value-Based Principles**:\n",
    "1. **Inclusive growth**: AI should benefit all of humanity\n",
    "2. **Sustainable development**: Environmental and social responsibility\n",
    "3. **Human-centered values**: Respect for human rights and dignity\n",
    "4. **Transparency**: Understandable and explainable\n",
    "5. **Robustness**: Safe, secure, and accountable\n",
    "\n",
    "### C. IEEE Ethically Aligned Design\n",
    "\n",
    "**Eight Principles**:\n",
    "1. Human Rights\n",
    "2. Well-being\n",
    "3. Data Agency\n",
    "4. Effectiveness\n",
    "5. Transparency\n",
    "6. Accountability\n",
    "7. Awareness of Misuse\n",
    "8. Competence\n",
    "\n",
    "### D. Floridi & Cowls: Five Principles for AI in Society\n",
    "\n",
    "1. **Beneficence**: Promoting well-being, preserving dignity, sustaining the planet\n",
    "2. **Non-maleficence**: Privacy, security, capability caution\n",
    "3. **Autonomy**: Decision-making power of humans\n",
    "4. **Justice**: Fairness, non-discrimination, solidarity\n",
    "5. **Explicability**: Intelligibility and accountability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Framework comparison\n",
    "\n",
    "frameworks = {\n",
    "    'Framework': ['NIST AI RMF', 'OECD', 'IEEE EAD', 'Floridi & Cowls'],\n",
    "    'Primary_Focus': [\n",
    "        'Risk Management',\n",
    "        'Values & Governance',\n",
    "        'Design Principles',\n",
    "        'Ethical Foundation'\n",
    "    ],\n",
    "    'Scope': ['Broad', 'Very Broad', 'Technical', 'Philosophical'],\n",
    "    'Actionability': ['High', 'Medium', 'High', 'Low'],\n",
    "    'Industry_Adoption': ['Growing', 'High', 'Medium', 'Low'],\n",
    "    'Best_For': [\n",
    "        'US organizations, risk-focused approach',\n",
    "        'International organizations, policy makers',\n",
    "        'Technical teams, designers',\n",
    "        'Ethical foundations, academic research'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_frameworks = pd.DataFrame(frameworks)\n",
    "\n",
    "print(\"RESPONSIBLE AI FRAMEWORKS COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "for idx, row in df_frameworks.iterrows():\n",
    "    print(f\"\\n{row['Framework']}:\")\n",
    "    print(f\"  Primary Focus: {row['Primary_Focus']}\")\n",
    "    print(f\"  Scope: {row['Scope']}\")\n",
    "    print(f\"  Actionability: {row['Actionability']}\")\n",
    "    print(f\"  Industry Adoption: {row['Industry_Adoption']}\")\n",
    "    print(f\"  Best For: {row['Best_For']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map frameworks to ethical principles\n",
    "\n",
    "principles_coverage = {\n",
    "    'Principle': [\n",
    "        'Fairness',\n",
    "        'Transparency',\n",
    "        'Accountability',\n",
    "        'Privacy',\n",
    "        'Security',\n",
    "        'Safety',\n",
    "        'Human Rights',\n",
    "        'Sustainability'\n",
    "    ],\n",
    "    'NIST': ['âœ“âœ“', 'âœ“âœ“âœ“', 'âœ“âœ“', 'âœ“âœ“', 'âœ“âœ“âœ“', 'âœ“âœ“âœ“', 'âœ“', 'âœ“'],\n",
    "    'OECD': ['âœ“âœ“', 'âœ“âœ“', 'âœ“âœ“', 'âœ“', 'âœ“', 'âœ“âœ“', 'âœ“âœ“âœ“', 'âœ“âœ“âœ“'],\n",
    "    'IEEE': ['âœ“âœ“', 'âœ“âœ“', 'âœ“âœ“âœ“', 'âœ“âœ“âœ“', 'âœ“âœ“', 'âœ“', 'âœ“âœ“âœ“', 'âœ“'],\n",
    "    'Floridi': ['âœ“âœ“âœ“', 'âœ“âœ“âœ“', 'âœ“âœ“âœ“', 'âœ“âœ“', 'âœ“', 'âœ“', 'âœ“âœ“', 'âœ“']\n",
    "}\n",
    "\n",
    "df_coverage = pd.DataFrame(principles_coverage)\n",
    "\n",
    "print(\"\\nPRINCIPLES COVERAGE BY FRAMEWORK\")\n",
    "print(\"Legend: âœ“ = Addressed, âœ“âœ“ = Emphasized, âœ“âœ“âœ“ = Core Focus\")\n",
    "print(\"=\"*80)\n",
    "print(df_coverage.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Core Ethical Principles in Practice\n",
    "\n",
    "### Applying NIST AI RMF: The Four Functions\n",
    "\n",
    "#### GOVERN\n",
    "**Objective**: Establish organizational culture and structure for AI risk management\n",
    "\n",
    "**Key Activities**:\n",
    "- Define AI governance structure\n",
    "- Assign roles and responsibilities\n",
    "- Establish risk tolerance levels\n",
    "- Create policies and procedures\n",
    "- Foster risk-aware culture\n",
    "\n",
    "#### MAP\n",
    "**Objective**: Understand AI system context and identify potential risks\n",
    "\n",
    "**Key Activities**:\n",
    "- Document system purpose and use cases\n",
    "- Identify stakeholders and impacts\n",
    "- Categorize risks (technical, societal, legal)\n",
    "- Assess data quality and bias\n",
    "- Map regulatory requirements\n",
    "\n",
    "#### MEASURE\n",
    "**Objective**: Assess and track AI risks quantitatively and qualitatively\n",
    "\n",
    "**Key Activities**:\n",
    "- Define metrics for trustworthy characteristics\n",
    "- Test for bias and fairness\n",
    "- Evaluate model performance\n",
    "- Measure privacy preservation\n",
    "- Conduct security assessments\n",
    "- Monitor ongoing performance\n",
    "\n",
    "#### MANAGE\n",
    "**Objective**: Prioritize and respond to AI risks\n",
    "\n",
    "**Key Activities**:\n",
    "- Develop risk treatment plans\n",
    "- Implement controls and safeguards\n",
    "- Document decisions and rationale\n",
    "- Establish incident response\n",
    "- Continuous monitoring and improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NIST AI RMF implementation template\n",
    "\n",
    "def create_ai_rmf_template(system_name: str, use_case: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Create a structured AI RMF assessment template.\n",
    "    \"\"\"\n",
    "    template = {\n",
    "        'system_info': {\n",
    "            'name': system_name,\n",
    "            'use_case': use_case,\n",
    "            'assessment_date': '2025-11-17',\n",
    "            'risk_owner': '[To be assigned]'\n",
    "        },\n",
    "        'govern': {\n",
    "            'governance_structure': {\n",
    "                'board_oversight': 'TBD',\n",
    "                'executive_sponsor': 'TBD',\n",
    "                'risk_committee': 'TBD',\n",
    "                'technical_lead': 'TBD'\n",
    "            },\n",
    "            'policies': [\n",
    "                'AI Development Policy',\n",
    "                'Data Governance Policy',\n",
    "                'Ethics Review Process',\n",
    "                'Incident Response Plan'\n",
    "            ],\n",
    "            'risk_tolerance': 'Medium'  # Low, Medium, High\n",
    "        },\n",
    "        'map': {\n",
    "            'stakeholders': [],  # To be identified\n",
    "            'potential_risks': [\n",
    "                {'category': 'Bias', 'description': 'TBD', 'likelihood': 'TBD', 'impact': 'TBD'},\n",
    "                {'category': 'Privacy', 'description': 'TBD', 'likelihood': 'TBD', 'impact': 'TBD'},\n",
    "                {'category': 'Security', 'description': 'TBD', 'likelihood': 'TBD', 'impact': 'TBD'},\n",
    "            ],\n",
    "            'regulatory_requirements': [],  # GDPR, CCPA, etc.\n",
    "            'impact_assessment': 'Required'\n",
    "        },\n",
    "        'measure': {\n",
    "            'fairness_metrics': ['Demographic parity', 'Equal opportunity', 'Calibration'],\n",
    "            'performance_metrics': ['Accuracy', 'Precision', 'Recall', 'F1-score'],\n",
    "            'privacy_metrics': ['k-anonymity', 'Differential privacy', 'Re-identification risk'],\n",
    "            'security_metrics': ['Adversarial robustness', 'Input validation', 'Access controls'],\n",
    "            'monitoring_frequency': 'Monthly'  # Daily, Weekly, Monthly, Quarterly\n",
    "        },\n",
    "        'manage': {\n",
    "            'risk_treatments': [],  # To be developed\n",
    "            'controls': [\n",
    "                'Human review for high-stakes decisions',\n",
    "                'Bias testing in development',\n",
    "                'Privacy impact assessment',\n",
    "                'Security penetration testing'\n",
    "            ],\n",
    "            'incident_response': 'Defined',\n",
    "            'continuous_improvement': 'Quarterly reviews'\n",
    "        }\n",
    "    }\n",
    "    return template\n",
    "\n",
    "# Example: Customer service chatbot\n",
    "example_rmf = create_ai_rmf_template(\n",
    "    system_name=\"Customer Service AI Agent\",\n",
    "    use_case=\"Automated customer inquiry resolution and support\"\n",
    ")\n",
    "\n",
    "print(\"AI RMF ASSESSMENT TEMPLATE\")\n",
    "print(\"=\"*80)\n",
    "print(json.dumps(example_rmf, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Translating Principles into Policies\n",
    "\n",
    "### From Abstract to Concrete\n",
    "\n",
    "Ethical principles must be operationalized into specific policies and guidelines that teams can follow.\n",
    "\n",
    "#### Example 1: Fairness Principle â†’ Bias Testing Policy\n",
    "\n",
    "**Principle**: \"AI systems should be fair and not discriminate\"\n",
    "\n",
    "**Policy**: \n",
    "- All AI systems impacting individuals must undergo bias testing\n",
    "- Test across protected attributes (race, gender, age, etc.)\n",
    "- Document disparate impact analysis\n",
    "- Achieve <10% difference in outcomes across groups\n",
    "- Annual re-testing for production systems\n",
    "\n",
    "#### Example 2: Transparency Principle â†’ Model Documentation Standard\n",
    "\n",
    "**Principle**: \"AI systems should be transparent and explainable\"\n",
    "\n",
    "**Policy**:\n",
    "- Complete model cards for all AI systems\n",
    "- Document training data, performance metrics, limitations\n",
    "- Provide explanation mechanisms appropriate to use case\n",
    "- Make documentation accessible to stakeholders\n",
    "- Update documentation with each model version\n",
    "\n",
    "#### Example 3: Accountability Principle â†’ Human Oversight Requirements\n",
    "\n",
    "**Principle**: \"There should be clear accountability for AI decisions\"\n",
    "\n",
    "**Policy**:\n",
    "- High-stakes decisions require human review\n",
    "- Define escalation paths for AI uncertainties\n",
    "- Maintain audit logs of all AI decisions\n",
    "- Designate responsible parties for each system\n",
    "- Establish appeal processes for affected individuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy translation framework\n",
    "\n",
    "policy_translations = {\n",
    "    'Principle': [\n",
    "        'Fairness',\n",
    "        'Transparency',\n",
    "        'Accountability',\n",
    "        'Privacy',\n",
    "        'Security',\n",
    "        'Safety'\n",
    "    ],\n",
    "    'Abstract_Statement': [\n",
    "        'AI should be fair and unbiased',\n",
    "        'AI should be explainable',\n",
    "        'Responsibility should be clear',\n",
    "        'Personal data should be protected',\n",
    "        'Systems should be secure',\n",
    "        'AI should not cause harm'\n",
    "    ],\n",
    "    'Concrete_Policy': [\n",
    "        'Bias testing across protected classes; <10% outcome disparity',\n",
    "        'Model cards + explanation mechanisms for all systems',\n",
    "        'Human review for high-stakes; audit logs; designated owners',\n",
    "        'Privacy impact assessments; data minimization; encryption',\n",
    "        'Penetration testing; access controls; incident response',\n",
    "        'Safety testing; human oversight; emergency shutoff'\n",
    "    ],\n",
    "    'Measurement': [\n",
    "        'Demographic parity ratio',\n",
    "        'Documentation completeness',\n",
    "        'Audit trail coverage',\n",
    "        'Privacy compliance score',\n",
    "        'Security audit results',\n",
    "        'Incident frequency'\n",
    "    ],\n",
    "    'Enforcement': [\n",
    "        'Pre-deployment review',\n",
    "        'Documentation gate',\n",
    "        'Quarterly audits',\n",
    "        'Automated checks',\n",
    "        'Continuous monitoring',\n",
    "        'Incident response'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_policies = pd.DataFrame(policy_translations)\n",
    "\n",
    "print(\"PRINCIPLE-TO-POLICY TRANSLATION\")\n",
    "print(\"=\"*80)\n",
    "for idx, row in df_policies.iterrows():\n",
    "    print(f\"\\n{row['Principle'].upper()}\")\n",
    "    print(f\"  Abstract: {row['Abstract_Statement']}\")\n",
    "    print(f\"  Concrete Policy: {row['Concrete_Policy']}\")\n",
    "    print(f\"  Measurement: {row['Measurement']}\")\n",
    "    print(f\"  Enforcement: {row['Enforcement']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Risk Assessment Methodology\n",
    "\n",
    "### AI System Risk Categorization\n",
    "\n",
    "Based on EU AI Act approach:\n",
    "\n",
    "**UNACCEPTABLE RISK** - Prohibited\n",
    "- Social scoring by governments\n",
    "- Real-time biometric identification in public (with exceptions)\n",
    "- Subliminal manipulation\n",
    "- Exploitation of vulnerabilities\n",
    "\n",
    "**HIGH RISK** - Strict requirements\n",
    "- Critical infrastructure\n",
    "- Education/employment decisions\n",
    "- Essential services (credit, insurance)\n",
    "- Law enforcement\n",
    "- Migration/border management\n",
    "- Justice administration\n",
    "\n",
    "**LIMITED RISK** - Transparency obligations\n",
    "- Chatbots (must disclose AI nature)\n",
    "- Emotion recognition\n",
    "- Biometric categorization\n",
    "- Deepfakes (must label)\n",
    "\n",
    "**MINIMAL RISK** - No restrictions\n",
    "- AI-enabled video games\n",
    "- Spam filters\n",
    "- Recommendation systems (in most cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI system risk assessment tool\n",
    "\n",
    "def assess_ai_system_risk(use_case: str, \n",
    "                          impacts_individuals: bool,\n",
    "                          high_stakes: bool,\n",
    "                          automated_decision: bool,\n",
    "                          protected_attributes: bool,\n",
    "                          reversible: bool) -> Dict:\n",
    "    \"\"\"\n",
    "    Assess risk level of an AI system based on key characteristics.\n",
    "    \n",
    "    Returns risk level, required controls, and governance approach.\n",
    "    \"\"\"\n",
    "    risk_score = 0\n",
    "    \n",
    "    if impacts_individuals:\n",
    "        risk_score += 2\n",
    "    if high_stakes:\n",
    "        risk_score += 3\n",
    "    if automated_decision:\n",
    "        risk_score += 2\n",
    "    if protected_attributes:\n",
    "        risk_score += 3\n",
    "    if not reversible:\n",
    "        risk_score += 2\n",
    "    \n",
    "    if risk_score >= 10:\n",
    "        risk_level = 'HIGH'\n",
    "        controls = [\n",
    "            'Executive approval required',\n",
    "            'Comprehensive bias testing',\n",
    "            'Human review of all decisions',\n",
    "            'Full explainability required',\n",
    "            'Privacy impact assessment',\n",
    "            'Security penetration testing',\n",
    "            'Quarterly audits',\n",
    "            'Appeal mechanism required'\n",
    "        ]\n",
    "        governance = 'Strict oversight with dedicated review board'\n",
    "    elif risk_score >= 6:\n",
    "        risk_level = 'MEDIUM'\n",
    "        controls = [\n",
    "            'Director approval required',\n",
    "            'Bias testing for key attributes',\n",
    "            'Explanation capability required',\n",
    "            'Privacy assessment',\n",
    "            'Security review',\n",
    "            'Semi-annual audits'\n",
    "        ]\n",
    "        governance = 'Regular review with risk committee'\n",
    "    else:\n",
    "        risk_level = 'LOW'\n",
    "        controls = [\n",
    "            'Manager approval',\n",
    "            'Basic testing',\n",
    "            'Standard security review',\n",
    "            'Annual review'\n",
    "        ]\n",
    "        governance = 'Standard development process'\n",
    "    \n",
    "    return {\n",
    "        'use_case': use_case,\n",
    "        'risk_score': risk_score,\n",
    "        'risk_level': risk_level,\n",
    "        'required_controls': controls,\n",
    "        'governance_approach': governance\n",
    "    }\n",
    "\n",
    "# Example assessments\n",
    "test_cases = [\n",
    "    {\n",
    "        'use_case': 'Resume screening for hiring',\n",
    "        'impacts_individuals': True,\n",
    "        'high_stakes': True,\n",
    "        'automated_decision': True,\n",
    "        'protected_attributes': True,\n",
    "        'reversible': False\n",
    "    },\n",
    "    {\n",
    "        'use_case': 'Product recommendations',\n",
    "        'impacts_individuals': True,\n",
    "        'high_stakes': False,\n",
    "        'automated_decision': True,\n",
    "        'protected_attributes': False,\n",
    "        'reversible': True\n",
    "    },\n",
    "    {\n",
    "        'use_case': 'Internal document summarization',\n",
    "        'impacts_individuals': False,\n",
    "        'high_stakes': False,\n",
    "        'automated_decision': False,\n",
    "        'protected_attributes': False,\n",
    "        'reversible': True\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"AI SYSTEM RISK ASSESSMENTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for test in test_cases:\n",
    "    result = assess_ai_system_risk(**test)\n",
    "    print(f\"\\n{result['use_case'].upper()}\")\n",
    "    print(f\"  Risk Score: {result['risk_score']}\")\n",
    "    print(f\"  Risk Level: {result['risk_level']}\")\n",
    "    print(f\"  Governance: {result['governance_approach']}\")\n",
    "    print(f\"  Required Controls:\")\n",
    "    for control in result['required_controls']:\n",
    "        print(f\"    - {control}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Practical Exercise\n",
    "\n",
    "### Translate a Principle into Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR TURN: Select a NIST principle and create organizational policy\n",
    "\n",
    "my_ethics_policy = \"\"\"\n",
    "SELECTED PRINCIPLE: [Choose: Explainable, Fair, Secure, Privacy-Enhanced, etc.]\n",
    "\n",
    "WHY THIS PRINCIPLE MATTERS TO MY ORGANIZATION:\n",
    "[Explain the business/ethical rationale]\n",
    "\n",
    "CONCRETE POLICY REQUIREMENTS:\n",
    "1. [Specific requirement with measurable criteria]\n",
    "2. [Specific requirement with measurable criteria]\n",
    "3. [Specific requirement with measurable criteria]\n",
    "4. [Specific requirement with measurable criteria]\n",
    "\n",
    "IMPLEMENTATION GUIDELINES:\n",
    "For Developers:\n",
    "- [Specific action]\n",
    "- [Specific action]\n",
    "\n",
    "For Product Managers:\n",
    "- [Specific action]\n",
    "- [Specific action]\n",
    "\n",
    "For Data Scientists:\n",
    "- [Specific action]\n",
    "- [Specific action]\n",
    "\n",
    "MEASUREMENT & COMPLIANCE:\n",
    "- Primary Metric: [How will you measure compliance?]\n",
    "- Target: [Specific target value]\n",
    "- Monitoring Frequency: [How often?]\n",
    "- Review Process: [Who reviews and when?]\n",
    "\n",
    "ENFORCEMENT:\n",
    "- Pre-deployment: [What must happen before deployment?]\n",
    "- Ongoing: [What happens during operation?]\n",
    "- Non-compliance: [What are the consequences?]\n",
    "\n",
    "EXCEPTIONS:\n",
    "- When allowed: [Under what conditions?]\n",
    "- Who can approve: [Who has authority?]\n",
    "- Documentation required: [What must be documented?]\n",
    "\n",
    "EXAMPLE SCENARIO:\n",
    "[Describe a specific use case and how this policy applies]\n",
    "\"\"\"\n",
    "\n",
    "print(my_ethics_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Discussion Questions\n",
    "\n",
    "1. **Framework Selection**: Which responsible AI framework (NIST, OECD, IEEE, or Floridi) is most appropriate for your organization? Why?\n",
    "\n",
    "2. **Principle Prioritization**: If you could only implement three ethical principles rigorously, which would you choose and why?\n",
    "\n",
    "3. **Risk Tolerance**: How should your organization balance innovation speed with ethical safeguards? What's your risk tolerance?\n",
    "\n",
    "4. **Trade-offs**: Transparency and privacy can conflict (explaining decisions may reveal personal data). How do you resolve such tensions?\n",
    "\n",
    "5. **Measurement Challenges**: How do you measure abstract concepts like \"fairness\" or \"accountability\" in practice?\n",
    "\n",
    "6. **Governance Burden**: How do you prevent ethics governance from becoming bureaucratic overhead that slows development?\n",
    "\n",
    "7. **Global Operations**: If operating internationally, how do you harmonize different regulatory requirements (EU AI Act, US frameworks, etc.)?\n",
    "\n",
    "8. **Stakeholder Conflicts**: When different stakeholders (customers, employees, shareholders, regulators) have conflicting interests, how do you prioritize?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Reflections:\n",
    "\n",
    "[Write your responses here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Key Takeaways\n",
    "\n",
    "1. **Ethics is not optional** - regulatory pressure and societal expectations make ethical AI a business imperative\n",
    "\n",
    "2. **Multiple frameworks exist** - NIST, OECD, IEEE provide complementary approaches to responsible AI\n",
    "\n",
    "3. **Principles need translation** - abstract ethics must become concrete policies with measurable criteria\n",
    "\n",
    "4. **Risk varies by use case** - not all AI systems require the same level of governance\n",
    "\n",
    "5. **Governance enables scale** - proper frameworks allow responsible deployment at scale\n",
    "\n",
    "6. **Stakeholder engagement matters** - diverse perspectives strengthen ethical decision-making\n",
    "\n",
    "7. **Measurement is challenging** - quantifying ethical performance requires thoughtful metrics\n",
    "\n",
    "8. **Continuous improvement** - ethics governance must evolve with technology and societal norms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Looking Ahead to Week 12\n",
    "\n",
    "Next week, we'll continue with **Technology Governance & Ethics II: Regulation & Implementation**.\n",
    "\n",
    "We'll explore:\n",
    "- Overview of regulatory landscapes (EU AI Act, GDPR implications)\n",
    "- Practical implementation of governance frameworks\n",
    "- Bias audits and transparency reporting\n",
    "- Data privacy controls and security protocols\n",
    "- Human oversight mechanisms\n",
    "\n",
    "**Preparation:** Review your organization's current compliance obligations. What regulations currently apply to your AI/technology initiatives?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "### Frameworks:\n",
    "- NIST AI Risk Management Framework: [https://www.nist.gov/itl/ai-risk-management-framework](https://www.nist.gov/itl/ai-risk-management-framework)\n",
    "- OECD AI Principles: [https://oecd.ai/en/ai-principles](https://oecd.ai/en/ai-principles)\n",
    "- IEEE Ethically Aligned Design: [https://ethicsinaction.ieee.org/](https://ethicsinaction.ieee.org/)\n",
    "\n",
    "### Ethics Research:\n",
    "- Partnership on AI resources\n",
    "- AI Ethics Lab guidelines\n",
    "- Montreal Declaration for Responsible AI\n",
    "\n",
    "### Implementation Guides:\n",
    "- Google's Responsible AI Practices\n",
    "- Microsoft's Responsible AI Standard\n",
    "- IBM's AI Ethics framework\n",
    "\n",
    "---\n",
    "\n",
    "*End of Week 11 Notebook*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
